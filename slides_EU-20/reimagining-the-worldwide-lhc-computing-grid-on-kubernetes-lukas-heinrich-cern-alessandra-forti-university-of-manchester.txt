Reimagining the Worldwide LHC Computing Grid on Kubernetes: PJCK-8607 - events@cncf.io - Wednesday, August 19, 2020 8:30 AM - 109 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:02 [W] Okay, hi there and Welcome to our talk.
00:00:09 [W] My name is Lucas Henry. I'm a particle physicist working at CERN and I'm going to give this presentation together with Alessandra who's also connected who is a Senior Systems administrator at the University of Manchester and we are both working for a particle
00:00:21 [W] Experiment called Atlas and we're going to talk about how we use kubernative to streamline our infrastructure.
00:00:30 [W] and so just to introduce our work a bit.
00:00:41 [W] So the main place of our work is CERN. It's the European lab for particle physics research and is located in Geneva Switzerland. And so here we do many experiments relating to partial Nuclear Physics, but the main experiment
00:00:48 [W] That you might have heard about is the Large Hadron Collider.
00:00:53 [W] It's a very big particle accelerator that is 27 kilometer and circumference and basically it's a big underground tunnel a hundred metres underground where we accelerate particles and and bring them to Collision for
00:01:05 [W] Action points that we have two particle beams one going clockwise and this big circle and the other one going counterclockwise and four points that are labeled Alice EMS lhcb. And Alice. We bring those particles to Collision and
00:01:20 [W] Um here we can see the what it looks like in the tunnel underground.
00:01:30 [W] So this huge magnets that accelerate the particles to almost the speed of light and so they Traverse this 30-kilometer tunnel ten thousand times a second and at these interaction points. Basically what we have done is we have constructed
00:01:41 [W] Giant cameras that take basically three-dimensional pictures of these collisions and these pictures help us understand nature better.
00:01:53 [W] So here we basically Collide a 40,000,000 receive 40 million collisions per second.
00:02:03 [W] And so we generate a huge amount of data. And so just to give you a scale of what these quote-unquote cameras look like.
00:02:07 [W] So this is real life picture of one of these detectors. So this is the Atlas detector that both
00:02:12 [W] Alessandra and I work at and you can see a human at the center of the image just for scale. So these are some of the biggest scientific experiments that have ever been constructed and we use that to understand the
00:02:25 [W] Basic building blocks of matter so some of the things that we are trying to understand what those Large Hadron Collider is what the basic structure of nature is, so we want to understand how the elementary particles work. So you might have heard about electrons
00:02:40 [W] our parks and but they're bunch of other Elementary particles and maybe one of the things we would really be excited about this to find new Elementary particles and through that uncovered the nature of dark matter or maybe Dark Energy because it turns out the with the
00:02:55 [W] In Theory of physics that we only understand 5% of the energy content of the universe and actually 95% of what's out there in the universe is unknown to us.
00:03:11 [W] And so one of the other big questions that we are also studying is what exactly the difference between matter and antimatter is because we see a lot of matter around us and while we know that antimatter exists, it's not very abundant in the universe and this is a big mystery that
00:03:20 [W] We're trying to answer with this big experiment.
00:03:29 [W] And so one of the things you might have heard about the Large Hadron colliders. That is we used it to discover the Higgs boson. So this has been a central piece of our physics theory and it explains what the nature of masses and why
00:03:38 [W] Particles have mass and so and 2012. We analyzed our data we could find this new Elementary particles and then professors Hicks angle are subsequently were awarded a
00:03:53 [W] Price through because of this discovery that we made here at CERN and another reason why you might have heard about CERN is because as we are all connected online through the web this is in part because of the
00:04:08 [W] The World Wide Web was developed at certain. So Tim berners-lee when he was working here in the late 80s and early 90s. He developed the web and it's basically the foundation of what we all use today.
00:04:23 [W] And so we need to you know, we need to record a lot of data to make these types of discoveries.
00:04:38 [W] And so it's not it's not surprising that big data and you know, a lot of it infrastructure has always been very closely linked to the physics that we're trying to do is so not only do we have this magnificent Harbor these huge detectors, but we also have
00:04:48 [W] Quite expensive it infrastructure.
00:05:03 [W] And so this is one of the control rooms that we have at CERN. And so just to give you an idea of how much data we are taking. So the big Atlas detector that you've seen earlier in the slide has a something around a hundred million readout sensor, so that all
00:05:05 [W] Stream data back to our disc and we so we see 40 million collisions per second and that gives us a data rate of almost petabyte-scale.
00:05:34 [W] us a potentially a discovery like the Higgs boson and so in order to process all of this data, we are not only relying on the central laboratory at CERN, but we are a worldwide collaboration of physics
00:05:50 [W] Tuitions the National Labs and universities that all pooled resources in order to process this data. And so this is what we constructed during the 90s and early 2000s and we call this the worldwide LHC Computing grid
00:06:04 [W] It and so Alessandra is one of the leading experts in this area and she'll basic introduce you how we use that and where we see kubernative playing into this.
00:06:15 [W] Yeah, okay. So I'm going to talk about WN Century Computing grid which was created around 2000. So it is
00:06:31 [W] 20 years older its duration of those things of in the individual academic institutions and they provide the resources to Global scientific collaborations. The LHC experiments are the
00:06:46 [W] More a user's that but there are other collaborations using the grid.
00:06:52 [W] Participating to decorate around 1150 sides that I actually pledge the resources. They are from a big International labs to smaller universities to even
00:07:08 [W] Department clusters and at the last count they offered the roughly 1.4 million CPU is a for around the 10,000 users.
00:07:24 [W] Of course.
00:07:25 [W] it is a distributed environment.
00:07:27 [W] And so the challenge is to provide the users with the uniform experience across the Attorney General's Computing landscape.
00:07:37 [W] And the glitter middleware was a developed a to act as an overlay to provide a common API to all these different terms of sources.
00:07:50 [W] of course the big I did it logdna t was also flatten the beat up in the past by the experiments and the sides
00:08:07 [W] I walked it is require the foresight to participate to the grade. For example, the main Computing resources that the working load behind the badge system had all to be
00:08:22 [W] I don't to install the erudite family operative system. They needed Intel CPUs and at least a two gigabyte of ram a
00:08:36 [W] So exciting General needed a number of middleware services to be deployed to be recognized as a great Insight the most important one, of course for this talk is
00:08:51 [W] Stem but there are a number a number of other services like a storage with the grid protocols for transfers information system authorization monitoring
00:09:07 [W] Does software space Etc?
00:09:15 [W] This is quite a heavy duty and rigid deployment model and of course can be applied only to pledge their resources.
00:09:22 [W] But there is also these are diversifying and becoming more opportunistic and we don't have any more control over them and what they deployed.
00:09:35 [W] so DW is CG experiments have already started to adapt to this new way of deploying by deploying
00:09:54 [W] for example containers tweeze the requirements on the operative system, for example
00:10:02 [W] But this is not the only thing that needs to be done.
00:10:12 [W] So how how can kubernative can add but the deployment in this case? Well four sides it offers
00:10:24 [W] scalable line centralized service employment many of the services mentioned before can be now centrally deployed by someone from the experiments or
00:10:39 [W] Someone that decided maybe in the in the region at the UK for example lies in an organization that can do this kind of deployment.
00:10:53 [W] This reduces the needed to train admins on the grid Services, which are very very peculiar to our environment and can allow to train people instead on something
00:11:04 [W] That is Miles more acquire those around the world also k8s lausanne in-toto.
00:11:11 [W] scaling which allows the service has to be scaled and up and down depending on the requirements since the site and might even be diversifying for example, they might want to
00:11:26 [W] Will blow resources behind the Jupiter Rob Reider than a batch system at a certain point.
00:11:35 [W] These can be very helpful.
00:11:43 [W] Then the also it allows arbitrary arbitrary size deployment.
00:11:44 [W] You can deploy kubernative on a laptop or in a larger Hub high-availability cluster and these as we will see in the demo
00:11:55 [W] He's a quite an interesting feature as a batch system. It is good at doing a resource management.
00:12:11 [W] Of course, it is a major missing feature for us, which is fair shares.
00:12:15 [W] So right now we are deploying it mostly ate dedicated sites dedicated means the device is different from pledged dedicated means that
00:12:26 [W] Iran only one experiment.
00:12:28 [W] So in particular since Lucas and I working at last how does this work out class in our class. It allows us to streamline our to provide a common basis.
00:12:46 [W] The on the tarot Eugene users or sees in particular.
00:12:54 [W] Hey on top of the standard the batch system sites.
00:12:57 [W] We have started to use HP season and cloud and our system has a broken system and they caused to this service called
00:13:09 [W] Harvest at Harvest their does the submission of the workflows and the has an API for each system when we arrived to k8s.
00:13:24 [W] Things are because now we can submit to enough to put to a university or for example to Google.
00:13:32 [W] And lately also to Amazon.
00:13:35 [W] Sorry, you can see here.
00:13:48 [W] We started to work on it last year and there is a steady decrease of resources and of sites that are starting to deploy and you can see that there is
00:14:00 [W] Is it?
00:14:01 [W] There are all their resources are represented there is certainly there is a university of Victoria.
00:14:12 [W] There is University of Chicago, but also Google and Amazon.
00:14:17 [W] And the in the future expected to these to increase the results of Taiwan daintily which was the first one to deploy.
00:14:29 [W] How do you do?
00:14:35 [W] How did we achieve this we have as I say that is?
00:14:40 [W] System the user submits to the broker the broker creates a central Q depending on what the workflows requirements are.
00:14:56 [W] It works into the Harvest there and then the Harvester Brokers to the operator resource is harvested doesn't submit the workloads law directly.
00:15:10 [W] It's some meets the something called at the pilot.
00:15:14 [W] the pilot is
00:15:16 [W] In sight of scripted I do intelligent things on the worker nodes stages in the data runs of the payload of stages out of the
00:15:32 [W] Y'all to put monitors the stage monitors the results. He said he for example the jobs that exceeds the memory requirements a eat
00:15:47 [W] He kills it also it here. It isn't really shown there. But in reality the job is wrong that he no Singularity container and what we have done
00:16:02 [W] He's forever.
00:16:05 [W] Kubernative C is simply to do all these in kubernative.
00:16:14 [W] Both Harvester has been reinstalled as a kubernative resources and it submits it to equip that is cluster and the biota runs inside the Pod so
00:16:25 [W] In the case of kubernative sir. We are also running next in containers, which is another interesting feature mainly is containerd e Singularity inside the containerd E
00:16:40 [W] Eventually in the future and the collaborative Paradigm of kubernative take over.
00:16:51 [W] That's at least what we all perfect but it is a bit too early and we will simply transform and the pilot features into kubernative. Say yeah more
00:17:03 [W] Files configuration files it job configuration files. So and with this I'm passing by Couture the to
00:17:20 [W] shift for introducing the demo
00:17:23 [W] Okay.
00:17:24 [W] Yeah.
00:17:25 [W] Thanks Alessandra.
00:17:25 [W] So in this case, we will do a little bit of a demo.
00:17:36 [W] So if you remember so that worldwide LLC Computing grid, you can basically think of it as a very large Federation of clusters and so traditionally well they were standard batch clusters as you would use them in
00:17:45 [W] Academic institutions we of course now have kubenetes that we could use to join this worldwide Federation.
00:17:56 [W] And so what we will try to do in this demo is to set up infrastructure on a brand new machine to create a kubenetes cluster and join this worldwide Federation, then we'll also submit a work load into this Federation that will then end up on our
00:18:06 [W] Just deployed infrastructure and do some Physics work for us.
00:18:14 [W] And so the demo that we prepared is basically simulating high energy physics collisions.
00:18:18 [W] And so this is a very important aspect of our physics work.
00:18:31 [W] So we need to kind of understand what our data looks like and so for that we kind of simulate this using some simulation code. And so with this I'll just pass on to a demo video that we recorded
00:18:35 [W] And after the video of you come back for a QA.
00:18:38 [W] It's a deploy a miniature version of the infrastructure that we are working on deploying more widely on the worldwide LLC Computing infrastructure.
00:20:06 [W] And so we will deploy to kubernative.
00:20:06 [W] okay, so this was the demo and so this basically showed us how we can use cloud native tools like kubernative and Helm charts to very quickly deploy resources into this Global Federation that actually
00:26:29 [W] On in production, so we actually connected these kubernative clusters that we just created to the global Federation and ran a job through all the all of the production system.
00:26:40 [W] I think we are quite excited about this because it shows us what opportunities we can get from cloud native tools to streamline our processes and manages worldwide distributed quite heterogeneous resources that is that are managed by
00:26:55 [W] Many different institutions and so before going to QA. We just wanted to acknowledge a few people who actually did the hard work and put together all of these building blocks that we use in this demo and now we'll go to the QA.
00:27:10 [W] Okay.
00:27:20 [W] hello can years in the QA?
00:27:22 [W] So we've received one question which is from Daniel who's asking if anybody can run the helm chart that we used in the demo. So all of the helm charge and code that we used in
00:27:38 [W] It was open source, but obviously so in the demo we deployed this to production order to connect to this worldwide Federation.
00:27:52 [W] We needed to have secrets in order to connect to this. So it's not like everybody can kind of connect to this without having any proper authentication, but the code that we're developing two Constructors
00:28:01 [W] Duration is open source.
00:28:03 [W] Okay.
00:28:15 [W] So I long for the other one to reach her is asking how many sides they participate in these at the moment.
00:28:17 [W] We have three universities that Blaster we are starting to use Google and Amazon.
00:28:27 [W] Free universities plus the CERN, of course CERN is a main player here.
00:28:36 [W] Maybe just add to this so so in-toto of the World by the LC Computing grid is many more sites. But so we have a handful that are currently deployed and kubernative and we're hoping to expand as we gain more experience
00:28:55 [W] Using kubernative Cindy's academic context hopefully will have a larger fraction of these sites run on kubenetes.
00:29:02 [W] Yeah.
00:29:04 [W] There's another question.
00:29:27 [W] So Marcos is asking do you have specialized nodes for example GPU nodes for GP U related workloads.
00:29:30 [W] So the answer is yes.
00:29:35 [W] So this is something that we've especially with the rise of machine learning that we're using in our science are very interested in and so there are various institutions that have GPU resources among others.
00:29:45 [W] The Manchester site that I was under is working on and
00:29:46 [W] Over here the container Paradigm is really fitting well because the software that is used for these machine learning workloads is highly Dynamic and so we are relying for example on base images for
00:30:01 [W] Sense of flow in pipes or these popular machine learning tools and so we are also experimenting with integrating those with kubernative sand.
00:30:15 [W] for example, the advanced scheduling that we need has kind of allows us to will be helpful here. But so far we've integrated gpus with the standard batch systems and connected them to the
00:30:24 [W] Federation but we use containers, but it's not yet on kubernative.
00:30:28 [W] There is a question number five. I carry on he's asking if we have solutions for my want my money topology Eternal Jane 80.
00:30:46 [W] itrenew Ma on the nodes four yeses and has a has that the other universities don't have it yet.
00:30:56 [W] So, how do you screen?
00:31:05 [W] Yeah, he's creating the some peculiar has Solutions because you have to be in the resources.
00:31:14 [W] The next question is from Michael.
00:31:24 [W] So how are the kubernative Clusters interconnected, or is it more loose coupling and can those clusters limit the resources available to the welc G and so the answers.
00:31:31 [W] Yes.
00:31:32 [W] It's a quite loose coupling. So the nice thing about the workloads that we are mostly interested in is that their embarrassingly parallel. So every Collision that we're analyzing is independent of each other and so we can parallel
00:31:45 [W] Embarrassingly parallel distribute this across the world. And so we don't require a very close coupling between the Clusters. And so this kind of coordinating service which we call Harvester is the one that has all the
00:32:00 [W] Q config files and can coordinate and you know add classes and remove clusters again. And so it's a quite loose coupling we are and also to certain it team is experimenting a lot with Federated kubernative.
00:32:14 [W] Solution so we've been looking into admiralty or we are also developing our own Federation Solutions and we've tried kubeflow V1 and V2 and but a lot of that stuff is not really required for the
00:32:29 [W] Clothes that we need for our science. And so this type of loose coupling has been quite useful and it's very easy to limit or completely remove a cluster again from this Federation by the management infrastructure that
00:32:45 [W] So there is another question on there's a he's asking why we are on the TOC any Munchies within the airport instead of running directly the workload.
00:33:03 [W] And the reason is that I take the system that we have in place at the moment.
00:33:11 [W] It's quite complicated.
00:33:17 [W] So we are on the what I call them the pilot that which is on this side of scripts and the pylon to call says Singularity containerd rather than a Docker container and this is
00:33:26 [W] And working across all Android and fifty sides and the easiest way for the moment was to Simply adopt what we have
00:33:41 [W] On the like it it was a controlled machine knife or a bare metal worker node in the future. We will put it to split these and
00:33:56 [W] Don't even splitter what we call my job and the workload a nice small and Panza and use kubernative small natively.
00:34:08 [W] No One technical requirement is that we need to also staged in data and Stage out data and so you see this in other operators like tekton tasks that you know, you need to be able to run a sequence of PODS and
00:34:24 [W] So which goes a little bit beyond of what the initialization containers and kubernative provide you but I did before wrapping up.
00:34:33 [W] I did want to answer a pair of son who is asking whether or not the difference in CPUs between different clusters makes a difference.
00:34:42 [W] So in the demo we were able to submit a work load and schedule it to a specific Target cluster. So if this makes a difference in our workload, the our brokering will take care of it, but mostly we don't care and if it takes a little bit longer just because lenses.
00:34:54 [W] Landed on the CPU that is not as powerful.
00:35:07 [W] We usually just deal with in the weight and and but take the advantage of this Global federation's that are focusing on the high-performing CPUs, which might be overcrowded.
00:35:11 [W] Okay. So I think we're out of time for the QA both Alessandra and I will be on the slack for the next couple of minutes and are happy to answer any additional questions and okay.
00:35:19 [W] you very much.
