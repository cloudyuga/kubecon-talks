An Open Platform for Trading Interconnected Equities and Assets: PQVU-7095 - events@cncf.io - Thursday, August 20, 2020 11:15 AM - 199 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:06 [W] Hi, everyone.
00:01:04 [W] Hi, everyone, in today's talk will be addressing creating an open platform for Equity trading at scale using analytics and deep learning as a tools for the underlying platform.
00:05:17 [W] The goal is to implement both a development and the production component of the system.
00:05:21 [W] in a production level trading we care about too many aspects not just the stock price we care about high and low in the day volume which many primitive training examples rely on for example, the relationship between precious metals
00:05:37 [W] In today's talk will be addressing creating an open platform for Equity trading at scale using analytics and deep learning as a tools for the underlying platform. The goal is to implement of a development and the production
00:05:41 [W] old and silver availability of social media feeds
00:05:43 [W] automatic analysis of analysis reports like SEC reports or earning cold calculation of momentum and cash valuation.
00:05:55 [W] In addition to this. We also have to comprehend the analysis with market indicators volatility indices as well as many other aspects that will have to get integrated into the Moon.
00:06:09 [W] This is the workflow of a single asset with it being a stock or a metal or commodity and so on surfaces and inputs to the system would include pricing volume media like videos and sound and
00:06:26 [W] Tweets Facebook statuses company financial data in addition to historical data one sources are available the are processed through stream processing polling and batch
00:06:42 [W] Data would be explored visualized cleansed and augmented and Invasion reached once data is ready.
00:06:54 [W] We start building the model and we start validating this model.
00:06:57 [W] Finally the model gets through cycles of testing until it's ready for deployment.
00:07:05 [W] That's when it's monitored for accuracy and performance post-deployment.
00:07:07 [W] In this slide were showing an example of developing production clusters. For example in the development side. The tensor flow model will be driving the training service based on long short term memory to predict the price and volume of an asset.
00:07:24 [W] We'll be deploying this model this model at scale.
00:07:28 [W] To give a sense of the complexity of the trading floor.
00:07:37 [W] There are multiple types of trading desks.
00:07:43 [W] For example Equity trading desks would handle everything from Equity trading to Exotic options trading fixed income trading desk will handle government bonds corporate bonds and other bonds and bond like instruments that pay a yield
00:07:52 [W] Range trading desk facilitate rating and currency payers by acting as market makers.
00:08:03 [W] They can also engage in proprietary trading activities commodity trading desks are focused on agriculture products metals and other Commodities such as crude oil gold and coffee each desk has multiple personas like brokers
00:08:15 [W] managers Risk Managers pencil
00:08:20 [W] in architecture, we have multiple clusters for each assets for each asset. So clusters for stocks clusters for precious metals another one for Commodities another word from the indices and so on.
00:08:36 [W] in this we show the need for kubernative as our platform to handle all the complications surrounding the execution of the prediction service on a single asset really the size of the box here is in tailing the complexity
00:08:54 [W] Sure that from Scully and others and they're published a reference at the bottom.
00:09:06 [W] But the idea is simply here is there are a lot of other services and tasks and processes that go before and around and after the machine learning code is done and being able to comprehend all of
00:09:16 [W] For the valid execution of the system.
00:09:20 [W] From a machine learning perspective.
00:09:24 [W] This is the machine learning path where the data gets ingested analyze transformed elevated. You split it between the model you build a model you validated then you augment with a training at scale.
00:09:36 [W] You start rolling out the model and serving it and then you Monitor and log it and we'll be talking about each stage in the coming few slides.
00:09:45 [W] For each asset will always have to ensure that we're remembering that components of development and production.
00:09:58 [W] They have to be dealt with separately so that you ensure the agility of the development while deployment at scale for production.
00:10:04 [W] as we said before stocks are not the only asset the there are other assets that are extremely important with cross impacts across all of these components like Precious Metals Commodities and odds
00:10:21 [W] For multiple equities for example stocks precious metals. We use more clusters for us to be able to comprehend and run this analysis in separation since each one of them will have its own
00:10:38 [W] And development and deployment different from the others.
00:10:47 [W] So being able to isolate them is extremely important to ensure the continuity of this work.
00:10:50 [W] Again, we're just getting you back to the machine learning path just to keep in mind that not only have to isolate the different Commodities stocks and others, but you will have to ensure this process goes forward with every cluster you have in hand.
00:11:07 [W] The first path which the data is ingested in and processed. For example, the ticker data are fed into the ticker processing pods, which in turn are ingested into ax and ra.
00:11:20 [W] Interface pad similarly that weeds are processed through pods, which are later saved into the Cassandra database.
00:11:34 [W] We relied on already published libraries from Google for example, like click to deploy for the ease of managed Services. The scalability is achieved through vertical could of scalar which actually helps us a lot that we don't have to worry about
00:11:43 [W] Interface pad similarly, the tweets are processed through Parts, which are later saved into the Cassandra database.
00:11:45 [W] We relied on already published libraries from Google for example, like click to deploy for the ease of managed services.
00:11:46 [W] The scalability is achieved through vertical put of scalar, which actually helps us a lot that we don't have to worry about it since it's a managed service from cloud provider.
00:11:49 [W] Data analysis segment is extremely important and it's really the foundation of building what comes next in the model. So process data are stored inside Cassandra including Twitter analytics, which have their
00:12:03 [W] And it's really the foundation of building what comes next in the model.
00:12:04 [W] So process data are stored inside Cassandra including Twitter analytics, which have their servers read the process data and provide the analytics around it many are based functions based on shiny provided.
00:12:11 [W] The final analysis for development part it is computed on job turn notebook within the kubeflow to provide direct analysis to the data stored inside Cassandra.
00:12:25 [W] Data scientists will use the Jupiter Hub. Notebook installed on the developing cluster for data exploration visualization and Analysis of processed an inference data that were stored in the Cassandra database.
00:12:41 [W] Many algorithms have been implemented on our platform like correlation momentum Martin angles arima macd and sentiment analysis and all them work directly on the Cassandra database.
00:13:00 [W] We'll use dataflow where pallet execution of algorithms to determine the best correlation and production.
00:13:07 [W] Here the multiple the here there are multiple Services one that development service pods read ingested tweets analyze the cinnamon the analyze the sentiments through communication with natural language processing parsers and finally recording
00:13:23 [W] In the Cassandra database data scientists test their algorithms through running Jupiter Hub and communicating with the sentiment analysis service.
00:13:34 [W] Another set of paths that generates correlation between two different stocks gold and other stock. This is very useful in prediction of prices.
00:13:46 [W] some of these
00:13:50 [W] filters are given here as examples in this list of filters that will be talking about for example, and arima filter autoregressive integrated moving average on all slacks and testing them against actual return for every filter we
00:14:05 [W] Rhythms and tested the best possible outcome from analysis of variance Anova. We looked at the independent two samples t-test thing.
00:14:16 [W] For analysis of covariance.
00:14:22 [W] We looked at the covariance as the main driver including predicted marginal means and linear prediction.
00:14:28 [W] Holt winter test Trends and seasonality in the data in hand and being able to understand that is important for addressing that trending that is key for addressing performance.
00:14:45 [W] Autoregressive moving average Arma would also used for being able to understand the the stock Behavior.
00:14:56 [W] And now getting into the next part as we're creating.
00:15:04 [W] Multiple sets of cron jobs want to transform data into formats used by tensorflow. Another to is to do the transformation.
00:15:17 [W] for example, PCA whitening normalization outliers and so on so being able to run the cron jobs is very important for us to be able to Fork off. These different tasks simultaneously.
00:15:30 [W] Here's an example.
00:15:36 [W] We're moving outliers the crown job for datadog mentation PCA principal component analysis as well as whitening.
00:15:42 [W] when we move to the data split and this is where it fits in the machine learning path, which is extremely important before you start training your model so that you will do this multiple times until you reach the conversion of a consistent model
00:16:00 [W] The desired workflow as well as expectations. Basically the incoming data is split into training validation and testing data sets and any changes in the data will re trigger the rebuilding of of the
00:16:16 [W] Any changes in the data will re trigger the rebuilding of the underlying components.
00:16:17 [W] examples of splitting is actual splitting is more complicated than just dividing them but it's extremely important to try different sets of the splitting that would result in directly influencing
00:16:34 [W] The model has be billed and this part is executed using tensorflow jobs the of jobs created on kubeflow where the long short term Uriel stm-4 price prediction the random Forest
00:16:49 [W] I said Market prediction and the natural language processing analysis for the media and the tweets and textual information multiple. Simultaneous jobs are created and validated.
00:17:01 [W] Here is an example of a mlperf data was red from multiple data sets before we started working on them.
00:17:11 [W] After that, we extend the model on larger number of assets and different number of intervals predictions within the last 5 minutes 10 minutes and so on.
00:17:21 [W] Yet another example here of the animal for the worker as we build the system up.
00:17:30 [W] Next idea is the hyper parameter optimization technique to test for different parameters making sure the sensitivity is within an acceptable range and then you start delivering on to the next steps for the buildup.
00:17:48 [W] Once the model has converged and settled we start worrying about the rollout and the serving of these models in at scale for it to be deployed.
00:18:02 [W] Will you sell them to serve the model and we use sto and correlation was seldom to control the traffic?
00:18:17 [W] Kubeflow has been key in building the whole process and controlling it would create a container images for pre-processing training analysis production deployment and each employment has been scaled independently.
00:18:35 [W] I hope this was a value to you and looking forward to your questions. Thank you.
00:18:41 [W] So we have a question just popped up why choose Cassandra? I think what Justice Sandra for the processing I think from a perspective of data management, there are different options
00:19:32 [W] the examples that we showed today, we're really to illustrate a couple of things to illustrate the need for multi cluster management and managing the interaction between them one of these clusters will
00:19:50 [W] Capable of replicating itself as needed or providing the data to the different clusters and also managing the interaction. So the right answer is Cassandra as white column was a good option, but is not the only option
00:20:06 [W] Sundra as white column was a good option, but it's not the only option you will see throughout this presentation.
00:20:11 [W] There are many other ingredients that we can actually select and choose show data flow a kind of done Apache air flow for example, but in general it was how to put the pieces together
00:20:23 [W] I show data flow. I kind of done Apache air flow for example, but in general it was how to put the pieces together to bring it up, but it's more of an exemplary not to exclude others.
00:20:28 [W] All right.
00:20:31 [W] There is another good question.
00:20:35 [W] I like that. I would like to share with everyone.
00:20:42 [W] It's asking about the latency sensitive application.
00:20:43 [W] How long does it take?
00:20:44 [W] for example an important tweet to a tradable information from the system? Remember these production systems were showing you a prototype looking at the demo what I have being
00:20:59 [W] To trade live or provide think that reading Services there is fundamentally.
00:21:09 [W] there are two pillars that need to happen a trading platform should show furnace meaning data should be available to everyone who's utilizing that platform on Fair basis without biasing one versus the other I cannot give
00:21:22 [W] Third down I'm giving it to other from the trading platform and in generality the multicast thing of such data and availability of it is also controlled by the need for it to be fair. Meaning everyone should have
00:21:37 [W] At the same time down to the micro millisecond were should go. So with that said I just want to give this background and the question about latency.
00:21:51 [W] There are multi points for latency.
00:21:52 [W] Not only the tweets even the sensitivity to the rights doesn't study through the Reeds of the database and if you really look at combine it with the question before if you look at the Cassandra from a cap
00:22:06 [W] Of is it really the right platform for you to save the data beyond the in a prototype or not?
00:22:15 [W] So to answer your question, there are multi points of latency. And not only that weeds that are controlled at these are highly regulated and also audited and it has to be available for everyone.
00:22:29 [W] So being able to do the whole TSA being able to provide the data with the same latency is a must-have, but this is beyond the scope of what we're showing here. Could you give more
00:22:37 [W] Tails, I well so the question is can you give more details in their presentation? I will I think we might have believe we have who will have enough time to look like to go through Asthma as many of the questions as I can and then we can do a quick demo
00:22:52 [W] A question here.
00:22:53 [W] Could you give more details?
00:22:53 [W] I will so the question is can you give more details and implementation?
00:22:54 [W] Well, I think we might have believe we have who will have enough time don't like to go through Asthma as many of the questions as I can and then we can do a quick demo to show that but yes happy to show you how things are running and they have the
00:22:56 [W] But yes happy to show you how things are running and they have the demo running and the chair the screen there.
00:22:58 [W] Okay, I have another question Rocky Mahmoud.
00:23:09 [W] What kind of performance unique tools are using and then we'll pipeline if any so again, what were showing here is a prototype. If you even look at the Holter filter for the seasonality and how to manage
00:23:19 [W] Measurement and prediction and estimation of this is a novelty. There are different factors for how to do that.
00:23:29 [W] I think the direct answer to your question or keep is optimization is really a science and art it has to be mandated by the regulation the workflow but it also has to make sense within the framework of that
00:23:43 [W] Of course, we've done a lot of pruning a lot of tuning and a lot of reduction of some of the neural networks when we started doing some natural language processing but in general there are other factors for the optimization
00:23:59 [W] Earlier, even if you do a specific filter, there are multi ways for optimized amazing the implementation and deployment of it as well.
00:24:09 [W] I think this is very much the questions we have so far. So, let's wait a second.
00:24:18 [W] So I think we can spend few minutes running the demo have that's okay with you guys.
00:25:06 [W] This might answer your question as well as Alexander when we're talking about sdu. I relied a lot on managed services for the quick implementation here.
00:25:18 [W] You know what let me just run the demo. I think it would be faster.
00:25:18 [W] So I'm going to share my screen.
00:25:20 [W] Hopefully you see you guys.
00:25:34 [W] And again Ali you look at the number of equity stocks included you look at the number of tools and you look at constrains our production that you will be developing against you create few of these pipelines exclude them with the direct management
00:25:52 [W] And that you will be developing against you create few of these pipelines exclude them with the direct management tools for the execution and based on that.
00:25:58 [W] You start implementing it so let me go back.
00:26:00 [W] And when you do the execution, you will be able to implement few of these hypotheses or general ideas that you try to implement and you take them afterwards the data where you store it.
00:26:23 [W] at the store we use Cassandra here for the like almond capabilities even being able to store a lot of contents there, but there are different options where you can go for if you're using different clouds
00:26:38 [W] Or Amazon. They also have other products that would fit the bill accordingly as well.
00:26:49 [W] And once you're done with all of this you would be able to start interacting with the data's outcome being able to visualize it or share it or send it to other parts of the organization. So
00:27:01 [W] as you go
00:27:03 [W] into the platform here and that platform of choice that we run things on is Google is start building your own instances of creating your own tasks with your identity and each
00:27:19 [W] Include the cluster build up for a specific asset or equity and you start building on top of them. What goes that so you shaking difference cluster?
00:27:31 [W] Yeah, and of course, it depends on the response time, usually this will take between 10 or 15 minutes until you create them depending on how slow or how fast the originals.
00:27:45 [W] As I said, we're using data flow, but you can also use Apache airflow but we use the manage one from Google here.
00:27:53 [W] Leslie monitor the execution it becomes relevant now for you to see the nature of the data the data production and also the results and how they are getting store and you take them up towards to be able to do so
00:28:14 [W] On the crunch your data on some of these filters we store them as we said when we Sandra we can be said also there are different data management console job options that can go beyond her somewhere depending on your preference.
00:28:30 [W] now we run the task the the data is concerned on that it is poured and the outcome of this filter is already available and extend over can give it up here now and based on that you can start seeing Trends
00:28:51 [W] The moving averages or different Windows of another sort of variable windows for the analysis.
00:28:58 [W] All right.
