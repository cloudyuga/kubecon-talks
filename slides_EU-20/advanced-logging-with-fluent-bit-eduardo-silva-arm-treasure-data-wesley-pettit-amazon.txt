Advanced Logging with Fluent Bit: MNNY-6911 - events@cncf.io - Wednesday, August 19, 2020 8:29 AM - 107 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:54 [W] Hello everyone.
00:01:00 [W] Welcome to the advanced logging with fluentd session. I puke on 20/20 Europe.
00:01:07 [W] My name is Eduardo Silva and together with Wesley Pettit from Amazon where fluentd maintainers.
00:01:17 [W] We're going to Deep dive into fluid bait internals how it is solving problems and everything that you was you waiting for.
00:01:18 [W] The main thing about logging is the bit have the capability to perform data analysis, but to perform data analysis.
00:01:35 [W] you need to collect the data.
00:01:38 [W] I'm a decentralised and in one place a common use case is to have a hardware or a piece of software that delivers a message some records from low and then centralize this information like in a database or a cloud service. So then you can perform your own data analysis.
00:01:53 [W] But when the data start to scaling up also, we need to scale up our logging pipeline.
00:02:01 [W] We need to scale up our nation layer and also the analysis get affected by this because now you have more data to process and sometimes timing performance is really important for any business.
00:02:14 [W] And also there are many challenges with this because data comes from different places.
00:02:22 [W] Alexis but when the data start to scaling up also we need to scale up our logging pipeline. We need to scale up our nation layer and also the analysis get affected by this
00:02:24 [W] Or any points from TCP UDP from the file system from the local machine or also through services like systemd or others.
00:02:32 [W] And before we can do data analysis, we need to of course compared take this data out from the source right compared to set up from maybe instructor format to a certain format performed at enrichment because sometimes we need to add some context
00:02:48 [W] This data to finally be able to the Nevis this data to our destination like a cloud service or your fancy database.
00:02:55 [W] And if we think about kubernative, I think the scenario is more complex kubernative is distributed environment your application lives in a pot and that pot is not alone in a node can have many pots together and a classic and have many notes.
00:03:12 [W] Leaps in a pot and that pot is not alone in a node can have many pots together and a classic and have many notes.
00:03:17 [W] So if your application is suggesting what poet and may or may be some different replicas.
00:03:18 [W] how do you correlate all this information back together in another is so you can do data analysis and that is one of the complexities of this.
00:03:27 [W] A common workflow for logging imagine that you have like a demo file which you have just one container this containers reading data to the standard output or some that error interface. Okay. So as an example, I have a table file on the
00:03:47 [W] Two dogs that just Generate random Apache log messages and at the end you get to some row messages like the one that you have the right with an IP address time, Sam and hold message from them from the HTTP protocol.
00:04:02 [W] But in kubernative or any containerized environment that message that was generated by the container is redirected most of the time to the file system. Sometimes handled by Journal D in a different way, but by default
00:04:19 [W] if I system to our containerd identification that has the pot name the name space in a normal log file that also not do not only contain the message but that message also comes with a bit of more context for
00:04:34 [W] Not message is encapsulated in a Json and this Jason gets the love the string name the timestamp.
00:04:42 [W] Then this information is not enough because we are missing the context and the context is where is this Kata was generated from which node which hosts a do we have any labels?
00:04:57 [W] Do we have any annotations that comes from any specific context?
00:05:03 [W] So this information alone is not useful enough.
00:05:05 [W] We need to correlate all this information together and because at the end with this information and then we want to have something more complete like this where we have all the kubernative context.
00:05:15 [W] The message for example have the host with this data was generated from the container name.
00:05:22 [W] They tow Katie because all this information is relevant to for us to be able to perform data analysis. Once we centralize the data in our database.
00:05:32 [W] And that is one a one of the solutions fluent bit fluentd is a local processor that is part of the fluid. The family we are proud to say that fluentd is a cncf supreme Under the Umbrella of fluently. And of course fully Apache License it
00:05:48 [W] In 2015 originally for embedded Linux, but then it quickly evolved for the cloud space and nowadays is almost everywhere.
00:06:04 [W] It's written in C language. And that's one of the main reason because it has a very low memory and CPU footprint. It has Black Market ector. We have more than 60 plugins available made by the community and these plugins allow us to collect
00:06:13 [W] Insoles from the into filtered data or send data out to different destinations.
00:06:18 [W] And fluentd internet is like a pipeline. You can consider that the data comes from an input. You can parse the data filter the data buffer the data to finally send its data out to a cloud service or your database.
00:06:35 [W] database. So the whole point about Logan's to take data from point A to point B, but in the middle due date of transformation and apply Auto logic that is necessary.
00:06:43 [W] in kubernative likely you deploy fluentd as a team monster, but also you can deploy the società but here the example we deploy effluent bit as a demon set running the note where is able to collect all the logs from the Note2 finally
00:06:58 [W] Mason back with the API server where the you can get information like labels annotations. So you final record can have all the information that is necessary for the data analysis.
00:07:14 [W] We're going to run a pretty quickly demo to see how this works.
00:07:18 [W] works. I'm going to share my screen.
00:07:20 [W] Some weeks ago.
00:07:28 [W] We released fluent back 1.5 one of the major air release of the fluentd area and it has a lot of improvements and also a lot of contributions from different companies one of the majors
00:07:38 [W] Or is the ability to have a more configurable and reliable networking functions?
00:07:51 [W] And before this film it was didn't have the option to send data through keep a light connections right now. But right now you can reuse keep a light connections. You can reuse all your TLS head checked. So of course you can send data
00:08:00 [W] Same data fastest this was not a need years ago, but nowadays it is sense people smoothly streaming data every second.
00:08:09 [W] Also, we have a full windows of the support version and on this version we have implemented the whole Windows service so you can run fluid and windows like a service also, the the plug-in to collect data from
00:08:25 [W] Has been highly improved in addition now, you can run through a bit for Windows on kubernative without any problem.
00:08:35 [W] did a couple of improvements. So if you're running a Windows on with coordinates Windows, you can run your pots and political Logs with linbit without any major issue.
00:08:46 [W] Also, the community has contributed a lot of ways to monitor a fluent bit.
00:08:55 [W] So right now we have in our documentation a couple of examples on how to run your own graph on a dashboard. So you can take advantage of the whole monitoring also from the same pipeline that you are working with in the part of the storage. We are
00:09:08 [W] Matrix to see how the data that is inside the pipe length is Flowing meaning how many chunks of data do I have in memory how many of them are in the file system so you can get a notion of how the data is Flowing also how much
00:09:24 [W] You same how much memory up in memory you consuming for all your data ingestion and delivery?
00:09:33 [W] As part of the Enterprise and politics of with it.
00:09:49 [W] We're happy to say that we have a new new connectors in pretty quick. So for example for the Amazon elastic service, we have a new plugin for the animal muscle Club. What was this going to talk more about it and also new Enterprise connected for
00:09:52 [W] services
00:09:55 [W] also need a small to have been working with Google to improve this Google stack driver connected influent bit, which is really in a sea, of course and we have many program for to consume kubernative resources latest special labels. You can append special keys
00:10:12 [W] reliable solution for most of the Google Cloud a customers
00:10:17 [W] as an overall status of the project in Json 2020 or half of this year right now.
00:10:26 [W] We're hitting more than a hundred million deployments. And this is just an insane grow of the projects for every day.
00:10:34 [W] We have more than 500,000 deployments from different classes and everywhere in the world.
00:10:39 [W] We don't have disability who or from where we just have you guys know that we are getting all this $1 every day. And of course this is thanks to the community and the traction that
00:10:52 [W] getting
00:10:53 [W] and in the Enterprise as I said that traction we have many companies use influent bait and suggesting fluent wait for the customers like a AWS Google Cloud Armstrong Milotic datadog DNA and all of them that you can see on these slides and many more companies that
00:11:07 [W] Course, they use fluent be privately for their own reasons.
00:11:12 [W] Now I'm going to pass the control to Wesley who's going to talk about his great work on the Amazon connectors.
00:11:23 [W] I wish we were all in Amsterdam, but oh well anyway, so as a word I've said I'm going to be talking about the work that I've done with it was plugins influent bit.
00:11:34 [W] I am the AWS Kuma maintainer of linbit.
00:11:38 [W] I maintain the interest portions of fluent bit and generally help out with the project as a whole.
00:11:39 [W] So last year I launched it was for fluent bit, which is the eight of us distribution of fluentd. It had fluentd bit bundled with a couple of external plugins written in go that's supported cloudbees logs Kinesis
00:11:55 [W] a streams
00:11:58 [W] So the reason why I wrote those external plugins and go the main reason was because so eight of us has its own authentication and credential and signing mechanisms.
00:12:15 [W] And there's a knative assess DK and go that supports those and there was not one in to see also in general, you know golang supports a very fast rapid development and so it allowed us to integrate with fluentd very quickly.
00:12:25 [W] So as I mentioned eight of us has its own like custom unique signing mechanism that we came up with to fit the scale of AWS.
00:12:43 [W] And there are many many sources that you can have for credentials depending upon where knative BS you running and so this was a blocker for me to contribute plugins to the core of linbit, but
00:12:53 [W] In Flint bit 1.5.
00:12:57 [W] There is a core library for eight hours authentication signing and credentials.
00:13:04 [W] So basically last year Eduardo and I met at Coop Khan in the nativist reinvent. We talked and then I took a long vacation in December and during that time I day and night worked writing code
00:13:17 [W] Create this Library custom Library influent bit to do authentication signing as you saw in the previous slide.
00:13:28 [W] It ended up being almost seven thousand lines of code. It was it was pretty huge but it's finally now released and it works and so I'm going to talk about what you can do with it.
00:13:41 [W] So the first thing that I did is using that Library, I took the elastic search plug in in in the core of fluentd it and I added support for
00:13:47 [W] For Amazon elastic search service.
00:13:58 [W] So Amazon elastic search service is you know, it's just a a hosted version of elastic search, but it has a toes authentication in front of it.
00:14:02 [W] So we need this Library. So as you can see here, there are new Fields. It was off it was region.
00:14:10 [W] You also turn TLS on and one thing I want to note is the port is usually 443 if you're using Amazon elastic search and for the host you do not put the
00:14:18 [W] To call you don't put the HTTP https in there.
00:14:24 [W] That's one one thing that sometimes people get confused with it first.
00:14:28 [W] And then also we're seeing here.
00:14:38 [W] There's also another field a diverse role aren't that lets you specify a im role that it will make an STS assume roll call on and so that you can use and Alternate role to put two elastic search
00:14:43 [W] Documentation if you want to learn more about that then so you know, as I said last year, I created a external plug in and go for cloud watch. I rewrote that plug in and contribute it to the core
00:14:59 [W] Which you can see here.
00:15:07 [W] This is a configuration for it supports all the same options. The only difference is that the name of the plug-in is cloudwatch underscore logs. The old plug-in was just Cloud watch.
00:15:15 [W] So that is the only difference.
00:15:20 [W] Otherwise, you can just very cleanly migrate between the two of them.
00:15:21 [W] So this new plug-in because it's in the core fluid bit. It's in C. It is more performant and it's not just a matter of see being more.
00:15:30 [W] And then go because go is really a pretty good language in terms of performance.
00:15:39 [W] It's partly that compiling go and see go together segoe compiler add some in efficiencies that you wouldn't get with pure C or pure go.
00:15:50 [W] It's also because influent bit plugins that are external that are in go cannot take advantage of the concurrency mechanisms that fluentd has to do asynchronous http.
00:16:00 [W] Requests but with this new cork or connector everything works very flawlessly so you can see here.
00:16:12 [W] There's a decent Improvement in the CPU usage, but what's really better and really bigger is the change in memory performance.
00:16:26 [W] It's also because influent bit plugins that are external that are in go cannot take advantage of the concurrency mechanisms that fluentd has to do asynchronous HTTP requests.
00:16:31 [W] but with this new cork or connector, everything works very flawlessly. So you can see here. There's a decent Improvement in the CPU usage, but what's really better and
00:16:33 [W] Really bigger is the change in memory performance.
00:16:34 [W] So you're potentially looking at a 3X Improvement in memory usage the that's that said that the new plug-in is using one. Third of the memory, even at fairly High load you could
00:16:37 [W] Using one third of the memory even at fairly High load you could be using less than 50 megabytes of memory.
00:16:41 [W] Even at MIT graph. You're seeing 20,000 log lines per second.
00:16:43 [W] So that's pretty substantial.
00:16:57 [W] We also have seen evidence that the the max throughput achievable with this new plug-in is significantly higher than the going plug-in. I had said that I was going to try to quantify that for this talk. Unfortunately things
00:16:58 [W] - I that for this talk, unfortunately things I got busy and I was not able to get hard numbers, but maybe maybe next year the future talk.
00:17:06 [W] we'll have more of that anyway, so long term things that I'm thinking of working on. So I'm I want to rewrite all of the go plugins in C and contributed them to the core of fluentd.
00:17:18 [W] So we have everything high performance and centralized in the float but code base then we can deprecated the go plugins Alias the names of the old plugin.
00:17:26 [W] As into the sea plug-in so that you can have a clean migration for all users from the go plug as the see plugins that can be entirely transparent because they support the exact same features the timeline for all of this is uncertain though.
00:17:41 [W] And as I say here don't quote me on it like the there's no guarantee. But this is what I'm thinking so also in fluentd 1.4. I built an ec2 instance metadata filter that adds a your instance ID and
00:17:53 [W] Plugins that can be entirely transparent because they support the exact same features the timeline for all of this is uncertain though.
00:17:54 [W] And as I say here don't quote me on it like the there's no guarantee. But this is what I'm thinking so also in fluentd 1.4. I built an ec2 instance metadata filter that adds a your instance ID and
00:17:56 [W] To your logs if you want it, I think in the future, maybe we'll expand that. I might also work on and ECS task / service / cluster metadata filter as well.
00:18:06 [W] We'll see and then in terms of outputs, I'm thinking eventually could maybe get all six of those things that you see there in the core of fluentd.
00:18:20 [W] So all of the existing ones also elastic search which are already launched. I'm now working honest.
00:18:23 [W] Three.
00:18:26 [W] I'm thinking maybe Amazon event.
00:18:29 [W] Bridge might be useful to I'm not sure interested in feedback on that.
00:18:36 [W] However, you want to send feedback to me issues or Twitter or whatever or this or the fluent slack but interested in see if people would be find that useful.
00:18:45 [W] So what am I working on right now? So right now I'm working on Amazon S3 output support. There's an issue for it on the fluentd.
00:18:53 [W] GitHub check that out if you have suggestions that let's see since it is in August that this talk is being streamed. Hopefully that will actually be nearly
00:19:09 [W] We'll be releasing fairly soon.
00:19:16 [W] We'll see our plan is sometime in September hopefully so I put a lot of thought into the S3 output.
00:19:27 [W] So the way that the fluent DS3 output works is it buffers files locally and then it uploads them all in one go to S3 and on that's okay, but it's not perfect
00:19:37 [W] A while your buffering on disk. There is this risk of data loss, right if linbit goes down or flinty goes down and you lose those files if you don't have a reliable disk like you maybe you're running in a more ephemeral via environment like a serverless
00:19:52 [W] You can't really trust the disk.
00:19:55 [W] It doesn't really work.
00:19:58 [W] So my goal is to make this S3 plug-in be more sort of like a streaming model.
00:20:07 [W] So it's going to use multi-part uploads.
00:20:08 [W] So the multi-part upload API basically lets you send data in very small chunks to S3. So you can break up a file into pieces and send it in chunks.
00:20:19 [W] But the thing is is it doesn't require that you actually have all of the file ready when you want to send it. You can as you get data stream up stream chunks to S3. And that's what you're my bad diagram is trying to show
00:20:33 [W] Is going into the bucket is that as fluent bit is getting data.
00:20:43 [W] It's adding a new piece to the bucket. And then once it reaches the file size that you want, it will tell us three to concatenate those together and create a final file.
00:20:50 [W] So basically the long and the short of it is that you're streaming to S3.
00:20:53 [W] You're not buffering more than a few megabytes of data locally at a time the amount of locally buffer data at any one point in time should be very low which
00:21:04 [W] reduces the chance of any sort of log loss from any sort of interruption or or fluentd goes down. You have to restart it.
00:21:19 [W] So here is an output definition that I'm thinking of as you can see here.
00:21:22 [W] You can set the file size that you want in S3 and remember note that like as I said, it will be streaming and uploading and little chunks. But the final file that you see will be whatever size you pick you can also specify
00:21:35 [W] Timeout so you can say like every 60 Minutes created a fly last three I can do that as well.
00:21:42 [W] I'm trying to come up with some interesting stuff some useful things on the key format where you can maybe have like used the
00:21:50 [W] Use the parts of the tag the syntax that you see there was introduced in the rewrite tag filter in fluentd 1.4.
00:21:59 [W] Check the documentation to see what I fly I come up with and if you have suggestions before it launches, please post them in the issue.
00:22:18 [W] So okay, if you are an AWS user and you want to get help with fluent bit, how you how you how do you go about it?
00:22:25 [W] We have really really high adoption of Flint bit.
00:22:28 [W] So we get a lot of questions and issues. The preferred mode is to open an issue on the AWS / 80 was for fluid bit repo.
00:22:36 [W] Because I have a bunch of folks at ABS who I have trained to understand float bit and we can all work on answering your questions.
00:22:47 [W] You can also try to contact me directly via slack or Twitter or on the core repo all of those are fine, but I think outside of GitHub your guarantee of response is much slower. I do not
00:22:58 [W] Later faster than a week often anyway, so another interesting thing that I want to talk about.
00:23:09 [W] This is kind of Switching gears.
00:23:11 [W] This is moving away from AWS.
00:23:15 [W] This is taking off my Enterprise hat and put it on my just pure open source hat recently. I have been working with the folks at the opentelemetry project. So opentelemetry is a new cncf
00:23:26 [W] It's an incubating project and its goal is to create a standard set of sdks and specifications for metrics and traces and maybe eventually logs to that. Basically all of the vendors
00:23:42 [W] You can instrument your application with these sdks that they'll produce and you won't have to worry about being locked into any sort of a monitoring service because the sdks will be supported by everything.
00:23:58 [W] So it's very exciting project.
00:24:05 [W] I'm very glad that exists and very happy to work with them the little bit that I have so they have recently started working on defining a log data model.
00:24:12 [W] Which might become maybe a standard schema for log data a standard kind of structured model for logs.
00:24:22 [W] That's its goal.
00:24:24 [W] Definitely.
00:24:29 [W] I do feel a little bit conservative but opentelemetry because it is an incubating project.
00:24:29 [W] will have to see if this specification especially for logs catches on it won't really actually be useful unless it's widely adopted and lots of vendors recognize the format.
00:24:42 [W] But I want to talk about what it might how this model works and what it might look like if we supported it in fluent bit.
00:24:58 [W] So I don't want to say I talked to the folks who worked on this model, but I can't take any credit for it.
00:25:02 [W] It was primarily created by an opentelemetry maintainer named tigran.
00:25:08 [W] If he is actually if he will be watching this talk, who knows I'll pray I'll try to tell him to watch it. Anyway, okay, so his the log data model that they came up with it has a couple different fields structured Fields.
00:25:25 [W] The goal is to standardize structured logging basically. So structure logging is great because it makes it easier to process and analyze your logs because the data's instruction format, but the ideal case would be that
00:25:39 [W] Endured way to structure them because if all if you structure your logs in different ways with different field names, you're still kind of in a bit of a mess and mass and so that's the goal.
00:25:51 [W] So you have a time stamp your next nanoseconds you have fields for correlation with traces and spans.
00:26:00 [W] You have a standard way to represent severity name for the log like an event name.
00:26:06 [W] you have a body which is for unstructured not machine readable, but human readable data you have
00:26:10 [W] It's which are for structured like key value pairs.
00:26:14 [W] that might be machine readable and part of the goal of this schema is to eventually create a standard key names for basically any type of data that you want might want to put in there. So I don't like kubernative metadata IP
00:26:28 [W] Certainly key value pairs that might be machine readable and part of the goal of this schema is to eventually create a standard key names for basically any type of data that you want might want to put in there so I don't like kubernative metadata.
00:26:30 [W] A standard field name that you always use for though, whatever metadata that you're adding to your logs and then the resource that's actually where the kubernetes metadata would go would be a standard way to with key value pairs represent.
00:26:44 [W] where a log came from so here's an example log. So as you can see here, there's the time stamp. Then there's a set of attributes. Like I said, these are key value pairs the ideas eventually have very
00:26:59 [W] very clearly defined semantic conventions for how you name those key pairs so that they are really uniform across, you know, all of your applications, then you have the
00:27:13 [W] Is where the log came from again? Same thing eventually, we want to create a very standard way of naming that naming all of those things and then we have a couple of field so we have a severity which has both a text description
00:27:29 [W] Numerical description and then that's about it.
00:27:41 [W] So what would it look like to support this influent bit?
00:27:49 [W] So this is something that we're thinking about we probably won't do it until later this year or maybe the beginning of next year because opentelemetry, like I said, it's incubating.
00:27:50 [W] It's extremely new this log data model was just created.
00:27:56 [W] I don't think it's really been adopted or used by anybody yet. So basically the kubernetes filter the easiest meditate
00:28:00 [W] He to metadata filter anything that adds metadata for where a log came from you'd want to add that to the resource in our tail plug-in influent bit.
00:28:15 [W] Normally, we encase a role all make it log message in a key called log that instead would be the body field because it's for unstructured data. If you parse that data, it would then become part of the attributes.
00:28:25 [W] that's that's how kind of work so if we decide to support this
00:28:31 [W] Bit and it catches on it's actually not a lot of work.
00:28:35 [W] I think in order and I can get it done quite quickly and that's exciting.
00:28:42 [W] So in some ways really for this opentelemetry log data model to catch on and really be useful.
00:28:47 [W] It's more about logging in your application. And that's something that opentelemetry eventually wants to do is integrate with existing logging libraries.
00:28:55 [W] Learn how to contribute to fluentd if you have some basic knowledge of how to write code in C, but you want to know how fluentd C code works.
00:29:11 [W] I wrote a whole entire guide to that in the repo check that out.
00:29:15 [W] Alrighty, so now we will go to QA, please ask any questions that you have for Eduardo and I thank you everyone.
00:29:21 [W] Who are you?
00:29:29 [W] Okay, so we have a question coming from David asking is the Lua filter will support modifying tax and not at the moment, but feel free to open an enhancement requests, but we have also other ways to
00:29:49 [W] So reducing it was nude.
00:29:54 [W] Sorry about that.
00:30:00 [W] So they have a questionable from David back window Louver filter support modifying tax.
00:30:02 [W] Not at the moment, but we have other ways to create records with the tax.
00:30:09 [W] We custom text. We right field there the string processing.
00:30:12 [W] Wesley you want to jump into the next one.
00:30:18 [W] Which one is the next one read it out for me?
00:30:25 [W] Newest we're doing some kind of random.
00:30:34 [W] Okay, I'm seeing are the slides available somewhere.
00:30:39 [W] I don't actually know the answer that that is a good question for the believe. We have somebody from the cncf here.
00:30:42 [W] Jennifer
00:30:45 [W] I think that's just nothing there at the moment.
00:30:55 [W] So I think that is let's to be available.
00:31:01 [W] Yeah, so I say, okay, so she's not she's not here on out of you.
00:31:03 [W] I dunno I'm not sure if this was related or helped. I knew that know that afterwards for a few minutes, at least we will be available in the to - koukin maintainer
00:31:17 [W] In the cncf slack and you can ask us questions there.
00:31:27 [W] Maybe we can maybe we'll have an answer for that.
00:31:28 [W] Okay.
00:31:31 [W] So next time think can it be used as an exporter for Prometheus.
00:31:35 [W] The answer is no I mean flip it has four logs.
00:31:36 [W] Prometheus is for metrics unless I'm missing something. I don't know.
00:31:40 [W] Not sure that with yeah. Well, he's both Nikki's for Prometheus, but although we don't ship dear.
00:31:47 [W] Yeah.
00:31:48 [W] Yeah.
00:31:48 [W] Yeah.
00:31:48 [W] Yeah.
00:31:49 [W] Yeah.
00:31:50 [W] Yeah. I'll see you suggest what I think yeah before we forget it Wordly.
00:31:53 [W] We have a hundred people in.
00:32:00 [W] So basically we have a demo on this presentation that for some we got a technical issue.
00:32:08 [W] if you watch another game, but if you watch the presentation again around minute five, you will get the demo abdomen about fluid bed. So what on demand in a
00:32:10 [W] Powers and that will be available.
00:32:12 [W] Okay.
00:32:17 [W] Yeah, and that demo I believe shows using Prometheus to monitor fluent bit right does it?
00:32:21 [W] No, he doesn't miss most about well the pipeline.
00:32:27 [W] Oh, well, okay well, but that definitely works and there's also a built-in Griffon a dashboard that we that the community created recently so go check that out.
00:32:40 [W] that out. That's in the repo.
00:32:42 [W] with word of water
00:32:46 [W] So Marcus framing thinking about protein environment. Is there any requirement tip for affluent big pot in terms of resource requests or limits?
00:33:03 [W] Well, you know, it depends of your load but I think that most of people deploy using 200 megabytes as a maximum but in general you don't need more than
00:33:07 [W] 50 megabytes of I think that Westland can add more tips on that.
00:33:11 [W] Yeah, it depends upon your exact level of usage in the plug-in using like I mentioned with the plugins that I built if you're using the older golang versions something maybe more like a hundred megabytes of memory is is probably a good safe number.
00:33:27 [W] I'd probably still actually sick stick with something in the range of a hundred hundred fifty.
00:33:39 [W] But with the most of the plugins the ones that are in the core that are in see you really should realistically they shouldn't be using more than 50 unless you're just getting extremely high high load.
00:33:47 [W] you're being told this is on the question of where are the slides the slides can be found in sched like the word schedule but
00:33:58 [W] So I actually don't know what that means.
00:34:06 [W] I've done a bad job of paying attention to how the coupon virtual works, but hopefully you guys will all in the audience wall now.
00:34:12 [W] Do you ever go man said CPU and memory limits while deploying fluent bit as demon said yes for any anything that is running coordinated. You have to set a limit like best practice Yeah. I'm seeing confluent buffer
00:34:30 [W] Target host for example elastic search is unreachable for a while.
00:34:35 [W] Yes, that is true.
00:34:37 [W] It has buffering capabilities same as fluentd. It works a little bit differently. If you Google for fluent bit buffering there's a whole section in the documentation that goes over it.
00:34:49 [W] I'm being told that okay.
00:34:55 [W] I'm being told that we only have like a few minutes left maybe looks like both want an interview this person whistling there.
00:35:01 [W] Yeah, okay, which will read it to me.
00:35:03 [W] Yeah using AWS fluent bit image with AWS Farragut doesn't allow including more config files.
00:35:13 [W] Are there any plans to support more flexible configuration with AWS Firelands?
00:35:16 [W] Yes, okay.
00:35:28 [W] So this is not really honestly fluentd fluentd question, but I'll still answer it. Anyway, I'm also the creator of Firelands in addition to being the editors fluentd maintainer.
00:35:31 [W] I basically do containerless gain at AWS.
00:35:32 [W] That's my speciality.
00:35:35 [W] We are working on that.
00:35:38 [W] We want to build some sort of better config mechanism in interest fargate for fluent bit.
00:35:46 [W] I cannot really pray. I'm not at Liberty, especially in this town.
00:35:47 [W] It could kinda promised any sort of timeline on that though, but eight of us has a public GitHub road map and you can go check that out and annoy us and beg for things there.
00:35:59 [W] We don't find it annoying.
00:36:02 [W] Prometheus, let's jump into slack because where I think that we are out of time at the moment.
00:36:12 [W] So thanks so much for joining this session.
00:36:15 [W] And as I said, we will have the demo available in a few hours if you want to re-watch this on demand.
00:36:19 [W] Thank you so much and enjoy the conference.
