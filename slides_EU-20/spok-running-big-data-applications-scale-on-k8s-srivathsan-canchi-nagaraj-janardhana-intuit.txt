SpoK - Running Big Data Applications @ Scale on K8s: TQEM-1752 - events@cncf.io - Thursday, August 20, 2020 11:17 AM - 197 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:02:44 [W] Hello, my name is Robertson and with me is nagaraj. Both of us are part of into Data platform.
00:03:01 [W] We are going to present spok into its implementation of spark on kubernative and how that has enabled us to scale our big data processing.
00:03:12 [W] to give a brief overview of
00:03:18 [W] To give a very brief overview of entered. We are the makers of Finance and Accounting Software such as TurboTax QuickBooks and meant our products serve the financial needs of consumers small businesses and those who are self-employed
00:03:35 [W] This mission is to power Prosperity around the world with AI driven expert platforms.
00:03:44 [W] that mission has never been more important than now.
00:03:46 [W] Powering into its Mission are the financial data sets from various into its products. All of this data is organized in into its data Lake which has upwards of seven petabyte-scale
00:04:05 [W] Of data in the lake and growing overall into its platform managers over 40 petabyte-scale data across our products.
00:04:15 [W] In order to process these datasets we leverage AWS emrs spending upwards of 7,500 emrs every day these emrs use over 50,000 ec2 instances purely
00:04:31 [W] Data processing purposes in addition to emrs. We also use other AWS services such as Sage maker Athena S3 and redshift and perform a majority of data processing using
00:04:46 [W] our Apache beam
00:04:47 [W] the data processing infrastructure is crucial to power into its data and mlperf forms and pipelines this part plus EMR ecosystem Powers the ingestion of data into the link curation
00:05:06 [W] Find data sets and exploration for analytical purposes furthermore. This infrastructure also powers mlperf from the same data sets processing of features training and evaluating of mlperf
00:05:22 [W] Inference for Protections in our products.
00:05:30 [W] These are the inferences that power the AI driven expert systems that into it as building.
00:05:34 [W] With such a large data processing footprint. It becomes crucial for optimizing the development and deployment life cycle of a data processing code in order to enable that we have a paved Road built around knative AWS
00:05:54 [W] us for data processing
00:05:56 [W] at the same time into it has also invested heavily on a paved Road for services development, which is based on kubernative Argo CD Argo workflows and other Cloud native Technologies.
00:06:11 [W] The service is paved Road also, encourages gitops and other modern development practices.
00:06:20 [W] We wanted to Leverage The Power of cloud native Technologies for data processing.
00:06:21 [W] And that's how spok was born?
00:06:27 [W] When while we were extensively using EMR and Ian we were very interested to integrate with the existing kubernative infrastructure in the organization.
00:06:47 [W] We liked containers / bootstraps scripts that we were used to in EMR world kubernative as a resource manager as compared to Ian.
00:06:56 [W] We also wanted a modern continuous delivery process for data processing jobs.
00:07:00 [W] jobs. Let's dig deep into each one of the above in the subsequent slides.
00:07:04 [W] Services within in-toto are deployed and managed on kubenetes into kubernative service shown. Some of the UI elements are shown here allows service developers to manage their service runtime and deployment
00:07:22 [W] Already uses kubernative at a large scale for running a majority of our online services here was an opportunity to have a single resource manager to manage both service and data processing workloads.
00:07:38 [W] This would help immensely in the management of infrastructure reducing costs reducing operational overhead on automation across the organization.
00:07:49 [W] Containers for data processing jobs are extremely useful because it helps data workers write their code once and deploy it across environments in a standardized and a very predictable way.
00:08:06 [W] This becomes extremely important in the cloud where your Dev environment and test environment and production environment are quite different.
00:08:11 [W] This approach combined with the declarative management or spok Jobs means no more complicated.
00:08:19 [W] She'll scripts to manage as you can see here. You can see the
00:08:24 [W] containers that are being deployed declaratively and our earlier systems where we use to manage these deployments using numerous shell scripts that were orchestrated via the bootstraps of emrs.
00:08:40 [W] The other big aspect is around scheduling.
00:08:48 [W] Whereas the arm is a specific scheduler for managing data processing jobs, which requires a lot of expertise and thorough knowledge earlier. It had taken us a long time to fine-tune Ian scheduler for the for its load
00:09:08 [W] Similarities to achieve maximum efficiency out of it given that it is quite complicated and it is hard to find experts for who are really good at Performance Tuning Ian becomes difficult to manage large.
00:09:24 [W] Solutions based on yarn
00:09:26 [W] so kubernative was extremely powerful choice for us because we could use the same scheduling Advanced scheduling capabilities of kubernative to run data processing workloads.
00:09:40 [W] The other really important aspect for any software development is the continuous deployment data processing jobs are no different. They need to have really well-defined predictable modern ci/cd pipelines
00:09:57 [W] Faster build and deploy Cycles. This improves the developer productivity and also improves the quality of production releases.
00:10:10 [W] Spok makes it easier to use the industry standard continuous delivery tooling for data processing workloads at Intuit. We use Argo CD as an operator to manage kubernative resources
00:10:21 [W] - delivery platform for data processing applications only this was very cumbersome due to the complicated shell scripts and multiple sets of imperative commands that we had to use in order to deploy the application.
00:10:36 [W] Monitor status here is a CD workflow taken from a real in production application where you can see the comet from the developer going all the way to production in a fully automated continuous
00:10:52 [W] To talk about the rest of the house spok was implemented on the internals of spok invite nagaraj.
00:11:04 [W] to continue with this presentation
00:11:07 [W] Thanks, Steve Watson.
00:11:13 [W] Thanks for the introduction.
00:11:16 [W] So this is a knative kubernative steady work flow that we have here.
00:11:27 [W] And this is an end to end view of what a data worker interacting with the data processing infrastructure experiences.
00:11:33 [W] The data worker does data exploration using jupyter notebooks and once happy with the results commits that to get then primarily using the gitops deployment model. This code is pushed to the
00:11:44 [W] The environments and for that we use Argo CD and Argo workloads very quiet.
00:11:51 [W] As part of this deployment of spark operator. We also have enabled higher meta store which stocks to a centralized High data store database to
00:12:07 [W] Target about the data is stored in S 3 which is into its data Lake.
00:12:13 [W] Along with that. We also have the spark history server deployment which manage this the history of the spark processing jobs and driving all of the spok processing within the kubernative is obviously the spark operator.
00:12:29 [W] Operator this one single installation of spark operator towards the entire cluster and it manages snyk space across all the nations across the cluster in future. We plan to host it outside of the kubernative
00:12:45 [W] manage spok jobs across multiple clusters
00:12:50 [W] the deployment model for data processing jobs.
00:13:00 [W] reuses the capabilities the centralized kubernative platform in the organization provides.
00:13:05 [W] the centralized kubernative platform in the organization provides some of the key capabilities such as Self Service name space creation and management the authentication and authorization into this namespaces the tools for alerting
00:13:20 [W] So sweet are launched within this namespaces.
00:13:24 [W] Stark operator is another deployment in one of these to manage the new spaces across the cluster.
00:13:32 [W] Next we will walk you through and use case which has been enabled using the new spark on kubernative infrastructure that we have talked about.
00:13:47 [W] QuickBooks is one of the flagship products of intuitive and transaction categorization is one of the key features within compose this transaction categorization helps users understand their banking transactions better.
00:14:03 [W] This categorization is done, right?
00:14:11 [W] This could provide critical insights into the earnings and spending patterns of the user and that can be a very handy information for the user and
00:14:18 [W] the challenges with categorization of features is a similar transaction could be of different category based on the user especially on the person of the user for example above is there is an abortion example of
00:14:36 [W] Implement transaction
00:14:38 [W] a contractor would categorize it as materials and supplies but a retailer might categorize it as repairs and maintenance.
00:14:48 [W] so
00:14:53 [W] in summary transaction categorization becomes a personalization problem to help with this the model needs to be trained to understand the categorization preferences of the user. This requires feature extraction per user based on their transactions.
00:15:09 [W] and under categorization history
00:15:14 [W] all this is done as a data processing job running on spark and this is an example of the the spok job.
00:15:25 [W] So it combines transactions and the user's Behavior to produce some features for that user.
00:15:33 [W] The previous feature extraction job for transaction categorization uses Argo CD for continuous deployment.
00:15:45 [W] this follows the gitops deployment model all the resources required for the job such as the spark roll the spark history server the actual schedule spark application jobs are managed as knative kubernative resources
00:16:00 [W] And the user's Behavior to produce some features for that user.
00:16:02 [W] The previous feature extraction job for transaction categorization uses Argo CD for continuous deployment.
00:16:04 [W] This follows the gitops deployment model all the resources required for the job such as the spark roll the spark history server the actual schedule spark application jobs are managed as knative kubernative resources
00:16:06 [W] CD make sure to keep them up-to-date with what's required or what's what the data scientist wants it to be.
00:16:11 [W] This simplifies infrastructure management as the data workers just need to specify the in a declarative.
00:16:26 [W] Um, well what it's to be done rather than having to deal with shell scripts or imperative commands on how things needs to be done.
00:16:30 [W] We had to build additional capabilities in addition to deploying spark operator within the kubernative cluster.
00:16:42 [W] Here are here are some of the learnings having gone through that Journey.
00:16:47 [W] We had to set up an heimat as tour service which connects to the centralized High meta store.
00:16:49 [W] Data package Hadoop AWS Jazz, which is not part of the standard Sparky mean.
00:16:55 [W] And we had to get it working with S3 a file protocol as to not interact with the object storage three.
00:17:05 [W] One of the critical challenges that we faced was getting spark to for Phi and Below to work with type 2 and latest Hadoop. AWS jazz is a dependency nightmare.
00:17:16 [W] And that's where the suggestion to move towards part 3 which has been recently released as most of the issues with working with Hadoop AWS and high has been fixed in that latest release.
00:17:30 [W] Of the important things like Dynamic allocations and external cephalic Shuffle service has been added in spok 3 and some of the critical pocketbooks or work related to park a has been fixed with the latest upgrade
00:17:52 [W] Below are some of the advantages we have observed running big data processing workloads on kubernative.
00:18:04 [W] The unified management of clusters across different types of workloads say service workloads and data processing workloads is a huge win.
00:18:14 [W] Container simplify dependency management and the data processing jobs. Also get to use all the tooling built around.
00:18:23 [W] The rest of the organization like artifactory ci/cd tools Etc.
00:18:29 [W] This also helps reduce cost as Opex and other resources are shared across multiple different kinds of workloads.
00:18:39 [W] Declarative specification of data processing jobs and infrastructure make it easier for data workers to specify and manage them especially for mlperf.
00:18:57 [W] These are some of the num numbers that we have run that we have seen with running spok in production.
00:19:12 [W] multiple different kinds of workloads
00:19:23 [W] decorative specification of data processing jobs and infrastructure make it easier for data workers to specify and manage them especially for mlperf.
00:19:27 [W] These are some of the num numbers that we have run that we have seen with running spok in production.
00:19:29 [W] They said around 30 percent of cost reduction due to better uses better views of cluster resources across different workloads.
00:19:35 [W] The Dell productivity has improved a lot because of the automated CD deployments in some cases.
00:19:36 [W] We are seeing it come down from week two a day from Dell to trot cycle.
00:19:38 [W] The operational overhead of maintaining this has reduced especially the security patches and the cluster upgrades since it is done as part of the entire Arc. This is no need for managing them separately as part of data work.
00:19:50 [W] And performance wise we are seeing putting an application running on the previous stack to on kubernative.
00:20:02 [W] We are able to get similar performance.
00:20:03 [W] I think there are we definitely see possibilities to tune the scheduler which would help us get further benefits in the future.
00:20:13 [W] Having built having worked on a spark operator for data processing workloads in future.
00:20:25 [W] We would like to build a custom mlperf to orchestrate mlperf flow and resources through different stages of MLF cycle such as feature engineering model training and model deployment.
00:20:37 [W] Yeah, hoping this operator will encapsulate and distract the nuances of the underlying infrastructure and runtime.
00:20:45 [W] as
00:20:48 [W] model training and model deployment
00:20:53 [W] We are hoping this operator will encapsulate and distract the nuances of the underlying infrastructure and runtime.
00:20:54 [W] as
00:20:54 [W] we also want to expose the manage a managed platform for self-serve of data processing jobs and their lifecycle management.
00:20:59 [W] Thanks for listening to us talk. If you would like to chat about any of the other things, please contact us at our emails.
00:21:13 [W] Hi, and she Watson from in-toto be talking about stock and running Big Data applications.
00:21:28 [W] It's kalon kubernative.
00:21:29 [W] Alright, before we before we end there you want to maybe do a little bit. We do some questions and answers after this. So maybe you just want to say we're going to take some questions and answers now and I'll do a quick edit now chop out the Dead Space here.
00:21:55 [W] I'm talking to you. That sound good.
00:21:58 [W] Sure.
00:22:01 [W] Please let us know if you have any questions will be glad to answer it like.
00:23:03 [W] Cost reduction was secured.
00:24:00 [W] Yeah. So the lesson there are multiple aspects to the cost one of us.
00:24:05 [W] For example EMR as a service has a cost of 25 percent additional surcharge on the underlying Hardware cost over and above the underlying Hardware cost which was straight away.
00:24:17 [W] We could avoid and also the operational expenditure around maintaining this clusters since within the organization we had
00:24:23 [W] add operationalized management of this clusters for rest of the services workloads.
00:24:52 [W] And also the additional EMR cost but we have we also see opportunities around scheduling basically, I mean optimizing the scheduler and stuff
00:25:09 [W] b****, we haven't done it yet, but we think will give us more benefits in the future.
00:25:15 [W] In addition to what nagaraj said the other reason why we are able to leverage constant realize cost advantages is because they're able to now join workloads together so we
00:25:32 [W] Occasions and other mly clothes in the same cluster, utilizing the same computer as well.
00:25:45 [W] So that gives us an overall cost of monetization.
00:25:50 [W] So we don't have like islands of capacity, which is what we used to have for with our EMR Solutions.
00:25:53 [W] So while 30% is 25% is a minimum cost advantage that we got and we have seen in some cases almost up to fifty percent of cost advantage.
00:26:06 [W] Another question how much effort was it to move from yarn to kubernative as a scheduler for your spok jobs from spok perspective.
00:26:24 [W] I think the effort was not much since spok supports at least the sparkly supports kubernative says a resource manager.
00:26:27 [W] I think it was around the rest of the infrastructure for example in our case is 3 verse of our data lake or over hdfs equivalent equivalent, and we had to do
00:26:36 [W] nice work to get it working with the Hadoop AWS jars because when you run an EMR, there is a Llamar file system which comes with it, which is spin, which is AWS for
00:26:51 [W] Cannot use with kubernative.
00:26:56 [W] So we had to switch back to the her S3 a file system, which is the open source equivalent of it.
00:27:08 [W] The other thing was Mr. Is a managed service which provides you things like high when stop rest of the stuff Hive server and type service to which
00:27:16 [W] In addition we use a centralized High matter store for managing the metadata.
00:27:23 [W] We had to build support for that.
00:27:31 [W] So I think this was a two big ones and then I think a lot of the pain pain has been eased with soir three with all this open source different open source components.
00:27:38 [W] yeah, just lifting and Shifting the existing application as is involved only this changes.
00:27:44 [W] I think yeah, there's a question from Steven which technical part of this migration Journey was the most complex and what more challenges did you face?
00:28:01 [W] I think as I said, I think it was mostly around the rest of the infrastructure like S3 High matter store and getting them to work with getting them getting all this big data infrastructure to move on to the kubernative old or so.
00:28:13 [W] challenging part for us
00:28:16 [W] There's a question in the chat.
00:28:27 [W] So all your data access by spok resides in S3 now.
00:28:28 [W] Yes, all the spark data is an S3 and we have the S3 a file system view which allows spark to access the data from S3.
00:28:42 [W] It's to add onto the effort question.
00:29:03 [W] I think it took us about six months start to end to actually create fully paved Road approach for around around spok where we were able to then
00:29:18 [W] cases and provide security controls and and so on a lot of a snag that said a lot of the work was around making spark and kubernative work with things like namespace
00:29:33 [W] Rules specifications and so on spark itself. We were able to get the spok application running in a few days. But the majority of the effort was spent on operationalizing the rest of the
00:29:50 [W] structure
00:29:52 [W] it's question to do experience network bottlenecks with S3.
00:30:07 [W] average maybe if empty
00:30:11 [W] yeah, so I think there are some challenges challenges with s3a initially but nothing specific to letter to bottlenecks.
00:30:26 [W] Obviously that I think the EMR there is lot of work going on in the S3 a file system related to at least on the commit side the different commit strategies.
00:30:37 [W] But yeah, we did not face much of the bottlenecks on the network bottlenecks.
00:30:42 [W] Question get on my life on using this park operator to deploy spok jobs on communities.
00:30:56 [W] Yes.
00:30:59 [W] We are using the spok operator from Google and we're using that to deploy spark on communities.
00:31:11 [W] We have a we have a local installation of the spok operator and this includes kubernative has its own controls around namespace security and so on so this
00:31:19 [W] But go straighten our cluster.
00:31:23 [W] There's another question on spok streaming.
00:31:32 [W] Are we running spok streaming jobs such as pack jobs.
00:31:36 [W] We are running both. We have both spok streaming jobs and Spark back chops running on this infrastructure.
00:31:42 [W] Majority are batch.
00:31:44 [W] There are a few streaming jobs as well in general for streaming into it uses beam Apache beam and we have a beam based streaming platform. So we try to direct most of the
00:31:56 [W] You mean use cases onto the beam platform.
00:32:01 [W] There are quite a few Legacy spot streaming applications, which are starting to move to this infrastructure.
00:32:10 [W] Question on as a data scientist data analyst what new things should they learn to work in the spok environment coming from the AWS EMR background?
00:32:37 [W] The good news is we as a platform team we've abstracted out a lot of the underlying spok infrastructure. So it is
00:32:46 [W] Question on as a data scientist data analyst what new things should they learn to work in the spok environment coming from the AWS EMR background?
00:32:52 [W] The good news is we as a platform team. We've abstracted out a lot of the underlying spok infrastructure. So it does
00:32:54 [W] The expectation from data scientists is that they are familiar with spark and they write really good spark code and after they commit this barcode the platform takes over and
00:33:02 [W] Familiar with spark and they write really good spark code and after they commit this barcode the platform takes over and the platform does the build and deployment in a standardized fashion for
00:33:07 [W] From a data scientist perspective.
00:33:14 [W] They don't really see the the underlying kubernative infrastructure and all the complexities that go around with it.
00:33:23 [W] So in addition to just having a spok application we had to build the ci/cd for the application so that we can provide data scientists and analysts with interface that was as close to
00:33:34 [W] That code on with that.
00:33:35 [W] So in addition to just having a spok application we had to build the ci/cd for the application so that we can provide a data scientist an analyst with the interface that was as close to
00:33:36 [W] Reno which is around spark and be so that they can be productive with that.
00:33:40 [W] Yeah, just to add on to that. I think once one of the big changes from going from EMR to on kubernative was the whole container Journey so which they were very familiar with
00:33:56 [W] Mr. Deploy their machine learning models.
00:34:05 [W] So now they could they can do the same thing for their feature engineering or data processing jobs.
00:34:14 [W] And I think since we have a reusing most of the stuff, which was already built in the organization, like ci/cd pipelines and build automation.
00:34:19 [W] So it's free.
00:34:21 [W] a fit seamlessly into that infrastructure, and they did not have to do much apart from
00:34:26 [W] Declaratively specifying in the real Mel what needs to be done to run their spok chops? Yeah.
00:34:32 [W] I think this question from Max be nice also on cover artists.
00:34:40 [W] Yes be nice also on kubernative. It runs on the entirely on the kubenetes infrastructure.
00:34:46 [W] We have been running with both samsa and Flink as back ends and we have again built a platform around it. So our customers who are a data scientist and unless they get
00:35:03 [W] The interface to write that beam code and the platform takes care of ci/cd and deploying it for them.
00:35:12 [W] It's a question from Sasha. Is it possible to get the slides?
00:35:22 [W] Definitely we will publish the slides on SlideShare.
00:35:24 [W] Of this equation from Max on how we manage the containers for spark. So the question is first part jocks. Are you preparing containerd for every job or do we have an Universal container which pulls the
00:36:39 [W] Containerd for every job or do we have a universal container which pulls the job jar?
00:36:42 [W] It's a little Nuance.
00:36:44 [W] So we have a spark base containerd which has all the spark related stuff. And then what we do is during the build process.
00:36:54 [W] We layer layer the spark job on top of that base container and we create a new container, which is Wordly.
00:37:04 [W] - and very specifically versioned and with the commit ID and that is used to propagate through our various environments like Dev test and production environments.
00:37:20 [W] Very specifically version and with the commit ID and that is used to propagate through our various environments like Dev test and production environments.
00:37:21 [W] So the answer is yes, there is a container for every job and this is built it is a the it's a thin layer, but it from the
00:37:33 [W] And this is where it is a the it's a thin layer, but it from the base container of spot.
00:37:36 [W] There are there is one there is another variant which we are using for Feature ization Feature processing in in the machine learning workflows where we interact with the feature store, which is
00:37:50 [W] In S3 and you need to run spark application to get the features from the features too. In that case. The feature store reader itself is a pre packaged application and we only pass it configurations, but majority
00:38:06 [W] Reader itself as a prepackaged application and we only pass it configurations. But majority use cases where the jobs will be will be built every commit gets built and gets version.
00:38:14 [W] Yeah, just to add on to that. I think this was a very pleasant experience also, for example, we take the open source base Park image and then this one level of overlay where we
00:38:31 [W] Against also, for example, we take the open source base Park image and then this one level of overlay where we add stuff like Hadoop AWS jars or specific jars required for meta store access and stuff like that.
00:38:38 [W] So that's one level of overlay and then that is released. And on top of that application teams can take that and just add their stuff on top of that.
00:38:52 [W] So I think it's because of the whole container thing. It's
00:38:54 [W] very easy to manage the dependency also and whenever there's a new version of a spark, we just the there's a central platform team or us who are responsible for releasing the base images.
00:39:08 [W] And then the application teams can take that and build their stuff on top of it.
00:39:10 [W] I want to go back to Stevens question on what new things they would have to learn to work in the spok environment coming from the EMR background. One of the things that we have done with this as we've actually reduced the things that a data
00:39:51 [W] Still learn as they don't need to learn about how to set up an EMR how to manage the infrastructure and right like terraformer clot formation scripts.
00:40:05 [W] So there's a lot of things that they don't need to do and instead they just need to focus on their spot code which has been the focus of spok.
00:40:09 [W] Okay, we just got a notification that we should be wrapping up.
00:40:16 [W] So thank you all for attending our Toc and this red great questions.
00:40:24 [W] Thank you so much.
00:40:28 [W] Love to connect then continue to continue the conversation.
00:40:30 [W] We will publish the slides and and share it in the appropriate forums.
00:40:37 [W] Thank you so much.
00:40:44 [W] And please do feel free to reach out to us for any questions and any any further conversations on this week like to collaborate.
00:40:49 [W] Yeah.
00:40:50 [W] sir.
00:40:51 [W] In the appropriate forums. So thank you so much. And please do feel free to reach out to us for any questions and many any further conversations on this week like to collaborate.
00:41:23 [W] Yeah.
00:41:23 [W] Thanks, sir.
