Intro: SIG Scalability: FKSP-1076 - events@cncf.io - Tuesday, August 18, 2020 8:27 AM - 70 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:02:07 [W] All right, welcome everyone to the sick School BT intro a few words about this because my name is Matt.
00:02:15 [W] I'm a software engineer working at Google at work.
00:02:18 [W] I work on Google container engine and I am six called a teacher wojtek who couldn't be here today. Unfortunately also works with me at Google and he is our six cubby detail if you should be during the Q&A session so we can ask him all
00:02:31 [W] Wojtek who couldn't be here today. Unfortunately also works with me at Google and he is our six cubby detail and if you should be during the Q&A session so we can ask him all those difficult questions that I'm not able to answer
00:02:35 [W] The questions that I'm not able to answer.
00:02:40 [W] All right, so tldr about six Cavite. What do we do?
00:02:47 [W] First of all we Define what is kubernative scalability you will see in a moment that it's not a easy thing actually Define and we also Define what are the goals for the scalability?
00:02:54 [W] We protect from scalability regressions and mostly by maintaining scalability and performance test Frameworks and based on that ci/cd.
00:03:01 [W] Three texts. And other than that we detect cavity bottlenecks and then drive performance improvements to get rid of them and last but not least. We keep kubernative scalable by coaching and Consulting kubernative Community
00:03:17 [W] Kirby tea and one important thing, please.
00:03:24 [W] Please do not confuse us with SQL to scaling. We share part of the name scale scaling. But other than that, these are two completely different six.
00:03:32 [W] Alright, so now let's try seeing why defining various quality is not an easy task.
00:03:41 [W] So if you ask the average kubernative user, what do you want in terms of scalability obvious answer is scalable Custer's but when you ask what does it really mean then usually people don't know what to say and that's
00:03:57 [W] And you discover. It is not an easy thing. Luckily.
00:04:04 [W] It's not that we are inventing the wheel here.
00:04:07 [W] It's called it is a non and really old Concept in computer science. And so we can take a look. For example how Wikipedia defines Cavite so Gideon says that scalability is the property of the system to handle a growing amount of work
00:04:19 [W] This system and actually that's very accurate definition, right?
00:04:24 [W] That's what we expect from a scalable systems that we can add more resources and then handle basically more loads more work.
00:04:38 [W] And in fact, this is only a theory these only Theory and is only ideal situation and no real system actually has this property of scalability defined this way.
00:04:46 [W] way why because in any system if you keep adding resources you can
00:04:51 [W] Tap them indefinitely.
00:04:54 [W] Eventually, you will hit some bottleneck and fix will start breaking eventually huge for example, some lock contention on or you will hit some database connection contention or network issues and you won't be able to add more resources the system
00:05:06 [W] This is exactly the same about kubernative scalability. And so in fact the scalability and our everyday work in six culpability is about discovering performance limits of a system and dragon improvements to to push these things.
00:05:21 [W] This is much more accurate definition.
00:05:26 [W] Alright, so there is also very often misconception about scalability people believe that kubernative scalability is only about number of nodes in the cluster on the left node of the left side of the slide.
00:05:36 [W] You can see some history of how many notes we supported in certain kubernative releases and can say okay. So as of now, we support five thousand nodes.
00:05:49 [W] These are all these resources that we can add to the system.
00:05:54 [W] And as we said we can add more because finger will start breaking and basically that's all right.
00:05:57 [W] That's another story. We can go home and the presentation.
00:06:01 [W] In fact, it's not end of the story just the beginning and we need to analyze scalability kubernative scalability in many many more dimensions and by many many more I mean hundreds if not thousands
00:06:17 [W] Starts breaking and basically, that's all right.
00:06:17 [W] that's end of the story. We can go home and the presentation.
00:06:18 [W] In fact, it's not end of the story is just the beginning and we need to analyze scalability kubernative scalability in many many more dimensions and by many many more I mean hundreds if not thousands
00:06:19 [W] Number of nodes in the cluster and it's just one of the dimensions.
00:06:28 [W] It's very important one, but just one of the dimensions other examples are namespaces number of Secrets number of services.
00:06:31 [W] I know size of Secrets or config max number of packets per service.
00:06:43 [W] that some virtual Dimensions like pot churn or like some other example of dimensions that's attached to other dimensions like a number of quotes Bernal and many many many more.
00:06:49 [W] Or and this is why it's called abilities difficult and hard especially kubernative scalability.
00:06:54 [W] We need to analyze this multi-dimensional space and to make things even worse those Dimensions they interact with each other.
00:07:04 [W] So for example, neither of these pieces it a number of posts.
00:07:10 [W] There is a dependence between them. If you create too many pots and you have only one main space most likely we run into some issues.
00:07:13 [W] Luckily.
00:07:15 [W] There is something that we call scalability envelope is a Subspace here.
00:07:17 [W] And it's a save some lives. If you are within this cavity envelope in cluster is happy so our task as Cisco ability is to make sure we are aware where exactly the safe zone is Discovery the envelope and we basically
00:07:32 [W] Information to the users so they know how much they can scale up their clusters and all right. So we more or less know the top-level overview of this cloudbees definition.
00:07:45 [W] Let's dig a bit more deeper into that and see how we can formalize this definition.
00:07:59 [W] Alright, so as you can imagine because of this multi-dimensional property of scalability and then defining and precisely Computing this cubby can develop is basically impossible.
00:08:03 [W] Possible. We have too many dimensions and too many values those Dimensions can take basically it grows.
00:08:10 [W] Exponentially.
00:08:12 [W] It's not possible to test all configurations and luckily we don't have to do that.
00:08:22 [W] We don't have to provide a very strict like mathematical definition of disability and below and we can approximate it by a set of limits and usually it is even better for the user because it's easy to understand
00:08:31 [W] When it's approximated by a set of limits, so what we say is that as long as your Caster will stay below 5,000 nodes and number of positive cluster will be less than 30 times number of nodes.
00:08:46 [W] You will have less than 10,000 Services.
00:08:49 [W] It will have less than 20 or 250 back and spur service and etc. Etc.
00:08:55 [W] Then your cluster should be okay.
00:08:57 [W] It should work fine.
00:08:59 [W] All right.
00:08:59 [W] box.
00:09:03 [W] What does it mean that your cluster will will work fine?
00:09:06 [W] We'll use happy or scales and we also need to be a bit more precise here. Especially if we want to have a framework where we can experiment and try pushing some limits if we push one limit we need to still be able to say whether cluster is ok or not. Right if after goes down
00:09:19 [W] The cluster Mac doesn't scale, but if it didn't go down basically just maybe some we noticed some performance degradation by 10 percent is still happy hard to say right?
00:09:34 [W] that's why you need more precise definitions. And again, luckily. We are not inventing it will we can base it on a concept that work pretty well in computer science.
00:09:44 [W] SLI service level indicator and SLO service level objective. You can think of SLI as a macstadium.
00:09:50 [W] Metric and SLO is this metric has some thresholds. For example APA called latency SLI and SLO.
00:09:57 [W] Is that a cake?
00:09:59 [W] All I can see is less than one second the slos this love that we have in scalability is are listed here.
00:10:13 [W] You can read more at the the link the bottom of the slide.
00:10:18 [W] And so the most important one is a Pico latency. We also have poddisruptionbudgets.
00:10:20 [W] Circle latency, we have a bunch of network plated solos like Network programming or network latency or DNS related latencies.
00:10:30 [W] All right, so that brings us to this definition like can make firmly say what does it mean that cluster of scales cluster skills, if all scalability slos are satisfied and to put everything together
00:10:46 [W] Framework is you promise we promise kind of framework.
00:10:55 [W] So if you promise to correctly configured cluster and then stay within the scalability limits then we promised that all of scalability slos will be satisfied. Meaning your cluster will scale it will be okay.
00:11:04 [W] Okay, and now based on this framework, we are running scalability test now, but before we describe Square beta test internal big before you run scoby t-test you need infrastructure for that and these like big part of the
00:11:19 [W] Work to maintain a normal disc ability test Frameworks and tools the most important one is cluster over to single sentence description is cluster older to is a bring your own young cluster
00:11:35 [W] And the motivation to create a stronger R2 was that before our tests were written in colic in an interactive way.
00:11:49 [W] So basically we had all these we need to called all these operations for creates deployment. Then create a config Maps then scale deployment of stay determined down create demon certain, etc. Etc.
00:11:58 [W] And like once it reaches certain scale or a complexity that is if we reach certain complexity, it began really hard to maintain and really hard to actually extend the test so weaveworks.
00:12:06 [W] His idea that scale like load testing could be could be much easier and ideally you just want to declaratively provide a states. That after should be.
00:12:17 [W] And then something should be closer to those States and that was the cluster other to does. So basically an input to Cluster other is a young file with a test and test can be set of States.
00:12:30 [W] So for example one state can be okay.
00:12:33 [W] I want to have cluster with 30 pots per hour note 10,000 Services some conflicts have some secrets and faster level being cluster to to that state and then I have States doing the test.
00:12:46 [W] I want to transition from State 1 to State 2 verse 8 to for example is 20 parts per notes, but some
00:12:51 [W] pretty much sets and Staples sets and controller will do that and during whole test after other measures that scalability as a thousand checks, whether they are satisfied and so basically in turn as the framework we have pushing to the Limit and then checking whether
00:13:02 [W] Those are satisfied in addition controller provides extra survivability of the cluster for debugging purposes is really important if something stops working because in general how we do actually it is not easy.
00:13:18 [W] So need all those tools like dashboards graphs metrics profile and Custer all their provides everything in addition in provides a bunch of extra features.
00:13:28 [W] We don't have time to describe all of them describe all of them. But if you are interested, then there is a link
00:13:34 [W] at the bottom of this type where you can learn more about Custer logger.
00:13:38 [W] Another important tool that we have is cute Mark tldr about Q. Mark is a cluster simulation tool created to make scalloped make scalability test cheaper and and
00:13:55 [W] To run like 5,000 note end-to-end tests. You need at least 5,000 RPMs for the nodes then some games for the master and then such tests can take like the fully old plaster to to search certain point the city it takes
00:14:11 [W] Like the fully old Caster to to search a certain point density. It takes a long time. You can take hours even more than 10 hours.
00:14:16 [W] So internal surfaces is really expensive and we created Q Mark to make doctor's cheaper so we can execute so we could execute them more often and the idea behind Q Mark is that when we are testing scalability of kubernative
00:14:30 [W] Mostly interested in control plane and we don't really care what happens on that on the Note locally. Obviously, we're interested in all the interaction between notes and pastures.
00:14:45 [W] So for example, all the requests that kubelet sends to kubenetes server or all the watches that Q proxy opens to the API server, etc, etc.
00:14:56 [W] Etc. And but we don't really care whether iptables return or not on the Node or whether certain container was around or not on the note and that's what basically could Mark does.
00:15:00 [W] So include Mark kubernative based on the concept of hollow note whole note is a set of three containers.
00:15:08 [W] Hollow kubelet.
00:15:10 [W] Hollow Cube proxy and follow.
00:15:10 [W] no problem detector. And as you can imagine I will explain using kubelet so Hollow Cube and is that kublr these are regular kubelet and but it has some parts of locked.
00:15:23 [W] So for example, there is no continuous runtime interface instead.
00:15:26 [W] There is no quick just pretends that containerd was wrong similarly for Q proxy. We don't we have mocked all the logic for
00:15:30 [W] I think maybe tables and because we don't really care about it and and all these Hollow notes. They connect to a real master called kubevirt master and together it forms a humor cluster and using it using this approach
00:15:45 [W] we are making basically our tests and about 10 times cheaper and we just a really great result and interesting one one note here Inception like think so as you can imagine to run
00:16:01 [W] Wasn't note Q Mark laughter. You need to run a lot of containers this case. It will be about 15 thousand containers.
00:16:11 [W] So, how do we do that?
00:16:12 [W] Right if we only had a tool that would allow us to run a lot of containers?
00:16:19 [W] And obviously we have such tool is it's called kubernative.
00:16:23 [W] And so every time there is a kublr cluster indeed.
00:16:24 [W] In fact, there are two clusters.
00:16:30 [W] have some real kubernative cluster where all the Halo Halo notes are wrong and then all these
00:16:32 [W] Hello node connect to who killed Mark master and these together forms a vacuum or cluster?
00:16:42 [W] All right, and I already mentioned observability in the bag ability.
00:16:49 [W] So either cluster older or other tools, they provided for example tasks Builder provides features, like profile Gathering From important kubernative components like a case Everett City keep control of manager we have
00:16:59 [W] All right, and I already mentioned observability in the bag ability.
00:17:00 [W] So either cluster older or other tools, they provided for example tasks Builder provides features, like profile Gathering From important kubernative components like a case Everett City keep control of manager we have
00:17:02 [W] Integration so cluster other is able to set up a Prometheus stack inside a cluster that is under test and the monitors basically everything is possible to Monitor and then we have this pre-configured graph on the dashboards with very a lot of interesting and very useful
00:17:15 [W] Sky began performance is very useful for debugging scalability regressions.
00:17:31 [W] And we also have perv - which basically reads data from from from periodic tests parses them and then dissipate graphs of how a certain metric change it over time in over over tufin runs and is also very useful to like
00:17:36 [W] Please wear something got broken.
00:17:39 [W] For example, her - can can can create a graph can show you a graph of memory usage of a pi server and we can see that okay starting from here. It doubled so most likely hear something changed in there is is a place where we start
00:17:53 [W] oceans and all right, so
00:17:58 [W] On top of this scabbia infrastructure, and we own a lot of scalability test using this infrastructure.
00:18:08 [W] And again, we could like spend at least 20 minutes describing all tests that we own.
00:18:15 [W] It's a it's a lot of tests.
00:18:17 [W] We don't have that much time.
00:18:18 [W] So I will keep it short.
00:18:21 [W] I think the main two groups of tests are preserved meats and periodic tests resubmits every time you send a TR against kubernative kubernative or kubernative.
00:18:33 [W] Perfect test. You will see some of this cavity drops running this be to make sure the spear doesn't introduce any accessibility.
00:18:38 [W] From obviously you cannot have all the tests as presubmit. For example, this large tests are too expensive too long to be run a piece of meat.
00:18:49 [W] So that's why we also need to really test and and we have a lot of periodic tests kinds of tests we have so we really like obviously end-to-end tests because like end-to-end tests are the best way to
00:19:02 [W] Sure disappear doesn't introduce any scalability regression. Obviously, you cannot have all the tasks resubmits. For example, this large tests are too expensive too long to be run a space helmet. So that's why we also need to really test and and
00:19:05 [W] His performance of such complicated system as kubernative.
00:19:13 [W] The performance tests are the cluster order to test and the correctness tests are the regular kubernative and to and correctness test like that check whether all like main functionalities work.
00:19:20 [W] We just run them.
00:19:26 [W] I really high scale for example five thousand notes to to see whether to make sure that all features that work on a smaller scale. They also are going to learn our skit Cube Mark that we already described then we have the benchmarks for
00:19:33 [W] examples of scheduler Benchmark where we just set up scheduler create a server and it's CD without creating a real cluster and under simulating some load or micro Benchmark durability test.
00:19:48 [W] Your some methods are like this important pieces of code inside kubernative called the base and one of the very important group of tests that we maintain our best color material is looking test.
00:19:59 [W] So basically if data is are not green if they don't pass it means that no new kubernative through this can be
00:20:03 [W] And these tests are hundred and five thousand note performance test and 5,000 note correctness test.
00:20:11 [W] Okay. So now all these tests are created in order to protect kubernative from scalability aggression.
00:20:27 [W] Looking test and so basically if they tests are not green if they don't pass it means that no new kubernative through this can be greased and these tests are hundred and five thousand node performance tests and 5,000 note correctness test.
00:20:37 [W] we D and we've seen regressions coming pretty much from everywhere from operating system changes of the sandbox through go like changes for example in the Marais occasion or like other curve along cyber risks that affect the scalability
00:20:51 [W] Dakota kubernative scold like controller scorable API Machinery, etc. Kubelet, etc.
00:20:59 [W] Pretty much everywhere.
00:21:03 [W] And what happened when we detect a regression, we usually debug it ourselves one. Once we know more or less what is going on, whether try to fix all styra cells or we we delegate to 2006 and a few examples
00:21:14 [W] What is going on?
00:21:16 [W] We either try to fix all styra cells or we we delegate to 2006 and a few examples of a recent regressions to give you some intuition about the scalability operations that we are facing and
00:21:22 [W] about the scalability operations that we are facing and so we had like to read an article and regressions in 113 that was changes to memory allocation
00:21:31 [W] completely broke the scalability of kubernative in Ghana 114 if I remember correctly was some timer changes so coral and ivory and they also like took us quite a long time working together with collecting to to debug and fix that
00:21:47 [W] And they also make took us quite a long time working dinner with collecting to to debug and fix that and another example is in 118 kubernative.
00:21:55 [W] We realized that cluster started to fail your ink master linkerd great. All right and protecting from school BT regressions just by looking at the test reacting to that.
00:22:06 [W] It's kind of a reactive think of course we want to do more than that.
00:22:09 [W] We do more than that.
00:22:10 [W] We also tried being proactive and impractical.
00:22:12 [W] The is Excavating the tekton discoverability bottlenecks and then driving improvements to get rid of them and again some examples here to give intuition.
00:22:27 [W] So recently we made some changes to watch cash. We other indexes there. We we change the world class size to be dynamic really good book marks where basically we
00:22:37 [W] Prevents to get rid of them again some examples here to give intuition.
00:22:39 [W] So recently we made some changes to watch cash.
00:22:39 [W] We other indexes there. We we change the watch case size to be dynamic probably what bookmarks where basically we built on the Virtual Console API that was added a few races ago changes in
00:22:43 [W] And that was added a few races ago changes in operator. Basically where we heavily optimized adding pot or deleting podcast and immutable secrets.
00:22:53 [W] So new and much more scalable API for secret and antique maps and many many more.
00:22:56 [W] Okay, and last but not least. We are also involved in kubernative community Consulting coaching about scalability and performance.
00:23:11 [W] One of the most important things here is the scalability approval process this part of the production Readiness review.
00:23:15 [W] So basically starting in the one making plus every cup will have this mandatory section with questions about scalability and performance.
00:23:23 [W] This is to make sure that outer of a cap is aware of potential scalability or performance issues.
00:23:25 [W] Design phase we do a lot of ad hoc. Scale testing example here is protein furnace.
00:23:40 [W] So basically other teams they work on the large changes in kubernative certain features like protein furnace and we help them with testing the prototypes to make sure they work as they should in terms of scalability and performance and also Consulting as I said being
00:23:47 [W] In discussions about performance about scalability and example that I really like the idea that I really like is consistent reads from Cash Cab.
00:23:58 [W] We stood here.
00:24:04 [W] Alright, and that was pretty much everything just one more slide about how to find us here are links to our homepage our slack Channel. I will mailing list.
00:24:11 [W] We also have these public meetings every second Thursday bi-weekly meetings. I encourage you to join and at the bottom of the slider like to
00:24:18 [W] To issue list that we tagged as help wanted and so feel free to check them. If you see anything interesting there feel free to assign and do it. Will really appreciate.
00:24:32 [W] All right, and that was everything so now it's time for the questions. Thank you.
00:24:39 [W] All right, Ingram room specific API calls to the control plane and given that I was presenting.
00:26:53 [W] I will that vitess cover till to answer this one.
00:26:54 [W] So basically the idea behind Cube Marcus not really to mock the API calls, but rather mock the underlying behavior of some components.
00:27:08 [W] So in particular like as an example, we are mocking like the calls to to contain a runtime.
00:27:23 [W] So so instead of like calling Docker to explicitly do something when the with the containers like starting them or stopping or whatever. We are we are.
00:27:25 [W] Replacing the that with like and mock implementation of Docker or container runtime interface in general that just claims its it does that but didn't really do anything didn't
00:27:40 [W] We are replacing the that with like and mock implementation of Docker or container runtime interface in general that just claims its it does that but didn't really do anything didn't
00:27:41 [W] We are not really mocking this the API calls per se like we are using the regular API server like the master part of the control plane. So in particular
00:27:56 [W] Server and it's CD that are involved in in.
00:28:00 [W] In answering API color like referee replying to API call our regular ones. What you can do is like you can install like your own web hooks as you can do in regular kubernative, but it's not really probably
00:28:17 [W] Yeah, I think there is one.
00:28:25 [W] There's another question.
00:28:26 [W] possible to run Halo cubelet without Cube Mark in the Life cluster for example in life cluster.
00:28:33 [W] But I wish do you want to answer that or yet? Why not?
00:28:38 [W] So technically it's possible.
00:28:52 [W] Hallo kublr is just a container so you can run it you can configure it. Like if you have all the like secrets and keys to connect to your your your master, then there is like nothing wrong with like it's possible
00:28:54 [W] Hello kublr is just a container so we can run it you can configure it. Like if you have all the like secrets and keys to connect to your your your master. Then there is like nothing wrong with like it's possible to connect
00:28:56 [W] People that follow kubelet the machine is like why do you want to do that?
00:29:06 [W] Right? So there's a question because as vitess keptn as I said like the in Hollow kubelet and the container runtime interface, this is smoked.
00:29:09 [W] basically Hollow kublr will not run any containers and the purpose of running calculate is just to reduce costs of performance tests that the in general you can do whatever you want with all equipment.
00:29:19 [W] So I can add one more thing to it.
00:29:24 [W] I've seen examples of people doing that actually and the one of the example is when you want to when you want generally want to run Cube Mark, but you also want to run real nodes where you would like real
00:29:37 [W] Or something else that this that should be that is not running on the master, but it's running like as a pod in the cluster. So if you want that those pots to be really running they need to be running on
00:29:54 [W] Running on the hollow nodes, but but you can like mix your cluster with like hello notes and those real notes and for example paint paint the hollow notes and ensure that
00:30:11 [W] Um paint the hollow notes and ensure that real parts will be running on the on those three out notes and all the other like test pots will be running can be running
00:30:19 [W] Running on the on those three out notes and all the other like test pots will be running can be running anywhere.
00:30:20 [W] So yes, there are in general. There are people that are doing that.
00:30:24 [W] All right. It looks like we don't have any more questions if there are any questions I will be available on slack.
00:31:17 [W] So feel free to ask the questions there.
00:31:23 [W] Yep, and this is it. So, thank you everyone for joining us today and hope to see you around either on six cavities public meeting on our slack or on some GitHub issues.
00:31:31 [W] you. Thank you very much.
00:31:36 [W] To ask the questions there.
00:31:50 [W] Yep, and this is it. So, thank you everyone for joining us today, and hope to see you around either on six cavities public meeting on our starboard and some GitHub issues.
00:31:50 [W] Thank you.
00:31:51 [W] Thank you very much.
