Zero Database Downtime with etcd-operator: HVZV-4111 - events@cncf.io - Tuesday, August 18, 2020 12:23 PM - 54 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:02:08 [W] hi, my name is
00:02:12 [W] Hi, my name is Carlos house key and I am presenting with Cody Gua, sir. Today. We will illustrate how entity operator automate said City database management and eliminates database downtime at CD is the primary
00:07:25 [W] Eddie's and holds all the kubernative cluster state
00:07:30 [W] internally at IBM our first entity deployment model for kudo clusters consisted of deploying and managing entity instances on VMS using ansible.
00:07:42 [W] As we scale the number of kubernative clusters we manage this deployment model became a bottleneck to the scale. We could operate at due to the amount of engineering hours. It took to maintain this automation suite at scale have any of y'all
00:07:58 [W] Going away for vacation for a week and coming back to an inbox of a thousand plus emails that you have to process and respond to that's the same feeling our engineer as we're experiencing as the various database creation and administration tasks.
00:08:14 [W] On a weekly basis in addition deployment and upgrade times of NCD clusters took on average one to two hours to complete which increase the total time it took to get a kubernative.
00:08:30 [W] Functional we needed to re-architect the way we executed Ed City cluster deployments in order to scale the amount of kubernative clusters.
00:08:42 [W] We could manage moving to an entity deployment model centered around at to the operator a lot of us to eliminate these bottlenecks to put it in perspective.
00:08:56 [W] We manage a thousand times more at CD clusters with RSD operator based solution then
00:09:00 [W] Our ansible based solution in addition over the past year. We have spent significantly less engineering hours maintaining all at CD clusters managed by it to the operator compared to those managed by our
00:09:15 [W] Now, let's illustrate the power of entity Operator by walking through a live demo of the entity cluster deployment process.
00:09:26 [W] A quick note for those that have downloaded the slide deck the number in the top left-hand corner of my screen corresponds to the slide number that has the content. We are currently covering and will be updated throughout the demo.
00:09:44 [W] To create a 3-node highly available.
00:09:52 [W] Multi AZ entity cluster managed by Ed CD operator. We first need to define the database that entity operator is going to create for us in a language.
00:10:00 [W] understands that language is the exedy cluster custom resource definition. The crd provides XD operator with structured data that outlines all the requirements for
00:10:15 [W] Or the entity cluster which allows it to the operator to take action toward creating a cluster that meets all those requirements.
00:10:26 [W] The process is conceptually similar to that of an engineer working at GitHub issue that outlines the requirements for creating a three node at CD cluster.
00:10:34 [W] I have an example of that here.
00:10:37 [W] Once the engineer has read the requirements he or she will begin executing actions toward achieving the desired result like provisioning three VMS and then installing and configuring Ed CD on each individual
00:10:55 [W] I have an example of that here.
00:10:55 [W] Once the engineer has read the requirements he or she will begin executing actions toward achieving the desired result like provisioning three VMS and then installing and configuring entity on each individual VM to
00:10:57 [W] Turning quaestor the engineer will provide status updates in the issue that will track their progress on the task and ultimately closed the issue when the task is completed.
00:11:11 [W] The GitHub issue is the same idea as the entity cluster crd in the engineer is the Ed CD operator for all the non engineers in the room.
00:11:23 [W] Think of a person Gathering three items in a grocery list at the grocery store the
00:11:27 [W] Person is going to read the piece of paper outlining the items and take three discrete actions that consists of finding each item in placing it in the cart.
00:11:38 [W] The grocery list is the structured data in this situation and the person Gathering the items is the operator.
00:11:46 [W] The key point in these analogies is enough information needs to be communicated to the operator in order for he or she to properly deliver the result.
00:11:56 [W] A lot of the same data you saw in the GitHub issue that the engineer worked is going to appear in the Ed City cluster cri-o resource.
00:12:03 [W] There's a few major sections of the NCD crd to highlight first is the size and version fields which are going to control the size and version that the entity cluster is deployed with.
00:12:20 [W] In order for he or she to properly deliver the result a lot of the same data you saw in the GitHub issue that the engineer worked is going to appear in the Ed City cluster cri-o resource.
00:12:30 [W] There's a few major sections of the NCD crd to highlight first is the size and version fields which are going to control the size and version that the entity cluster is deployed with.
00:12:33 [W] The next section are the scheduling rules which are going to build upon kublr scheduling logic to ensure the pods that run individual entity instances and up on three different nodes and three different availability zones.
00:12:39 [W] Lastly there are customizations to the configuration of individual and CD instances including the supported Cipher Suites.
00:12:50 [W] We are going to apply this entity cluster custom resource definition into the cluster.
00:12:58 [W] Which SD operator will then read to to execute and create the database and to the operator will execute the following series of logic Loops after it detects the creation of the crd.
00:13:13 [W] In the first logic Loop and to the operator has been told to create a cluster of three entity instances at version 342 and currently has no existing instances Etsy operator will create one at CD instance.
00:13:31 [W] To initialize the cluster and give it time to get healthy as the operator will also update the CRT to reflect what it has accomplished so far.
00:13:41 [W] Now the next logic Loop starts Ser prayer has been told to create a cluster of three entity instances at version 3 for 2 and has one healthy instance.
00:13:54 [W] It's the operator will call into the NCD cluster and configure it to add a new member.
00:14:06 [W] Then it will spin up an entity instance that will function as that member and give the cluster time to stabilize.
00:14:10 [W] Now the final logic Loop starts that will execute an action.
00:14:30 [W] It's Ade operator has been told to create at CD cluster of 302 the instances at version 3 for 2 and has two healthy instances.
00:14:31 [W] It's the operator will call into the entity cluster and configure it to add a new member.
00:14:44 [W] Then it will spin up an entity instance that will function as that member and give the cluster time to stabilize.
00:14:45 [W] Finally XD operator has been told to create a cluster of three at City instances at version 3 for 2 and has three iot three healthy instances.
00:15:02 [W] work is finished and now it takes some much needed time off puts on a VR headset and watches the tulips in Amsterdam safely from its home.
00:15:09 [W] At this point we have accomplished our goal of creating a functioning multi Z at TD cluster that is ready to serve request.
00:15:18 [W] VR headset and watches the tulips in Amsterdam safely from its home.
00:15:28 [W] At this point we have accomplished our goal of creating a functioning multi easy and CD cluster that is ready to serve requests.
00:15:28 [W] In this theoretical situation the cluster is put into production use and results in down time if it goes down.
00:15:29 [W] The cluster is also under strict compliance guidelines that the organization managing it has to me security vulnerabilities can be disclosed at any time that impact the HD version.
00:15:44 [W] We are currently running.
00:15:49 [W] This is just one of the many reasons why we need to have a system that automates the entire entity cluster upgrade process without any downtime.
00:15:54 [W] I see operator makes this process as simple as one command an update to the version field in the entity cluster cre source.
00:16:03 [W] After we apply this change into our environment.
00:16:10 [W] SED operator will detect that on the next reconciliation Loop to begin the upgrade process. The upgrade process will consist of the following logic loops.
00:16:23 [W] In the first logic Loop and to the operator has been told to create a cluster of three at CD instances at version 343 and has three healthy instances running 342 SE operator will patch the image of one at CD instance to be
00:16:44 [W] And then give the cluster time to stabilize the execution of this logic Loop can be identified when a pod in the entity cluster registers a restart.
00:16:55 [W] The execution of the next logic Loop can be identified by the restarts of to Ed CD pods.
00:17:04 [W] in this loop, it's the operator has been told to create a cluster of three at CD and Sciences at version 3 for 3 and as three healthy instances to running three four two and one running
00:17:19 [W] I'd buy the restarts of to at CD pods.
00:17:20 [W] in this loop, it's the operator has been told to create a cluster of three at CD and Sciences at version 3 for 3 and as three healthy instances to running three four two and one running
00:17:22 [W] Match the image of one at CD instance to be 343 and then give the cluster time to stabilize.
00:17:28 [W] The execution of the final logic Loop that executes an action can be identified by the restarts of three and CD pods.
00:17:40 [W] Ezio operator has been told to create a cluster of three entity instances at version 3 for 3, it has three healthy instances one running 342 and to running three for three Etsy operator will patch the image of one of
00:17:58 [W] Be 343 and then give the cost our time to get fully healthy.
00:18:05 [W] At this point the cluster is fully healthy since all the pods are into of to ready State and we have completed the upgrade process.
00:18:16 [W] You'll note throughout the entire upgrade process.
00:18:22 [W] We had two pods ready to serve traffic at any given time which ensures writes and reads can be processed through out the entire upgrade to recap.
00:18:31 [W] We were able to set up a HMO ta Z @ C cluster in under four minutes and execute an entire database upgrade with zero downtime in under four minutes as well now, I'll turn it over to Cody to
00:18:44 [W] It's the operator can provide in disaster scenarios.
00:18:50 [W] Thank you Tyler.
00:18:58 [W] My name is Cody glosser. The next thing I'm going to talk about is what makes SED highly available and go through a few different Disaster Recovery scenarios.
00:19:00 [W] It's Tyler pointed out before yet CD cluster custom resource definition has potting rules that prevent pause from being scheduled into the same Zone and host based off the node labels, as you can see in the image here the pot of finiti rules
00:19:14 [W] specifies the zone
00:19:16 [W] let's go through a few different scenarios to help visualize High availability in this scenario or SED cluster pods are scheduled across three zones Dell 10 Dell 12 and gal 13.
00:19:30 [W] Each Zone has for worker nodes.
00:19:35 [W] Now, let's say we have a power outage and we lose nodes and Dell 10 including the note that the SD cluster poddisruptionbudgets.
00:19:37 [W] Ci/cd operator will detect the Pod is dead in the actual peers does not equal the desired peers.
00:19:45 [W] So I'll attempt to schedule a pot will look for a running and ready node and Dell tend to be able to schedule too.
00:19:49 [W] As you can see in this example the new pod came up on worker 1 until 10:00. Now. Let's do the same scenario interacting directly with the kubernative cluster.
00:19:59 [W] As you can see here.
00:20:05 [W] I have the same setup with 4 nodes and Dell 12 for and Dell 13 and 4 and Dell 10 Additionally you can see ci/cd pods are scheduled to a node in each zone now.
00:20:16 [W] now. We're going to simulate as own failure and Dell 10 on these two nodes.
00:20:19 [W] With the watch on the SED pods, you will see that the STD pot starts to go into a terminating state.
00:20:26 [W] And then eventually ci/cd operator spin back up a new healthy poddisruptionbudgets.
00:20:34 [W] Happens if all nodes go down in Dell 10.
00:20:52 [W] We either have the option to open up a new zone or wait until we get a healthy node in doubt n up while we were at two or three pots get City cluster is still healthy and achieves Quorum.
00:20:59 [W] So there's no outage to the instance if we were to lose another zone or SED poddisruptionbudgets.
00:21:04 [W] Back to our kubernative cluster you'll demonstrate the scenario as you can see.
00:21:10 [W] We have all the way up to the pods back up and running next. We will simulate killing all nodes in the zone.
00:21:14 [W] With this watch up, you'll see that the LCD patan goes into a terminating State as you can see all the notes below and all ten of entered his scheduling disabled State and it will eventually hit our not ready State and you had to be potted could spun up
00:21:33 [W] new node becomes healthy in downtown
00:21:37 [W] Describing the pending poddisruptionbudgets is zero. Twelve notes are available before nodes unschedulable and eight nodes.
00:21:57 [W] do not meet our potted Fern de T / anti Virginia rules for our next scenario.
00:22:06 [W] what happens if we break quorum quorum is the minimum number of members of an STD clustered necessary to have it in a functioning State Quorum is achieved from having majority of PODS and it running and healthy state in our case is two or three running pots as you can see in this scenario
00:22:14 [W] For nose and Dell 10 and a note and Dell 12 that has there are other ci/cd Padre en it with only one at Sea pod running.
00:22:27 [W] The data is still being persisted in the Pod and is in a read only state to the XD operator instances.
00:22:32 [W] If we were to lose the final pot and Dell 12 will not be able to recover the data and manual recovery is needed to recover from the most recent backup. Now, let's go back to our kubernative cluster and demonstrate both of these cases and losing quorum.
00:22:41 [W] As you can see they have City cluster is 303 running pods.
00:22:50 [W] Now, let me write some data to the NCD database.
00:22:52 [W] Let's put in kuchen 2020 is the value and demo as the key and let me simulate failure and Dell 10 by killing all the nodes meds own and one node in Dell 12.
00:23:02 [W] As you can see to the three pots have gone into a terminating State the third running poddisruptionbudgets 8 to allow us to take a backup of the most recent and CD data. Now, let's try to read and write to the STD database again.
00:23:17 [W] Let me write the value lost data to the key demo.
00:23:23 [W] And as you can see, I'm getting a context deadline exceeded in let me try to just read the key demo.
00:23:36 [W] in context deadline exceeded error
00:23:47 [W] with this one at CD pot up.
00:23:52 [W] Let's attempt at backup and restore.
00:23:56 [W] So now we need to construct an Etsy backup custom resource definition.
00:23:58 [W] You can see an example of that here.
00:24:02 [W] The sections highlighted in blue is information on where this backup will be stored contains the backup location the path and the credentials to get to that location.
00:24:13 [W] The section highlighted in red is the information on how the SED backup operator can contact the NCD instance. This some point is SED headless kubernative service endpoint for this.
00:24:19 [W] CD cluster back to the kubernative cluster.
00:24:25 [W] Let's apply the FCC backup resource and wait for it to succeed.
00:24:26 [W] You can see that X exceeded it by looking at the status field of the NCD backup resource.
00:24:39 [W] Now that ci/cd data is backed up. We can restore the SD cluster without losing any of our data before we do a restore. Let's recover our nodes.
00:24:43 [W] Now we need to construct and that's the restore custom resource definition example of that can be found here section highlighted in blue is the information on where the backup is stored notice how this is the same path we used for the entity backup resource
00:25:01 [W] Before we do a restore. Let's recover our notes.
00:25:02 [W] Now we need to construct an SD restore custom resource definition an example of that can be found here section highlighted in blue is information on where the backup is stored notice how this is the same path. We used for the SD backup resource
00:25:04 [W] red is the name of that City backup resource that the data will be copied to
00:25:07 [W] back to the kubernative cluster. Now, let's apply the XD restore resource and wait for all the NCD pods to come back up.
00:25:15 [W] Now that the pods are back up and running.
00:25:33 [W] Let's try and read that key from Etsy confirm.
00:25:34 [W] We do not lose any data.
00:25:35 [W] So again, I'm going to read from Key demo.
00:25:40 [W] And we can see that Coop con 2020 is still written in that field.
00:25:49 [W] Jaden's still intact and everything is back to functioning normally for a last scenario scenario for we lose all our at CD pause and break Quorum as you can see here.
00:26:01 [W] We lost all our nodes and Dell 10 and one note in Dell 12 and another node in Dell 13 back to the kubernative cluster as you can see the ad C cluster is back to through three pods crying now, let's write data the SUV Bay database again.
00:26:12 [W] It's put in lost data.
00:26:13 [W] And write it to the key demo.
00:26:16 [W] Let me simulate a complete failure in Dell 10 by killing all the nodes one node and Dell 12 and OneNote and out 13.
00:26:25 [W] Now that you can see all the pots have died.
00:26:34 [W] As you can see in the status of the backup, we were unable to complete it because he has to be back up operator is not able to contact the LCD cluster.
00:26:44 [W] The next step to do is to restore from a most recent backup the one we stayed in the previous scenario, but first, let's recover our nodes again.
00:26:54 [W] Now that the nose are recovered.
00:26:58 [W] Let's apply the same Ed CD restore resource again and wait for the all the NCD pods to come back up.
00:27:03 [W] Now that all the pods dropped and running.
00:27:18 [W] let's try and read from a Rat CD database.
00:27:20 [W] Let's read the key demo again.
00:27:21 [W] And as you can see the most recent data, we wrote lost data is no longer there and it was reverted back to coupon 2020 that completes my demo and NCD High availability and disaster recovery.
00:27:34 [W] So does anybody have any questions you can ask them here in the text terminal?
00:27:53 [W] Looks like we've answered all of them so far. So if anybody has any new questions we can talk about them here.
00:28:01 [W] And today to learn more about the system and we thoroughly enjoyed talking you through it.
00:28:10 [W] Now there's a second page of questions Tyler.
00:28:47 [W] Okay.
00:28:49 [W] Are you posting all the commands used somewhere?
00:28:57 [W] Yeah, we're going to post the slides and then I think coupons going to upload this and on YouTube so you can review it.
00:29:00 [W] later as well.
00:29:03 [W] The slide should be posted somewhere in the resources.
00:29:11 [W] We can post them in the answer again.
00:29:12 [W] There's also a repo to that has pizza that automation Cody that I'll link down on public GitHub.
00:29:22 [W] All right.
00:29:24 [W] Let me check page 2.
00:29:25 [W] She's somewhere.
00:29:35 [W] Yeah, we're going to post the slides and then I think coupons going to upload this and on YouTube so you can review it.
00:29:35 [W] it later as well.
00:29:36 [W] The slide should be posted somewhere in the resources.
00:29:37 [W] We can post them in the answer again.
00:29:37 [W] There's also repoed to that has pizza that automation Cody that I'll link down on public GitHub.
00:29:38 [W] All right.
00:29:40 [W] Let me check page 2.
00:29:40 [W] Provide your pal commands and are scrip additionally in the kit repo.
00:29:43 [W] Can look into doing that.
00:29:45 [W] Tyler do you know the version of that CD that we used?
00:30:13 [W] I do arm version 3 4 2 and 4 3 4 3 on the upgrades and in your portion as well.
00:30:22 [W] As for the questions about the deprecated Coral s public repo. We are working with the previous owners to update on our ship on the repo. So for all those questions,
00:30:51 [W] So what we will be doing.
00:30:54 [W] None of that CD that we used.
00:31:05 [W] I do on version 3 4 2 and 4 3 4 3 on the upgrades and in your portion as well.
00:31:05 [W] As for the questions about the deprecated Coral s public repo. We are working with the previous owners to update ownership on the repo. So for all those questions,
00:31:08 [W] So we'll be doing.
00:31:08 [W] And then also just for those commands in the same repo with those slides.
00:31:37 [W] it has the has some of the automation that we use to provision the Clusters and pieces like that just to reiterate that just go saw that was a couple questions there.
00:31:42 [W] Cool while I give another minute or so for questions and if there is no more will in the broadcast here appreciate everybody for joining.
00:32:29 [W] Cool, give another minute or so for questions. And if there is no more will in the broadcast here appreciate everybody for joining.
00:32:36 [W] Second and Cody. Thank you.
00:32:36 [W] Thank you all so much for your time.
00:32:40 [W] Hope you have a wonderful rest of coupon as well and looking forward to speaking with you on in the future.
00:32:42 [W] Encrypting your SD volumes tell you one answer that.
00:32:57 [W] Great question. It depends on the provide.
00:33:07 [W] Well, it sort of depends on how your provider sets up empty jars on. The backing storage is an empty directory and coup in some providers that will be encrypted and others on that won't so in our specific example.
00:33:18 [W] The data was encrypted.
00:33:20 [W] But it's all based off how your provider handles an empty directory if that makes sense.
