Is There a Place For Distributed Storage For AI/ML on Kubernetes?: PBER-0698 - events@cncf.io - Wednesday, August 19, 2020 10:46 AM - 1191 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:04:08 [W] Hello everyone. Welcome to Virtual Amsterdam.
00:09:00 [W] And is there a place for distributed storage in a IML on kubernative? He's
00:09:01 [W] Hello everyone. Welcome to Virtual Amsterdam. And is there a place for distributed storage in a I am L on kubernative has?
00:09:14 [W] I'm Diane fundament my principal software engineer at red hat and today I'm speaking with Kyle Bader who is a principal software architect for cloud storage and data services at Red Hat.
00:09:15 [W] So next slide, please.
00:09:16 [W] We want to know is there a place for distributed storage for a IML workloads on kubernative ease?
00:09:26 [W] What do you think?
00:09:29 [W] I would say?
00:09:29 [W] Yes, absolutely.
00:09:34 [W] And why why were we interested in doing this?
00:09:42 [W] Well, there was you know kind of kind of stemmed from a number of conversations that that we had internally as we were thinking about distributed storage and the implications of machine learning.
00:09:52 [W] And kind of boils down to you know, many of you probably have an environment.
00:10:00 [W] Well, if you're lucky having environment looks maybe something kind of like this, right and you know kubernative is awesome. So you've chosen to use kubernative. He's to manage this environment and you know, most the time there
00:10:14 [W] storage and data services at Red Hat
00:10:14 [W] so next slide, please.
00:10:14 [W] We want to know is there a place for distributed storage for a IML workloads on kubernative ease?
00:10:15 [W] What do you think?
00:10:15 [W] I would say?
00:10:15 [W] Yes, absolutely.
00:10:16 [W] And why why were we interested in doing this?
00:10:17 [W] Well, there was a kind of kind of stemmed from a number of conversations that that we had internally as we were, you know, thinking about distributed storage and the implications of machine learning.
00:10:19 [W] And kind of boils down to you know, many of you probably have an environment.
00:10:24 [W] Well, if you're lucky having environment looks maybe something kind of like this, right and you know kubernative is awesome. So you've chosen to use kubernative. He's to manage this environment and you know, most of the time there
00:10:25 [W] Some number of General General compute general-purpose compute nodes that are you know, running your applications and your storage and so on and so forth.
00:10:27 [W] And then the the lucky part is if you are able to have the special purpose compute with additional gpus in order to kind of accelerate your machine learning workloads so that you can you know,
00:10:38 [W] If finished, you know both for any jobs and inference jobs a little bit more expeditiously and collectively, you know, this this kubernative cluster that we're looking at here represents a fairly significant capital investment.
00:10:54 [W] So we want to make sure that that it's used as efficiently as possible.
00:10:58 [W] Right?
00:11:00 [W] We want to maximize their utility from this this investment.
00:11:02 [W] So kubernative to the rescue, right, you know at a very simple level right kubernative ziz a scheduler does a lot of other things in addition to scheduling but one of the things that it's here to help you with is packing bins
00:11:17 [W] As efficiently as possible, right?
00:11:18 [W] We want to maximize their utility from this this investment.
00:11:19 [W] So kubernative to the rescue, right, you know at a very simple level right kubernative ziz a scheduler does a lot of other things and in addition to scheduling but one of the things that it's here to help you with is packing bins.
00:11:20 [W] Nodes is these bins and instead of having Dimensions like length and height and depth those notes have Dimensions like amount of CPU amount of memory.
00:11:32 [W] How much storage space do they have on their local storage devices how much I owe can those storage devices handle?
00:11:36 [W] And we want to we want to effectively Phil Phil these bins with pause and we want to fill them as full as possible without them kind of impacting each other in order to maximize the utility of that cluster.
00:11:52 [W] So we schedule a pot.
00:11:55 [W] And that pot is going to consume some of the CPU some memory, you know, it'll generate some amount of storage I/O and consume some store some amount of storage space.
00:12:11 [W] And you know, if you if you if you draw this up on onto this kind of, you know, multi dimensional object here you get a shape and so each podcast has its own shape and you know what with all these different shapes of
00:12:21 [W] You know fill up fill up the each of these Dimensions as full as possible.
00:12:32 [W] So as you add another POD at represented here by this kind of dotted purple line.
00:12:38 [W] we want to we see that it's taking up, you know some additional amount of CPU additional amount of memory, you know, it's going to consume some more storage IO and takes more storage space.
00:12:46 [W] Now some of you might have noticed a problem here and and I'll kind of pointed out is that we kind of exhausted the amount of CPU resources here.
00:12:58 [W] And what does that mean?
00:12:58 [W] Why is that?
00:12:59 [W] Why is that bad?
00:12:59 [W] Right?
00:13:00 [W] we want to maximize usage of it.
00:13:04 [W] And so we want to use the ball the CPU don't we? Well, yes we do, but we don't want to trap we don't want to trap the other resources. Basically, we don't have it want to have this like stranded capacity in terms of memory and
00:13:18 [W] Unused storage space and unused storage I/O, we want to use those want to use those efficiently as possible to so one of the problems that you want to solve with the scheduler is to avoid trapped capacity.
00:13:29 [W] One way you can help avoid traffic capacity is by having fewer Dimensions that are constraints.
00:13:40 [W] Have ability to be scheduled in more places and there's less constraints on scheduling them such that it's easier to fill your bins maximally and in turn maximize the utility of that that
00:14:06 [W] Maximally and in turn maximize the utility of that that significant capital investment.
00:14:08 [W] Now the other you know time when distributed storage comes into the picture is when you need more storage space for a particular application then then then you have on any given node in your cluster, right?
00:14:25 [W] So if you if you need to have or if you have data that's larger than a node then then you're going to have to spread that data over multiple nodes and guess what distributed storage helps helps you do that, right? So by kind of virtualizing the storage
00:14:36 [W] fighting a bunch of disks together and then using various, you know, software-defined storage methodologies, you know, it can it can kind of create this either virtualized block or file system storage or object storage and then
00:14:52 [W] You know larger data sets that a pod is interacting with than than what you could get from any individual node.
00:15:00 [W] The other problem that might present itself is okay you have available compute right and and you want to want to use it up but the data already exists on another pot right because you're using local storage somewhere.
00:15:18 [W] Your data is over here.
00:15:21 [W] So now you're kind of you know, sure you could have something that's going to make that that data over over on the right host be remotely accessible right? Maybe I suppose your NFS or something to the notary were on the left,
00:15:34 [W] Bring that that NFS or eyes because Gateway is going to consume so now right and so you can't kind of ad hoc provision it and it would really just be better if we just kind of that is distributed storage system that was in place and and and gave you more flexibility
00:15:50 [W] gave kubernetes more flexibility in terms of scheduling
00:15:53 [W] So this is and this is not you know, none of this is unique to kubernative or machine learning workloads. But but it does apply equally to machine whirring workloads.
00:16:09 [W] And so it was kind of in this thinking this this context that we we started to ask the questions about distributed storage and machine learning.
00:16:18 [W] but a very simplistic right kind of truncated version of the machine learning life cycle right now, of course, there's going to be something on the left that's you know, the applications are there sensors or whatever that's generating the data
00:16:34 [W] More on the right like training is not the end all be. All right, you're going to have to take that model and you know wired into intelligent application and do something useful with it, you know shopping cart suggested products or
00:16:50 [W] Something useful with it, you know shopping cart suggested products or detecting fraud or so on and so forth.
00:16:59 [W] But for the purposes of this discussion here, we've kind of simplified it to okay. So, you know, there's there's to the time to value is something that we want to shrink and some of
00:17:07 [W] They need to be performed before we can realize value from from that data is we're probably gonna have to prepare it Wrangle it together in some sort of way.
00:17:21 [W] And then without distributed storage we have to move the data.
00:17:29 [W] So if we will, you know, if it's going to take a really long time to run it on the general purpose computer that was processed on the general-purpose compute it we're gonna have to move it to the special purpose to compute in order to you know, finish it in a sort of reasonable time for him.
00:17:39 [W] And then finally, once the data is there then we can actually start training the model against that data. And then once it's once it's completed, we're kind of on to the next next phases of the life cycle though
00:17:52 [W] And then finally, once the data is there then we can actually start training the model against that data. And then once it's once it's completed, we're kind of on to the next next phases of the life cycle.
00:17:54 [W] All right.
00:17:54 [W] Can we can we compress this?
00:17:55 [W] can we can we compress and have get to that value faster?
00:18:01 [W] Can we reduce the time that it takes to get to that value?
00:18:03 [W] and if you start if you implement kind of some sort of distributed storage, you can potentially kind of compress the data moving and model training into into the same kind of into the same space and
00:18:18 [W] And perhaps perhaps that does mean that it inflates the time that the actual model train takes but you you know, you you potentially save yourself that data moving step stage now, of course an astute reader might go. Okay.
00:18:34 [W] The data is not remote and the GPU node is accessing over to the network.
00:18:40 [W] Isn't it?
00:18:46 [W] Kind of like the data is moving and the answer is yes, right. So it's going to be kind of over the course of the model training its kind of incrementally or either incrementally over the course of the whole training is grabbing some of the data it needs to keep the keep the gpus busy or it kind of
00:18:55 [W] And then and then and then processes it from there, right? So yeah, this is this is this is what we want, right? We want to we want to see if we can compress that time to value by kind of collapsing these two stages by using remote storage
00:19:11 [W] It Institute right in some sort of distributed file system or object store or something like that.
00:19:18 [W] So the other interesting thing that comes about with distributed storage is it helps a lot with with with parallelism right?
00:19:35 [W] And you know, just kind of simple simple, you know explanation of parent level parallelism much many of you already know this but you know, if I'm digging a ditch and I'm alone and I break it up into you know, six work units, you know, it's going to take me longer right
00:19:47 [W] ditch faster, I can recruit two of my best friends that are that are equally hard workers as me and we will each compute compute complete two units of work and that b**** will be Doug faster right and that's great
00:20:02 [W] The reduction in time that it takes to dig that ditch is speed up, right and that's the kind of the computer science see what they call that is is speed up.
00:20:17 [W] So can we additionally instead in addition to kind of collapsing those to the compute stage and or the the training stage and the data moving stage, but we also affect speed up to further reduce
00:20:29 [W] Any time potentially compensating for or going to further than we would be able to otherwise so we have this model training and collapse model training and data moving step.
00:20:45 [W] Can we can we paralyzed can we add additional GPU hosts?
00:20:57 [W] Of course, this would be after we've we've stuffed as many gpus is NVIDIA has engineered to be able to fit into a single system, but you know once you once you
00:21:01 [W] Exhausted the ability to scale up. You want to sneak your oh, you're only left with the ability to scale out.
00:21:10 [W] So if we scale out can we can we further reduce the amount of pain that takes to do this kind of collapsed didn't moving model training step.
00:21:20 [W] So it's in the in the in like what this in mind like this is the overall kind of like Theory Theory level thinking behind you know, the question about you know, is is there a
00:21:28 [W] Further reduce the amount of time it takes to do this kind of collapsed didn't moving model training step.
00:21:30 [W] So it's in the in the in like what this in mind like this is the overall kind of like Theory Theory level thinking behind you know, the question about you know, is it is there a place
00:21:31 [W] Age for AI NL in in kubernative, right?
00:21:38 [W] So next we needed to we kind of needed to test this hypothesis.
00:21:41 [W] And so without further Ado. I'll let Diane talk a little bit about a benchmark that we use for some testing called mlperf.
00:21:52 [W] Next slide please.
00:21:56 [W] Hey, so as Kyle said we're going to run an experiment and we need models to train for this experiment and we're going to train them with data that's sitting on distributed storage and then compare that to data that's sitting on
00:22:16 [W] didn't do and so we decided to choose the mlperf French training benchmarks to run and mlperf is like an emerging industry standard for
00:22:32 [W] a standard Benchmark that is designed to be used as a Level Playing Field for comparing machine learning infrastructure.
00:22:45 [W] There are all these companies that are shown here that are contributors to mlperf and University researchers as well.
00:22:54 [W] So we thought this would be a great choice of a real world Benchmark that the entire test the entire system to run in our are distributed storageos.
00:23:03 [W] experiment
00:23:04 [W] Next slide please.
00:23:06 [W] The mlperf benchmarks solve real world real world problems. As I said such as computer vision problems, like identifying images identifying objects and images and
00:23:24 [W] Language to another so we are going to run the SSD Benchmark which consists of a model this SSD resonate with SSD with resnet 34.
00:23:40 [W] And uses the cocoa data set which is in publicly available data set.
00:23:52 [W] So then we also are going to run the Transformer model and that uses the WMT public data set.
00:24:02 [W] So I'll talk a little bit about a little bit about each data set and all the code that we used here is open source, you can go to mlperf that word and see the code that we ran.
00:24:10 [W] And also we're running it'll prefers and 0.6 training in our experiments next slide, please.
00:24:18 [W] This is an example of object detection and segmentation.
00:24:25 [W] And you can see that the objects have been detected and they've been labeled and then the segmentation is happened also on the guy in the bicycle. So
00:24:38 [W] These sorts of applications use things like the cocoa dataset which we're using and each year. There are competitions to improve the state of the art in this
00:24:54 [W] And ends segmentation that we ran one of these benchmarks next slide, please.
00:25:01 [W] Little bit about the cocoa dataset which that we used.
00:25:10 [W] It's the common object in context data set and it has over 300 thousand images. It allows researchers and
00:25:24 [W] Allows researchers and people are doing benchmarks like me are to take real world problems of finding these objects in these scenes and you know outlining them and labeling them and seeing how your mom does
00:25:35 [W] Accuracy accuracy level that you achieve.
00:25:44 [W] So the data set was created by Cornell and a joint project with Microsoft originally and if you'd like to check out the data set online next slide, please.
00:25:52 [W] You can download it and you can also peruse around the data set and search for things like in this case. You can submit queries at on the cocoa Explorer where you want to see all the scenes with
00:26:07 [W] Bicycles and cars all the images or whatever combination of the objects it recognizes. You can choose them here and just
00:26:17 [W] Check the data set or if it's got the sort of objects that you're looking to do recognize in your images next slide, please.
00:26:28 [W] So pal talked a bit about why we want you to use we wanted to get that speed up that I was talking about. And if these benchmarks we use neural networks that rely heavily on linear algebra things like the Matrix
00:26:46 [W] Like that and GPS are extremely efficient at doing that Matrix math.
00:26:54 [W] They are great number crunchers for Matrix math.
00:26:59 [W] And so you could get you can get thousands of times speed up It's Not Unusual say get a 3000 times speed up. If you run like a matrix large Matrix Multiplied on a GPU instead of the CPU.
00:27:13 [W] So what we're trying to do here is use these gpus to reduce the training time make those Matrix and Matrix operations go faster, and we also have to feed those gpus with data and that's
00:27:28 [W] Testing, you know, are we getting the data from this distributed file system there fast enough to keep the DPS busy.
00:27:36 [W] So many times can take days or weeks and if you made a slight error in your code, and then you figure that out a week later, then your turnaround time the work time for the data scientist or the researcher
00:27:51 [W] Not reasonable, so we want to shrink down that time to train so that you can iterate more make more changes for your model quicker next slide, please.
00:28:04 [W] In mlperf, we will be using python. There are many languages that you can use lists are Java you could choose other languages to write of neural network application, but
00:28:23 [W] is that you can use lists are Java you could choose other languages to write of neural network application, but the benchmarks that we are going to be writing use Python and there's a good reason why
00:28:28 [W] Going to be writing use Python and there's a good reason why many data scientists prefer Python, and it's because you have access to libraries and I'm just listing for year that are very useful when writing your own network. So
00:28:39 [W] There's the arrow tensorflow Karis and Pie torch and today in our example.
00:28:46 [W] We'll be using pi torch.
00:28:56 [W] Another really nice thing about using python is that in your python code? You not only have access to in our case to the pie charts library, but you also have access to numpy and you can move these multi-dimensional
00:29:01 [W] Back and forth between between numpy and high torch very easily. The API is very easy to work with and I think that's why researchers and programmers like it so much next slide, please.
00:29:16 [W] So just a little bit more about pie torches open source was created by Facebook was initially intended as a replacement for numpy.
00:29:30 [W] That use gpus now Polly doesn't Empire runs on CPUs, I torch Pennsylvania gpus, but just minor modifications to your code.
00:29:44 [W] You can make your multi-dimensional arrays in numpy run on gpus when you're using I torch so
00:29:56 [W] Patricia's created to does that it was easier to build neural networks use something called tensors which are really just multi-dimensional arrays imperative ultimate dimensional arrays that run on gpus and
00:30:11 [W] I mean that when you execute as you go down through your code and you execute a line that uses a tensor did the computation is performed immediately when you hit that line of code
00:30:27 [W] and you execute a line that uses a tensor did the computation is performed immediately when you hit that line of code and not all implementations are like that the first release of tensorflow
00:30:33 [W] Not all implementations are like that. The first release of tensorflow was not imperative. They have now switched to the imperative model because it's so popular and intends to go to Kudo. They also use the
00:30:43 [W] Hutch, so in pie torch, you can write on CPUs or GPU use you can switch back and forth easily some minor changes in your code and you get the benefit of light changes to your code allow you to
00:30:59 [W] that's by using gpus and
00:31:04 [W] Next slide please.
00:31:06 [W] So we were preparing our benchmarks to run on the cluster that we created the kubernative cluster that we created in AWS. We first containerized mlperf benchmark.
00:31:24 [W] We created a Docker file for it. And in our dockerfile, we used invidious version of Pi torch. We built it from Source then we added the scripts that read the envelope art Benchmark and
00:31:40 [W] created that
00:31:42 [W] painter image wished it to Quay I/O just has a repository online repository and then we pulled it into AWS when we actually ran this job on our cluster and I'll show you the
00:31:58 [W] Yeah next that we use to hold that image next slide, please.
00:32:04 [W] So this is actually a different image that we're pulling that not in this case.
00:32:17 [W] We were pulling a coup de tat your ad but in our case we pull the image for the mlperf meshmark, but what I'm showing here is that you have some llamo that kubernative use
00:32:25 [W] It's and schedules you onto the correct worker node based on the resources you're asking for so here. I'm asking you for for gpus which is what we had in our cluster and AWS and kubermatic is Will and open shaped.
00:32:41 [W] Schedule that poddisruptionbudgets that has the Jeep.
00:32:47 [W] So what do we see?
00:32:53 [W] what happened when the rubber hit the pavement out of this turnout? So as a before before we had before I ever talked about, you know what we see
00:33:07 [W] Give some just splash up some details about the environment so that if people want to recreate this sort of work, they can they can do so they have kind of the laundry list of versioning on the left there. We used openshift containerd platform.
00:33:23 [W] 4.5 which is, you know maps to
00:33:53 [W] we use tanks and Toleration to ensure that only the Storage storage workloads are running on them.
00:34:03 [W] They had a couple ssds each and then the workers are are relatively not not particularly relevant here because all of the workload that we were running or the one workloads that we were writing. Whereas basically the different pipe I torch
00:34:18 [W] tests and those had those GPU constraints that and so that made sure that they were scheduled to the P-38 Excel that we had provisioned which has a for of for of that Nvidia
00:34:34 [W] Excel that we had provisioned which has four of four of that Nvidia V100. They each have 16 gigs of memory age.
00:34:38 [W] We use ec2 for it to run our cluster uswest to region and then we Spread spread both the compute at the Masters storage and the worker nodes across
00:34:52 [W] Spread both the compute at the Masters storage and the worker nodes across different availability zones for fault tolerance and but of course because we only had one GPU worker. It was it wasn't a single availability Zone.
00:35:00 [W] So the first Benchmark that we ran is an object depends detection Benchmark called SSD and you can see here that you can see the software stack that we ran.
00:35:19 [W] We you know, just like we just have described a minute ago.
00:35:28 [W] ago. And so we're using Python and Pie torch and Kudo and on top of that we're running open shipped and
00:35:35 [W] Stop. So and of course red hat or less is the operating system that were running and you can see that we have a training time where the data was sitting on local SSD. So we say local SSD we
00:35:50 [W] You know just like we just have described a minute ago. And there were using Python and I torch and Kudo and on top of that we're running open shipped
00:35:52 [W] The same worker node where the be 100 su-100 gpus are and so we had a time of 45 minutes 92 seconds.
00:36:04 [W] It's point nine two minutes actually and then we had training time.
00:36:12 [W] We using south of forty five point seven eight minutes so you can see there is essentially no difference here at the differences in definitely noise at this point.
00:36:22 [W] You could have run it again and it could have been faster with Seth. So that was an excellent outcome. And it shows that yes, it's reasonable to use distributed storage for these training benchmark.
00:36:32 [W] Next slide please.
00:36:36 [W] That's good news, right?
00:36:37 [W] We're not we're not completely crazy.
00:36:38 [W] Hila great. Great.
00:36:52 [W] What was the other one that we ran? It was so each random the translation Benchmark which translates from German to English and English to German.
00:36:54 [W] is a natural language processing Benchmark, and it was only about a 4% difference here in our timings slightly over for .01% I think.
00:37:05 [W] So again, we you know, put the training data on the local node and tested it and got a timing out of sixty two point nine one minutes and then to compare side-by-side.
00:37:19 [W] He put the training data and stuff and got a timing of 65.4 three minutes.
00:37:31 [W] So again, this is a great experiment and we got a good result.
00:37:32 [W] So we're happy about what is interesting.
00:37:41 [W] I mean, they're so close right when you're used to four percent variance or something.
00:37:45 [W] It's like if we ran, you know, if we ran each of these, you know 20 times when it just be like in the error bars, right?
00:37:50 [W] It wouldn't even it's effectively, you know in different and so we gotta go.
00:37:58 [W] Okay. Well, this is interesting. Why why I did it.
00:37:59 [W] Why did it come out this way?
00:38:00 [W] we wanted to dig in a little bit to
00:38:04 [W] Yes, right.
00:38:05 [W] Look at some of the Telemetry from the test to really try to understand what's going on here.
00:38:13 [W] Why do we why do we why do we see what we saw?
00:38:14 [W] And so I you know, it's very handy and useful use Prometheus and Griffon a together in communities and openshift and right here. We're looking at our Transformer run.
00:38:31 [W] And you can see that along the top there.
00:38:34 [W] The top row graphql is GPU utilization and it goes from zero to nearly a hundred percent when the job starts.
00:38:47 [W] At 9:55, you can see the time along the x axis there.
00:38:58 [W] And so we are just giving these gpus very busy for the duration of that run and you can see that in staff. There are you know, I'd also actually next
00:39:08 [W] For the memory utilization and word about 15 gigabytes of GPU memory used for the duration of the job.
00:39:17 [W] So and then the this fio mlops are kind of distributed equally across the entire run and are are pretty low.
00:39:33 [W] A lot of not really getting the storage that heart. So how would you like to say any more about this?
00:39:36 [W] Yeah.
00:39:42 [W] was it was kind of like, you know, we were at with what we would launch these bet we would launch this, you know, this particular workload and and and I was sitting there looking at the storage cluster. I was like
00:39:48 [W] Is it working?
00:39:52 [W] I think you know this step is board. Right?
00:39:54 [W] Like we're not doing anything and and I think you know, the the iot we're mapping here is is the the data operations, right?
00:40:06 [W] So there might be there might be some additional like like file system like, you know stats and listings and stuff that are going on here there that are increasingly cri-o.
00:40:18 [W] For the most part these are these are relatively, you know, it's reading reading like a sentence right?
00:40:32 [W] You know doing a comparison type thing. So it's a relatively at least for this workloads to not a particularly storage in tensive machine learning workloads.
00:41:09 [W] I am intensive and the iot is human language sentence pairs that together.
00:41:16 [W] So we look at the time actually.
00:41:22 [W] it was more interesting. I thought because it was it was totally different right then then what we saw with the
00:41:29 [W] the Transformer write this so this was the single.
00:41:35 [W] What is it single single ship safe detector detection.
00:41:39 [W] Chloe just bulk load everything in the very beginning and then you know, once it was once it was over to the gpus then it you know, Seth was just kind of relaxing and while the while we, you know
00:42:12 [W] Of energy crunching and doing this this image detection.
00:42:20 [W] So that was kind of interesting.
00:42:21 [W] We saw a lot more iot.
00:42:28 [W] Hello, I don't see I can't hear Diane. So I don't know if that's
00:42:45 [W] Something's wrong with my side.
00:42:50 [W] We did have a question come in about whether we headed experienced any experience pulling data and out of Hadoop have used.
00:43:03 [W] I've done a lot of work with using Hadoop and another spark system or and Spark and other you know, Big Data tools to pull data in and out of self as well.
00:43:17 [W] I'm not sure if when you say knew what do you mean like using you know, pull it pulling data out of haproxy?
00:43:22 [W] A fast and then doing training off of it.
00:43:25 [W] I haven't done that.
00:43:29 [W] I do have experience with doing using Hadoop and other big data tools to do data processing with data stored and stuff.
00:43:38 [W] my side
00:43:43 [W] we did have a question come in about whether we headed experienced any experience pulling data and out of Hadoop of used. I've done a lot of work with using Hadoop and another spark system
00:43:44 [W] In the case of these benchmarks, right then then what we see here with mlperf, right? If you run TP Z DS or those sorts of workloads or any sort of conforming workloads or doing the etls.
00:43:56 [W] They stress the storage system much more than and at least one and I'll perform right? So it would you know me and some colleagues have really looking around to see if there's any sort of storage we really storage and intensive mlperf
00:44:10 [W] Haven't kind of come up with the empty hands.
00:44:14 [W] We had another question about if the slides were available online.
00:44:33 [W] I had thought that they were uploaded but I think they didn't like the format. If you would like a copy, you know, you can ping me on slack.
00:44:36 [W] I'm the cncf slack and I'd be happy to send them over to you.
00:44:38 [W] But yeah.
00:44:40 [W] see
00:44:42 [W] very little questions
00:44:46 [W] Give it give it a few moments to see if anyone else has any final thoughts.
00:44:54 [W] If not, then we can retire to slacken can find us there. If you have any follow-up questions that you think about later.
00:45:03 [W] We didn't get so then the question came in. Did we do we do some some parallelism to speed up the process and compare the iot pressure.
00:45:28 [W] We can stop some comparable times.
00:45:51 [W] We didn't get to the point to where we would had set up to do the parallelism inside of kubernative.
00:45:56 [W] So we didn't actually get a chance to do to test the the speed-up hypothesis.
00:46:03 [W] And that's certainly the kind of the next place we want to look is, you know, we want to paralyze cross multiple GPU hosts.
00:46:08 [W] See if we can affect a speed up because that's really the
00:46:10 [W] the the benefit you get from the storage.
00:46:21 [W] The storage doesn't seem to be you know, even if we doubled or tripled the amount of workload storage would be still pretty bored.
00:46:23 [W] We really saw negligible iot and if it was if we could get the iot up to where it was some sort of meaningful out of work for the storage system.
00:46:36 [W] That's when we would start. You know, we had hoped to play with some of the formats like record iot to kind of
00:46:41 [W] You know coalesce a bunch of Records into the larger size files so that it would be less iot pensive, but there's just been no need to do that with the with the datasets that we were working with.
00:46:53 [W] We have a question about how do we measure GPU utilization and make this data available to revonnah?
00:47:04 [W] For the gpus and and sends it to the sensitive or fauna.
00:47:18 [W] So that's that's how we were able to use it as it was kind of we got that out of the box from from the Nvidia operator.
00:47:28 [W] So when that's how we deployed the Nvidia operator and then that goes ahead and then builds the builds a GPU driver for your hosts and installs the Telemetry and then it sets up the gpus as a resource and kubernative.
00:47:38 [W] He's and then you know has it so that the nodes have have it's a consumable resource on the nodes that you can add GPU as a constraining schedule about.
00:47:50 [W] So that is another thing we'd really like to do is you know, all the benchmarks as is written in python or CH and to have Pi torch interact with an object store like like an S3
00:48:22 [W] Compatible Object Store.
00:48:27 [W] So instead of using the set of distribute file system using its Object Store, you know switching the taking those pie torch training applications and then converting them to use boto instead of
00:48:38 [W] Like posix file system calls and then you know retest this against object storage. We didn't really have time to rewrite to do the to do those tests if we did if we experimented
00:48:53 [W] Other training Frameworks, like tensorflow some of them have like knative S3 and Google cloud storage and those the think faculties and so you can just like pass a path to the cocoa dataset or whatever.
00:49:08 [W] That's like S3 or GS colon slash slash.
00:49:11 [W] How is it possible to calculate the amount of cloud resources for different applications so that their quality of service requirements are satisfied who that's a that's a doozy.
00:49:35 [W] You know, that's that's that's really kind of the art of capacity planning.
00:49:45 [W] That's I want to presume the I mean I could do a whole session on just that probably but but ultimately you kind of need to you know.
00:49:49 [W] the early you'll have to project project the demands of your applications and try to stay ahead of the curve and make sure that you're able to if you're doing kind of like bursting capabilities that you're able to spend them up faster
00:50:05 [W] and the load can Spike
00:50:06 [W] All right.
00:50:34 [W] Well the very least thank you everybody for joining us for our session today.
00:50:44 [W] If if anyone thinks of other questions, we happy to chat and Wax and Wane about machine learning and distributed storage on the cncf slack. Thank you.
