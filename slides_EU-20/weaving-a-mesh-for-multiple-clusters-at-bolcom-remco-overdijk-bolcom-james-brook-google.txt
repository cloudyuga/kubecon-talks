Weaving a Mesh for Multiple Clusters at bol.com: FOQR-0551 - events@cncf.io - Tuesday, August 18, 2020 6:52 AM - 92 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:02:33 [W] Hey everyone.
00:02:40 [W] Welcome.
00:02:42 [W] We're coming live to you from Amsterdam.
00:02:42 [W] name is Remco.
00:02:43 [W] Hey everyone.
00:07:15 [W] Welcome.
00:07:16 [W] We're coming live to you from Amsterdam.
00:07:24 [W] My name is Ram covid Ike. I've been a bol.com as a systems engineer for over three years now, and I'm currently working in our compute infrastructure team.
00:07:28 [W] Our team deals with all types of infrastructure, especially compute ranging from bare metal and virtual machines in our data centers up to kubenetes in Google cloud and everything in between.
00:07:39 [W] between. We also play a pivotal role in the transformation of our infrastructure to public cloud weaveworks.
00:07:43 [W] Not quite coincidentally is also the stepping stone for today story.
00:07:48 [W] Hi, I'm James Burke and I'm a Solutions architect to Google Cloud.
00:07:54 [W] My team works with some of Google's largest customers and we help them to solve difficult and interesting engineering problems.
00:08:03 [W] I'm also a servicemeshcon to use Yost and I've been following the technology from the very beginning.
00:08:08 [W] not actually part of a team working directly on servicemeshcon.
00:08:18 [W] With a large number of Barker selling their we're through a platform combined with a rod or large selection of products of our own were to first stop in online shopping for large portion of Dutch and Belgian customers enjoying same and next-day delivery and excellent
00:08:36 [W] Being a major e-commerce platform takes a lot of hard work and infrastructure.
00:08:43 [W] We've experienced a steady growth in the best allowing us to employ an ever-increasing number of people many of them focusing their attention on solving it related problems to stay ahead in this competitive market.
00:08:56 [W] With the amount of teams, we employ staying agile is key.
00:09:00 [W] As such we feel it's important for our featured teams to be able to work independently without having to wait on platform teams to deliver are meeting their infrastructure for them.
00:09:10 [W] For example, that's why we believe that Self Service infrastructure is a future for us.
00:09:18 [W] Ebola comb. We used to doing things ourselves much of our infrastructure still lives in our private data centers and being in online retail. There's obviously very significant seasonal variation in both our business and traffic.
00:09:33 [W] Historically, we've been statically skilled for the busiest day of the year plus a little safety margin.
00:09:38 [W] We do have lots of automation. I manage our machines using puppet software development teams can deploy their applications in our data centers using automation, but all of the interests infrastructure interaction on their behalf is left aside reliability teams.
00:09:54 [W] We believe in devops and we want to allow teams to really focus on building applications and not all the other stuff in a Datacenter self-service is partially possible, but it's a bit obscure and inflexible. Our philosophy is you build it.
00:10:10 [W] You run it you love it to further Embrace that Mantra we try to increase the level of self Surface. We offered granting instana me a sharing ownership and a true devops way of working turned out to be hard in our current setup.
00:10:23 [W] Separation of concern was a real issue.
00:10:28 [W] We just couldn't find a way to have the over to pagerduty plication teams.
00:10:30 [W] So, how could we get more autonomous teams?
00:10:39 [W] We started by integrating public Cloud into our platform catalog tikv here was to combine elastic scaling infrastructure with the truly programmable interface with apis that could cleanly separate concerns for developers as well.
00:10:49 [W] Newer applications nowadays are deployed to Google kubernative engine a managed kubernative cluster on top of the Google Cloud.
00:10:57 [W] The applications that we deploy our built or rebuilt specifically to take advantage of the cloud the kubernetes our ecosystem of automated platform tools knowledge first clear first class support for the way. We use cloud services as well
00:11:13 [W] Is the coup cleaning separate concerns for developers as well?
00:11:13 [W] Newer applications nowadays are deployed to Google kubernative engine managed to kubernative cluster on top of the Google Cloud.
00:11:14 [W] The applications that we deploy our built or rebuilt specifically to take advantage of the cloud and kubernetes our ecosystem of automated platform tools knowledge first clear first class support for the way.
00:11:16 [W] We use cloud services as well developers get the right a kubernetes like manifest to deploy a rich set of gcp resources, which in turn are deployed by centralized tool.
00:11:21 [W] Application teams can build their own ci/cd pipelines in gitlab and mix-and-match support of gcp products based on their requirements.
00:11:29 [W] Move to the cloud together with regulations like GDP are also a fact that the way we needed to think about security the platform gives freedom, but it also enforces policy guardrails at the same time.
00:11:43 [W] Given the scalability of kubenetes. The original idea we have was to have a single cluster / violent in order to reduce the overhead these multi-tenant clusters house all of the different applications and services isolated into namespaces
00:11:59 [W] It's like GDP are also a fact that the way we needed to think about security the platform gives freedom, but it also enforces policy guardrails at the same time.
00:12:00 [W] given the scalability of kubernative the original idea we have what's to have a single cluster for environment in order to reduce the overhead these multi-tenant clusters house all of the different applications and services isolated into namespaces
00:12:02 [W] The next to each other using network bolus. He's this work well for a while, but some mistakes were made along the way one in particular since we want our gcp services to be reachable from our data centers and vice versa.
00:12:14 [W] They're using private interconnects to form a large private network sharing same class a network. This also means that we had to use our IP subnet space sparingly.
00:12:26 [W] Given that we had around 400 microservices in the data center at that time initially. We figured that having a thousand twenty-four services in a single kubernative closer will be plenty out of such we set up the
00:12:40 [W] / 20 through something that for services allowing for twenty thousand thousand twenty-four services at that time though.
00:12:51 [W] fact that almost everything including monitoring requires a service and kubernative.
00:12:54 [W] We also interested in the rest of knative the popularity of the self-service model and soon we saw a ramp of growth of new Services being used for all kinds of proof of Concepts and new microservices.
00:13:09 [W] This ended up being a bit of a crisis in the end.
00:13:14 [W] We came very close to hitting the surface leaving effectively halting the creation of new services in that environment.
00:13:15 [W] Unfortunately, though one of the Clusters one circleci as were created.
00:13:25 [W] It was not possible to increase the size of an assigned subnet without recreating the entire cluster moving all over hundreds of workloads to a bigger cluster would have been a major project.
00:13:33 [W] still wouldn't be completely free future-proof seems extremely large kubernative clusters are certainly possible but come with a new set of scaling challenges. So we concluded that we needed multiple clusters / environments and treat them as cattle rather than pets.
00:13:46 [W] I guess that's our first lesson to share today who plenty of thought into your IP subnet allocation strategy, believe it or not.
00:13:56 [W] This was actually the primary reason for looking at the servicemeshcon first place.
00:13:57 [W] Since extremely large kubernative clusters are certainly possible but come with a new set of scaling challenges. So we concluded that we needed multiple clusters / environments and treat them as cattle rather than pets.
00:14:11 [W] I guess that's our first lesson to share today with plenty of thought into your IP subnet allocation strategy, believe it or not.
00:14:12 [W] This was actually the primary reason for looking at the servicemeshcon first place.
00:14:13 [W] So we knew we needed multiple clusters.
00:14:13 [W] We had quite separate data in the cloud and data center worlds.
00:14:15 [W] We have many different ways to load balance and discover services and there wasn't even really an easy way to find a service either now servicemeshcon.
00:14:30 [W] servicemeshcon
00:14:32 [W] servicemeshcon Stephen C complex-- it's not that mature technology either. We had experience with that from gambling on kubernative when it was still young.
00:14:42 [W] but we started to think seriously about a servicemeshcon bonus features started to become quite aberrant like strong identity Mutual TLS authentication and authorization for services would make it much easier to please our security teams
00:14:58 [W] Did everything in our power to go milk the cluster without servicemeshcon?
00:14:58 [W] Servicemeshcon fnc complex.
00:14:59 [W] It's not the mature technology either. We had experience with that from gambling on kubernative when it was still young.
00:14:59 [W] But we started to think seriously about a servicemeshcon bonus features started to become quite apparent like strong identity Mutual TLS authentication authorization for services would make it much easier to please our security teams.
00:15:02 [W] Expansion the idea of seamlessly expanding the mesh to machines in our data centers really appeals to us as well.
00:15:05 [W] We worry a lot about data exfiltration and a concept of filtering out on traffic based on requests URLs using egress gateways seem promising.
00:15:14 [W] We approach trying a feasibility research very carefully and appointed the project manager a firm cross-functional working groups to figure out particular details key criteria for selecting a product include it multiply so support meshmark.
00:15:30 [W] Since eventually we want to incorporate our den data centers into the same mesh.
00:15:35 [W] It should support egress gave ways because egress gateways would provide the support needed for filtering outbound traffic.
00:15:42 [W] And a strong community.
00:15:45 [W] we evaluated all the available options at the time but this being about 18 months ago is still was really the only product that provided both multi cluster support and ticked all the boxes or would have upcoming support for those
00:16:00 [W] The relatively short term our close cooperation with Google. Also meant that he has access to a co-developers which proved to be very useful when touched on will touch on that later as well.
00:16:13 [W] Service - servicemeshcon ology has come a long way in these past 18 months any available options are now more mature more diverse and tailored to specific scenarios as well linkerd e for example recently launched multi cluster support. So make sure that
00:16:28 [W] Election based on your requirements before diving in headfirst. Okay.
00:16:36 [W] We said this wasn't going to be a what is servicemeshcon.
00:16:36 [W] awk.
00:16:40 [W] But we are going to Deep dive into some of the things that were particularly relevant to the journey as you probably know is to use data plane is made up of envoi proxies.
00:16:52 [W] So typically on communities an Envoy proxy gets injected into each of your application pods.
00:16:53 [W] You see the envoy referred to as the service proxy or the sidecar the sto control plane, which is the community's API server and maintains a registry of all the services in the mesh and it uses that registry along with the
00:17:08 [W] Shouldn't you provide to manage envoi proxies using the envoi XDS apis and that's how a load of separate envoy's are formed into a single mesh.
00:17:19 [W] One Services have an invoice psyched are the proxies become directly responsible for all of the routing of your service to service traffic?
00:17:27 [W] For the client side load balances envoi montañés clusters of healthy endpoints.
00:17:35 [W] That's not kubenetes clusters. So it's definitely useful to have a pretty good understanding of envoi terminology if you're serious about doing meshmark,
00:17:42 [W] Each endpoint in a cluster is simply the destination IP address for port and a port the traffic flows between poddisruptionbudgets.
00:17:56 [W] By having a servicemeshcon awning across multiple kubenetes clusters.
00:18:02 [W] We hope to make the cluster boundary much less significant client side load balancing is one of the most powerful Concepts in servicemeshcon.
00:18:09 [W] And bol.com gke clusters are set up to use VPC knative networking which basically means the pods get an IP address from a reserved Subnet in the VPC and all of the pot IP addresses in all of the Clusters that directly routable in a flat Network.
00:18:25 [W] separately managed internal load balances can now be deleted
00:18:42 [W] so zero trust networking
00:18:46 [W] We originally expected to hear an audible groan when we heard this when you heard this jargon mention, but this talk is recorded.
00:18:54 [W] So it doesn't matter now.
00:18:58 [W] It sounds like jargon and and marketing hype but it's important security is really important to bol.com, especially in these days of GDP our regulation here is how history oh and envoi help.
00:19:09 [W] Each service gets its own identity and the important thing about that identity.
00:19:15 [W] is that it truly represents the application and like an IP address, which is in the communities world is something very ephemeral.
00:19:21 [W] We want habitations to be able to communicate with each other, but we need a way for applications to trust each other based on their identity and not an IP and for that to happen.
00:19:32 [W] We need a few other things credentials. So these come in the form of TLS certificates.
00:19:37 [W] Mutual tearless provides the means of authentication and for those of you who haven't run into mutual GLS, it's very simple.
00:19:48 [W] It just uses an optional feature of the standard TLS handshake so that the client also presents the certificate and the server verifies it and of course Mutual TLS gives you incription to and then finally authorization
00:20:01 [W] Identity and authentication. We have a policy API to say which services are allowed to talk to each other.
00:20:09 [W] Later on, we found that with there were some caveats to zero trust with servicemeshcon loan and we'll get to that soon.
00:20:21 [W] So it's clear that this was not a so-called Green Field deployment. That would have been a wonderful luxury.
00:20:23 [W] Bol.com are running hundreds of applications using all sorts of different Technologies being able to migrate incrementally was a serious concern concern and it turns out rightly.
00:20:37 [W] So this is the hardest and most time consuming part of the project and this is what we are getting at when we talk about weaving the mesh an application becomes part of the mesh when an invoice service proxy is injected as a sidecar and the pods
00:20:49 [W] Instead of all traffic entering and leaving the application is handled by envoi.
00:20:54 [W] In this project. We were only focused on enabling the sidecar don't start using any of the features yet. Only when you've got the side cars in then start using features and realizing the benefits.
00:21:07 [W] At a really high level we plan several phases to weave the mesh lots of preparation enabling sidecars in permissive Mutual tearless mode with something called Auto M TL S turned on enabling authorization policy
00:21:23 [W] We've the mesh lots of preparation enabling sidecars in permissive Mutual TLS mode with something called Auto M TL S turned on enabling authorization policy switching Mutual TLS to strict
00:21:25 [W] Mutual TLS to strict mode disallowing plaintext connections and then eventually cleaning up all the Loose Ends and start adding more features.
00:21:34 [W] Broadly broadly each step depended on the previous step for the whole mesh.
00:21:38 [W] We knew the adoption would have to happen incrementally namespace by namespace or even by workloads.
00:22:09 [W] All about enabling the side curve surely. You just need to label a namespace or deployment.
00:22:20 [W] It turned out that enabling the sidecar was messier than expected more effort went into rolling out service proxies to the existing applications than any other part of the project.
00:22:29 [W] Remember there are approximately 500 services using all kinds of different Technologies and protocols.
00:22:32 [W] So we're going to talk a bit in more detail about some of the things you need to think about.
00:22:39 [W] We're not inside cars to your applications.
00:22:42 [W] There are some simple things that you just have to get done.
00:22:43 [W] Do some HTTP response validation a malformed response header could trigger a 5030 so that just comes down to testing and fixing our reputations are monitored with Prometheus and the logs go to elastic search.
00:23:11 [W] So that just comes down to testing and fixing our reputations are monitored with Prometheus and the logs go to elastic search when you enable restrictive policies.
00:23:16 [W] You have to make sure your observability tools.
00:23:18 [W] just keep working and probably the most obvious thing is that sidecars don't come for free.
00:23:25 [W] There are thousands of pods in the Clusters.
00:23:28 [W] So they said that we knew that there would be thousands of extra containers and that's overheard you need to plan for you can see it as a tax you get
00:23:32 [W] Pay for the benefits of servicemeshcon.
00:24:02 [W] But before we dive into some of that there was another major issue we ran into the luckily.
00:24:09 [W] You don't need to worry about now, and we picked up a couple of best practices Remco is going to tell you all about that.
00:24:18 [W] We deployed our gke test environment on Google preemptible VMS, which can disappear with a few seconds notice.
00:24:22 [W] This turned out to have an unexpected benefit.
00:24:24 [W] They serve as an excellent chaosmesh key.
00:24:28 [W] Is your users an inner container to regrow reprogram the bus Network so that all TCP traffic in and out of the Bob is handled by the proxy.
00:24:43 [W] We decided not to use to default Network keptn mechanism because we had concerns about privileges because the in each container needs NetApp new capability to modify iptables that basically meant that we would have need to grant that
00:24:59 [W] Because the inner container needs NetApp new capability to modify iptables that basically meant that we would have need to grant that privilege to anyone and anything that could make a deployment on to our communities clusters.
00:25:04 [W] Luckily, there was an alternative and it's Toc and I plug in.
00:25:09 [W] Which does not need those elevated privileges unfortunately are preventable chaosmesh.
00:25:17 [W] He exposed the problem pots on a VM that just got preempted would often come up without the IP table rules being properly updated.
00:25:25 [W] Don't necessarily an issue if you don't use printable VMS in production, but the issue might occur there as well under other circumstances.
00:25:34 [W] Another piece of luck here is that we happen to be at cube. Consignee a go at the time when we notice problem James was there as well any track down some SQL contributors and we managed to find the cost by sitting at while sitting at the booth.
00:25:50 [W] It turned out to be a race condition with Calico when new nodes came up that's issue is solved Upstream now, so it won't bother anyone anymore.
00:25:59 [W] So there's two suggestions here.
00:26:03 [W] You can use the ecoc and I plug in for never capture rather than the default behavior and do some chaos testing preemptable VMS helped us in the beginning, but we ended up deploying a tool to randomly kill pots in the end as well.
00:26:15 [W] The overhead of the data for them when the sidecar container is injected some Rook resource requests and limits are set as you can see from these tables.
00:26:28 [W] Ultimately we were able to substantially reduce the default limits that is do sets.
00:26:29 [W] One thing to watch out for resource quotas are used on the namespaces at bol.com and adding a bunch of side-by-side cars could easily push a team above its limits. So be prepared to adjust these or you might not even be able to start pods.
00:26:45 [W] The proxies don't buffer traffic and the potential through push in requests per second is generally determined by the available CPU.
00:26:53 [W] Elimination is 0.4. CPU power. Proxy gets us about 700 requests per second.
00:26:59 [W] One thing that made a dramatic difference to memory consumption was limiting the configuration the scope of the proxies by segmenting the mesh more about that in a moment.
00:27:09 [W] If you use the horizontal POTUS kaler, you need to think about that too. It works by summing CPU requests for all the containers in pods adding the sidecar can easily skew the thresholds of which auto-scaling kicks in
00:27:25 [W] Two new world its goal is to avoid not scaling soon enough or wasting resources.
00:27:31 [W] Obviously not every application is the same because of that we needed an easy way to tune proxy resources per deployment.
00:27:46 [W] For example, we might have applications that do lots of heavy processing but don't handle many requests or vice versa his co-star ports resource annotations, which teams can simply add to their deployments to tune requests and
00:27:56 [W] We just explained that we were able to use roughly half the memory per side card that issue would request by default in the most simple flat meshmark.
00:28:15 [W] Luckily, there is a solution is to provides a kubernative custom resource called sidecar this resource lets you limit which services are configured as destination in points in the client side load balancing config so you can use it to segment
00:28:31 [W] space were even with fine grain workloads selectors
00:28:35 [W] Then the proxy is our only programmed with config for the destinations that they need to know about. This greatly reduces conflict bloat and the memory consumption of proxies, but also the demands on the control plane.
00:28:49 [W] It's completely essential to do this at any real scale.
00:28:53 [W] Bol.com use the sidecar resource to segment their meshmark the beginning of the roll out. So we don't have a nice graph like the one in this tweet from Carl Stony at AutoTrader UK.
00:29:05 [W] It was actually possible to generate all of the sidecar config because the existing platform automation at Bowl had a graph of all the dependencies between the services another fun issue. We ran into while enabling site course is the fact that currently is sidecar
00:29:21 [W] You cite Garden kubenetes.
00:29:26 [W] It's just another container running inside the same pot.
00:29:29 [W] There are all first class citizens all this can be problematic for all types of workloads.
00:29:35 [W] The problem with this particularly impacts chronosphere.
00:29:51 [W] It will remain running. Once the main container is done effectively keeping these crunch up running.
00:29:57 [W] The same holds true for starting workloads many applications require network connectivity during their initialization.
00:30:07 [W] That's a problem. If the side car isn't ready yet knative support for side Garth is a feature that the kubernative team is currently working on but to too many prerequisites might take some time for the particular feature to appear in a release
00:30:19 [W] We weren't able to wait that out.
00:30:24 [W] So we needed a temporary solution cuse cuddle cuddle is lightweight wrapper for the main container in a pot that blocks until the sidecar proxy is ready to handle traffic and it will shut down to proxy once the main process terminates.
00:30:39 [W] This address the problem with crohn's Ops as cuddle is currently recommended for all side guard enabled workloads within our setup.
00:30:52 [W] Unfortunately that also meant that most container images have to be updated to include Scuttle which was a job that needed to be executed by the future teams, since they were the ones building to containers in addition to the work. They have to perform to make their apps
00:31:01 [W] Then our setup unfortunately that also meant that most container images have to be updated to include Scuttle which was a job that needed to be executed by feature teams, since they were the ones building to containers in addition to the work. They have to perform to make
00:31:03 [W] Place one of the first benefits of having a majority of the workloads now being equipped with a sidecar with the automatic use Mutual TLS for service to service traffic.
00:31:13 [W] The side cars when they were injected with configured to use permissive mode with mutual GLS that meant that service proxies could accept both Mutual TLS and plaintext connections allowing both regular and sidecar enabled services to communicate
00:31:29 [W] Initially, we thought that we would need to configure which routes should use Mutual GLS and then a feature called Auto M TL S appeared Auto M TL S uses the information in the message measures service registry and will automatically use
00:31:45 [W] In this wherever there is supported Nats saved us a lot of extra manual configuration hassle.
00:31:52 [W] And while this is a great feature to allow for a seamless migration at some point the plan is to enable strict mode for most of the applications to ensure security of in-flight data on all of the connections between the services.
00:32:08 [W] There are all sorts of applications out there.
00:32:13 [W] We've set a bit about needing to customize resource requests for service proxies. We've also found that the reality is that sometimes you'll need to bypass the proxy for certain ports.
00:32:20 [W] The particular bol.com using Technologies like hazel cast and zookeeper these both have some sort of Discovery and clustering mechanism.
00:32:32 [W] They want to announce their pod IPS directly to each other and that doesn't work properly with the service proxy in the middle.
00:32:35 [W] The solution was to exclude certain Ingress ports from being captured by the proxy and luckily is still provides annotations to do that and bol.com teams can use those on their deployments.
00:32:49 [W] So if the proxy can be bypassed what happens to the whole 0 trusting to fill those gaps, we decided to fall back on network policy-based bulkheads. And of course that needed to be automated the bol.com
00:33:04 [W] Being captured by the proxy and luckily is your provides annotations to do that and bol.com teams can use those on their deployments.
00:33:06 [W] So if the proxy can be bypassed what happens to the whole 0 trusting to fill those gaps, we decided to fall back on network policy-based bulkheads. And of course that needed to be automated the bol.com
00:33:07 [W] Some communities control that to watch for deployments with certain annotations and then automatically create the necessary Network policies.
00:33:14 [W] The network policy was also used to blog UDP traffic between this name spaces because it's gond intercepts TCP traffic and that brings us to the current state of affairs.
00:33:27 [W] It's been an Epic Journey so far.
00:33:30 [W] We're nearing the end of the migration has discussed today at the time of this recording.
00:33:37 [W] We have the envoy sidecar enabled for over 91% of all of her workloads across all environments totaling roughly 3,500 bucks.
00:33:42 [W] Not all of you workloads have em TLS and authorization and able to yet. But since those migrations are left up to the team's we're hoping to have the majority taking full advantage of the meshmark. If it's by the end of August, here's a final summary of some things that we think
00:33:58 [W] Epsagon be very incremental prepare and educate application teams.
00:34:05 [W] They need to be on board understanding void its configuration use the sto.
00:34:13 [W] Cri-o Ginn for injection.
00:34:16 [W] Keep a careful eye on data plane resource usage avoid conflict bloat and segment with the sidecar resource.
00:34:21 [W] And allow developers to tune service proxy resource usage for their applications.
00:34:27 [W] test everything carefully and chaotically
00:34:32 [W] expect to deal with exceptions
00:34:34 [W] We've learned a lot during this implementation and migration most of which we've already mentioned in detail during this talk. But one last thing to remember is it servicemeshcon are a relatively young technology and development on most of them is fast paced as
00:34:50 [W] So you should be prepared to keep up with those developments in order to make the most out of your implementation.
00:34:59 [W] Even if that means adjusting your plans a couple of times along the way even though we spent quite some time and preparation to get the servicemeshcon its current state.
00:35:09 [W] We realize we're only at the beginning of the journey with so many options to explore.
00:35:10 [W] But those Futures Adventures will have to wait for another Cube compressive station at a later date for now.
00:35:20 [W] I'd like to conclude this talk today by thanking James and Google for their continued support as well as the cncf all contributors and the community for enabling is to build our foundation on awesome products, like kubernative and
00:35:33 [W] Co Prometheus and open policy agent none of this would be possible without them.
00:35:41 [W] Thanks so much for joining us today, and we're happy to answer your questions though. Thank you.
00:35:47 [W] All right.
00:35:52 [W] I'll first of all thanks everyone for all their questions write a few of them came in. We're pleased about that.
00:36:01 [W] Let's start at the top you'll notice and how did you decide on what servicemeshcon use what we kind of answered that during during the talk? But yeah, we had a project manager some working groups.
00:36:18 [W] We established some matrices with all the requirements and then we compared to all the products that were available at the time against those products asked us on the slack and I'm happy to share these matrices with you so you can get a
00:36:32 [W] grasp of the stuff we were looking for
00:36:34 [W] James
00:36:37 [W] yeah, let's pick one.
00:36:45 [W] That hasn't that we haven't already answered.
00:36:49 [W] What tools do you use for monitoring?
00:36:59 [W] I guess that's a good one for you.
00:37:00 [W] Ready room go. That's it.
00:37:01 [W] Yeah Prometheus for for our cloud cloud environment.
00:37:09 [W] Prometheus is right there at the top.
00:37:12 [W] We used to use nagios in a data center. But we switch to full Prometheus when we started using kubenetes.
00:37:18 [W] Yeah that with graffiti bit of final sprinkled on top on the some areas and logging is still elastic search the same stack were using in a data center.
00:37:29 [W] And I guess the most the most fun thing to notice here is that the teams are developing their own alerts now rather than getting predefined alerts in the data center.
00:37:47 [W] Yeah, and if you've seen a whole batch of other questions that I hadn't spotted before sir.
00:37:53 [W] Have you seen any issues with the it's gocnak plug-in from Maxim?
00:38:01 [W] I wonder I mean we talked about an issue that we experienced in there in the course of the adoption which is which is now resolved, but I think in general Remco have you have you seen any
00:38:14 [W] Servicemeshcon it into production knows I mean most of the kings were ironed out before we went live.
00:38:25 [W] That's a that that picture of us at Cube cone in San Diego that that was when we were tearing our hair about tearing your hair out of that issues in cni-genie, which is now time for the
00:38:41 [W] That keep calling in San Diego that that was when we were tearing our hair about tearing your hair out about issues there wouldn't you know plug-in which had a thing for be resolved in the code?
00:38:43 [W] Yeah.
00:38:47 [W] That's probably one of the top three issues. We encountered during the implementation but ever since we got life, I have to knock on wood obviously, but we haven't found any any major stuff yet.
00:38:54 [W] Okay, question-bapuji altar.
00:38:59 [W] How do you operate do you operate in steel yourself or do you use Google's ecog as a service?
00:39:04 [W] No, we brought our own we use open source is co we have a separate cluster that rooms are controlled playing. So all the SEO configuration is in a separate processor, which makes it easier to upgrade and in less interference from the production workloads.
00:39:19 [W] Operator still yourself or do you use Google's ecog as a service?
00:39:20 [W] No, we brought our own we use open source is co we have a separate cluster that roams are controlled playing. So older a CO configuration is in a separate processor, which make it easier to upgrade and in less interference from the production workloads.
00:39:21 [W] And then we have the data plane is on our actual usual regular clusters.
00:39:25 [W] I think that harness is mostly answers David's question as well.
00:39:33 [W] How did you set up your multi car stereo stereo, I guess what we could call what we have as a it's like a dedicated control plane shared between the workloads pastors, and
00:39:46 [W] What we could call what we have as a it's like a dedicated control plane shared between the workloads pastors. And I think like you said Remco it was decided that isolating the control plane
00:39:50 [W] Idream kind of it was decided that isolating the control plane compute from the workloads computers considered a nice separation of concerns and it also enables you to upgrade the control thing
00:40:02 [W] The formula from your workloads so you can do yeah, definitely pretty Canary control plane upgrades and then in terms of reliability at some point, you know, it might be possible to introduce additional
00:40:19 [W] Running the control plane for a liability that moment is especially a single single region deployment single failure domain. So I have a feeling that clashes running on a high availability Regional classroom with notes in with their
00:40:34 [W] different zones and so on
00:40:38 [W] So another interesting one, what about meshmark Spanish in experience in production?
00:40:48 [W] So as I said me of the time we recorded this talk meshmark sanshin was not in production yet, but it was the first thing that the bowl engineering team moved on to and I think they have it they have it
00:40:59 [W] Environments to their satisfaction.
00:41:06 [W] It was a was a lot if I recall about getting getting, you know, the nice RPMs built for the heroes that's running on those systems and deciding how to get secrets on to them machines
00:41:19 [W] Les between one premise services and services running actually on the cloud casters would be working.
00:41:29 [W] All right.
00:41:33 [W] I think we're almost running out of time.
00:41:36 [W] I think we have like one minute left.
00:41:45 [W] So if we fail to answer any of your questions join us on slack and we'll be happy to get into more detail detailed answers.
00:41:47 [W] I guess I can do one more.
00:41:48 [W] How do you skill workloads?
00:41:52 [W] Well, we have a Giotto scaling for the closest themselves.
00:41:55 [W] So that's that's the way we skill clusters workloads themselves. We use horizontal pull down the scaling for the Copo doubt the scaling.
00:42:01 [W] We're experimenting with other types of multi-dimensional pulled out the scaling as well. But it's up to the teams to decide how they do it themselves.
00:42:10 [W] I guess that's it.
00:42:13 [W] Yeah, I will be will both be on the Syfy Channel and happy to take more questions.
00:42:22 [W] Thanks a lot for joining us.
00:42:25 [W] Ciao.
00:42:27 [W] Thank you.
