Introduction to Autoscaling: CCSN-3150 - events@cncf.io - Tuesday, August 18, 2020 7:40 AM - 87 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello, my name is Martin Vail goose and today together with my colleague beta we will give you an introduction to auto-scaling in kubernative.
00:04:52 [W] Covid and coronavirus are probably the most hated words of 2020.
00:05:00 [W] because a lot of grief Sadness and Sorrow many people lost their loved ones.
00:05:05 [W] The economy all over the world went to a standstill lots of business had to close.
00:05:12 [W] Almost everyone had to adapt to new science area requirements.
00:05:17 [W] In 80, we are privileged we could move from offices to our homes and pretend it was business as usual.
00:05:26 [W] Computers were easy to move video conferencing had been there for ages. So is it business as usual?
00:05:36 [W] Well, not really as the world changes. I think needs to catch up usage and traffic patterns change some Services popularity skyrocketed some barely get any interest new features are accelerated to keep the business
00:05:51 [W] Migration to cloud is also accelerating escaping own data centers may become challenging. Should we have another round of lockdowns in the fall?
00:06:02 [W] And as many business and customers are struggling margin are shrinking being cost-efficient has become much more important than used to be companions want to pay only for what they really need now are may need in the near future
00:06:19 [W] turkey prank
00:06:21 [W] Unfortunately, nobody can predict the future so adaptability and cause optimization may be the key to success and this will be the topic of our talk today during this talk.
00:06:37 [W] We want to help you to build a checklist and provide solutions to problems like
00:06:40 [W] Can my application adapt to changing traffic can effectively manage resource consumption?
00:06:47 [W] Can my infrastructure adapt to changing number of pots there are sizes to new application and their needs?
00:06:57 [W] Where can I almost effortlessly reduce cost?
00:07:01 [W] In other one, in other words, we want to give you a helpful hand a bit of automated assistance robotic Guardians so that you can have a bit of Peace of Mind in these challenging times.
00:07:16 [W] Let's start the first robot and I'm going to talk about is called horizontal potato scalar.
00:07:23 [W] Horizontal put autoscaler is based on metrics that Express the load that the application gets.
00:07:33 [W] It can be either real CPU usage or something more custom like the number of queries per second or other goats leg value provided by the user.
00:07:41 [W] Metric compare it with the user defined Target and depending on the comparison adds or removes replicas hoping to move the metrics to our the desired value.
00:07:55 [W] Let's take a look on how does it work in practice. But before that let me clarify something while talking about HPA I will use a word utilization.
00:08:06 [W] I understand it as a ratio between the current load expressed by some Matrix and the maximum amount of load that the application can handle for example, if Potter requested one CPU and is currently using only half of
00:08:21 [W] And utilization is 50%
00:08:25 [W] if pot can handle 1,000 queries per second, but it's processing on 200 at this moment. Then the utilization is 20%
00:08:35 [W] Okay.
00:08:44 [W] Now Dimension example in this example, we have four posts that are burning hot. They are reporting very high utilization. There are load is about the target.
00:08:49 [W] They also use almost all of their capacity.
00:08:53 [W] If the traffic increase there might be not enough power to handle it.
00:08:58 [W] If we add another pot and the same traffic is spread across five pots.
00:09:05 [W] Then the average per pot load will obviously decrease leaving more space for spikes and coming closer to the Target.
00:09:14 [W] If the situation is different and for each of the poles the load is way below the target.
00:09:23 [W] It might be a good idea to delete a pod.
00:09:27 [W] Then the traffic on the remaining possible increase the pots will be better utilized and you will have more free resources in the cluster.
00:09:37 [W] And this is what HP does ADD and delete spot replicas when needed?
00:09:43 [W] When to use HPA HP works the best with stateless serving workloads that can spin up new replicas reasonably fast.
00:09:55 [W] You can also use HPA with Cubase processing.
00:09:59 [W] How to enable HPA HP is part of core kubernetes API is HP is controlled by an HP object.
00:10:14 [W] You can create this object either manually by providing a uml definition or by a cube sit here autoscale command.
00:10:17 [W] CPU is just available out of the box custom metrics require ultimate more work.
00:10:24 [W] While setting up HPA is crucial to set the target value or more generally Target utilization and write 1 minus utilization is your Spike capacity and a driver for scale apps.
00:10:40 [W] Two small buffer May prevent scale ups and may overload the application to Big obviously causes waste.
00:10:47 [W] The value of the target utilization should allow you to run with the increased traffic at with the old number of replicas for let's say two minutes and still have a bit of buffer.
00:11:03 [W] So if you expect that within any two minutes your traffic may rise by 50% set the target utc's into less than 65 percent if it can double 2050 and so on and so on.
00:11:16 [W] What are the other best practices for HPA?
00:11:19 [W] Applications under HP should correctly handle Sig terms.
00:11:28 [W] That means that after receiving the signal the application should not stop but finish all of the requests that are in flight and still listen for the incoming connections that may arrive after the pot termination begins.
00:11:39 [W] It takes a while to update all q proxies and load balancers. Sometimes even one minute if your application dies before all q proxies and load balancers are updated some requests May and end up in
00:11:54 [W] Causing arrows on the client side. You can handle Sig term signals by adding the appropriate code in your app or by putting something like sleep 60 in a purse top hook.
00:12:06 [W] Also, your new replicas should not receive requests before they are ready for that to ensure that please set up Readiness probes and liveness probes.
00:12:20 [W] It's a good practice even without HPA but it is particularly important when your posts are constantly starting and stopping.
00:12:28 [W] Please keep your metric serverless Z it is the source of CPU utilization metrics.
00:12:37 [W] And without it HPA will not work last but not least.
00:12:41 [W] Please enable cluster autoscaler while adding new replicas you may end up without any free capacity in the cluster and you need something to fix it up. Also when the number of your pot decrease something
00:12:54 [W] It's to decrease the size of your cluster.
00:12:58 [W] As I said before HP adjust the number of pots to keep the desert utilization.
00:13:14 [W] So if your current average application integration is 20% and you put this application under HPA with 60 percent utilization Target. You may get up up to 3X cost reduction
00:13:20 [W] Assuming of course that you enable cluster autoscaler.
00:13:23 [W] Now it is time for me to pass the mic to Beta will introduce you to the second helpful auto-scaling robot vertical patata scaler.
00:13:34 [W] Thank you much.
00:13:36 [W] So we've seen that horizontal put out two scalar cares for the right amount of pot replicas for a workload vertical put autoscaler.
00:13:52 [W] However, recommends podcast sizes it constantly monitors the real CPU and memory usage of your application to estimate how much CPU and memory for pods should be requesting.
00:13:59 [W] The recommendation that it makes make sure that the real usage is well below the requested resources, but why are Padma quests exactly like boxes for cats?
00:14:14 [W] cats? They should neither be too small nor to bake fund requests are with kubernative scheduler uses to decide where to put your pots based on how much free space a given machine has
00:14:24 [W] the request is also what kubernative govern teas to provide to your application to run.
00:14:31 [W] This means that if the requests are too low the scheduler may put your prodyna note that doesn't have enough resources to provide for your pop this in turn can result in out of memory errors workloads evicted due to memory pressure or your workloads underperforming.
00:14:47 [W] Not enough CPU available know just from the point of view of your customers.
00:14:55 [W] This is already an outage on the other hand specifying two big resource request is very safe, but it causes your resource waste you are not using resources that could otherwise be useful somewhere else.
00:15:06 [W] So let's take a look at the simple example.
00:15:11 [W] We have a pod. It uses 95% of the requested capacity for an extended period of time if mind also occasionally burst and use more than hundred percent would be Pa will do is it will recommend increasing pot size so that
00:15:25 [W] Only let's say 75 percent of capacity is used.
00:15:29 [W] Another situation is if the load on the part is very low for extended period of time BPA will recommend to decrease but size and free some space this will then allow for better utilization of your machines.
00:15:45 [W] We PA Works in 3 modes in the first one, it only provides recommendations, but it doesn't change actual research requests.
00:15:58 [W] You may then choose to apply the recommended settings manually in the second one. It only sets requests for positive.
00:16:04 [W] Newly created.
00:16:04 [W] doesn't change pods that are already running and the last one third one automatic mode allows vpa to evict an existing product if it has suboptimal request.
00:16:17 [W] And we created with recommended ones.
00:16:19 [W] BPA is best suited to figure out usage patterns over longer periods of time.
00:16:31 [W] time. It's not tailored to responding to sudden changes of usage for this HPA is definitely the better tool you can use BPA for both staple and stateless workloads, especially if they are not a good fit for HPA.
00:16:41 [W] And this current implementation it's important to remember that the EPA Taylor's their communication to the majority of the workloads life, sir.
00:16:57 [W] So if you have a workload with let's say very very short and a very hot start up phase this may cause me PA to recommend not enough resources for your workload to be able to start efficiently.
00:17:05 [W] And lastly the outer Mount is useful if your application can handle restarts and if you gained enough confidence on the recommendations that you are getting using
00:17:20 [W] only mode
00:17:21 [W] EPA is not a part of kubernative. He's so you either need to install it as an additional component or enable it in your kubernative managed solution, like for example gke and then you do it with the appropriate cloud provider specific commands.
00:17:38 [W] After you do that you still have to create a VP a object pointing to the application that you want to scale.
00:17:46 [W] This is similar to what we've seen with HPA.
00:17:51 [W] However here we have no dedicated keep CTL comment, but you can check out the examples of how to configure this in our outer scalar Repository.
00:17:58 [W] Now some tips on what to watch out for when using the EPA as I mentioned earlier. You should start your advantage with EPA in recommendation. Only mode.
00:18:08 [W] Give it a couple days convince yourself that the recommendations that you're seeing are according to your expectations and then consider switching it to out.
00:18:17 [W] You should also always use poddisruptionbudgets to control how many pots can be restarted at any time.
00:18:29 [W] You should always do this regardless of using PP or not, but it is especially important if you're using vpa in Ultimo.
00:18:32 [W] As a safeguard you should always set minimum and maximum container sizes in the VP a object.
00:18:40 [W] And also keep in mind and VP doesn't activate the deployment spike. It only add the exported requests for pods as they are created.
00:18:55 [W] So a good practice is to take a reasonable Podrick West like the one taken from the peer recommendation and put it in the deployment itself.
00:19:04 [W] So just in case I don't know if someone turned out BPA you have a reasonable default to fall back to
00:19:06 [W] as I said before if you PA tracks.
00:19:09 [W] Trends over time so to do that. It has to store multiple days of history.
00:19:18 [W] That means that very significant changes should either be done gradually or they may even require any P object and to do this properly.
00:19:30 [W] You should also create a new deployment with a different set of labels.
00:19:33 [W] To function correctly vp8 takes its metrics from metrics Provider by default and kubernative.
00:19:42 [W] This will be metric server.
00:19:47 [W] It is very important to keep that Matrix provider healthy for VPI to be able to function and lastly in the same way as we page be a you should enable clustering for scalar in your cluster to make sure that as the workloads sizes
00:19:57 [W] Down the infrastructure can react accordingly.
00:20:02 [W] Now can you use both horizontal and vertical scaling all the same workflow? The short answer is no the more detailed answer as always start with it depends and execute question.
00:20:17 [W] Technically you can always use vpa in the recommendation only mode.
00:20:22 [W] It's safe.
00:20:25 [W] It doesn't change anything or your workloads.
00:20:27 [W] So nothing bad will happen to you.
00:20:31 [W] However, in order to have confidence in the results you get you should not configure HP and CPU utilization and use vpa at the same time. The options that you do have is to either use
00:20:42 [W] Custom metrics for HPA like requests per second or use absolute values for hva targets if you all want to scale on CPU.
00:20:52 [W] And you should always make sure that HPA gets enough traffic.
00:21:05 [W] So it should never be at minimum number of replicas for extended periods of time.
00:21:05 [W] Now in this set up switching vertical part of the scalar to auto mode is a bit risky and it shouldn't be done without deeper understanding of both HPA vpa and understanding your
00:21:21 [W] Characteristics so if you want to do it, we recommend extensive testing before hand.
00:21:27 [W] Now, how does weepy saving money?
00:21:31 [W] If you use vpa your workloads only request the resources that they're actually need to run safely and efficiently this helps you keep those costs in line and make sure that you are utilizing your infrastructure. Well,
00:21:47 [W] Without vpa you just use your best guess and what we've seen is that usually people to be on the safe side significantly over profession. Sometimes a couple times.
00:21:57 [W] Now, let's move on to the third and the last robot in our autoscaler family.
00:22:06 [W] This is cluster of the scalar. We've been talking about HP a and vpa and they are both targeted at scaling user workloads, which cluster of the scalar does it adjusted underlying Computing infrastructure
00:22:20 [W] It's notes for pods that don't have place to run into cluster and it also compact underutilized or empty notes.
00:22:29 [W] It's important to note that it is based on simulation of scheduler decisions and declares Padre quests. It doesn't use any actual Dynamic Matrix like usage or observed
00:22:45 [W] request for seconds
00:22:45 [W] We'll take a look at how it works in practice.
00:22:49 [W] So here we have simple infrastructure for a nose for pods free puts our big one is small and I would like to remind you. Once again that decides that we're talking here is coming from the declared pot requests.
00:23:05 [W] So for just one small part that we're seeing at least some free space on the Node. So if a new pod comes
00:23:14 [W] Scheduler will see that it can actually fit into the fruit now and that's where it will place it.
00:23:25 [W] There is no work for cluster. Just killer in this situation.
00:23:33 [W] However, if all modes are kind of full then the scheduler has no way of placing the pot and you notice needed and this is when cluster of the scalar kicks in it talks to a cloud provider and
00:23:40 [W] Requests springing up any not like this then you notice provided and now the scheduler seized in your space and puts the pending part of the empty node.
00:23:50 [W] Now on some occasions notes might not be utilized to different potential if we take a look at this picture.
00:24:01 [W] There are small pots on the 3rd and the 5th note and they have no spring them selves cluster out go scalars job is to notice that it can move the Pod is no to the final and this way one now.
00:24:13 [W] It will become empty and can be safely.
00:24:15 [W] safely. Delete it just like this one simple trick to reduce your monthly bill.
00:24:22 [W] Now when she's cluster of the scalar here, we also had a short answer and it's always and what I mean always if you have any infrastructure that is more complicated than two or three notes or if you have any changes in your cluster of the over time
00:24:37 [W] Like we're close coming and going scaling up and down.
00:24:46 [W] You should use cluster autoscaler to make sure your infrastructure reacts to those changes.
00:24:47 [W] Plus there are two scalar works with multiple Cloud providers.
00:24:54 [W] see the list on the slide here. In addition to that. There are some Cloud providers supported out of the core repository, which we ask seek out the scaling do not closely follow.
00:25:02 [W] Now to enable cluster of the scalar in your cluster.
00:25:08 [W] You should follow cloud provider specific guys some environments like gke they have a built-in easy to use switch to turn on cluster of the scalar in others.
00:25:20 [W] They may require additional manual steps. Please check the appropriate documentation for details.
00:25:23 [W] Best practices when using cluster of descaler.
00:25:32 [W] you should always use the hosted version of cluster to scare though if it's available as it is likely to be optimized for better experience.
00:25:39 [W] If you are managing your cluster two scalar yourself, please pick the one that matches your plaster version.
00:25:43 [W] There may be some subtle differences between versions of scheduler and cluster autoscaler. And this may result in some very subtle and very hard to debug issues.
00:25:55 [W] And lastly again Define poddisruptionbudgets for your applications.
00:26:06 [W] When discussing DPA that you should always do this, but for cluster descaler, it will let you control how often it can restart your pods when it wants to scale down a node.
00:26:14 [W] Now for the fun part with savings cluster autoscaler will usually drive your cluster of utilization measured in terms of the requested capacity to something
00:26:30 [W] About 10% about the scale them threshold and the skills of treasure chest shoulders configurable and by default it's 50% So generally you will see somewhere around 60 percent utilization in the cluster.
00:26:45 [W] If you're not using cluster two scalar, your utilization depends mostly on your own bin packing skills and on large clusters that is actually really difficult and what we sink seeing their out in the wild on average it would be significantly
00:27:02 [W] The work done. It would have been if you have cluster of descaler enabled.
00:27:07 [W] Okay, we've met all the robots in our cute after skimming family and stockist lovely getting to the end.
00:27:17 [W] So I would like to give you a little bit more information about sick outer scaling itself.
00:27:23 [W] We meet every Monday.
00:27:26 [W] most of our contributors are in Europe. So the meetings are held at 4 p.m.
00:27:31 [W] European time, which is unfortunately 7 a.m.
00:27:32 [W] At the West Coast, but we also have quite an active select Channel where we talk all things about this chaosmesh.
00:27:38 [W] me
00:27:39 [W] and as to where our staff lives new stuff, like clusters killer and vpa sits in the kubernative softer. Scalar repository HPA is the oldest outer skin a robot and it sits in the car kubernative
00:27:55 [W] Squirtle is rapper.
00:27:56 [W] And at this virtual keep con dition.
00:28:03 [W] We have a couple more after scaling talks that we encourage you to check out just to mention two of them.
00:28:10 [W] There will be a Sig of the scale indeed dive with more in-depth description of cluster of the scalar and horizontal product a scalar. And also there will be a auto scaling and cost optimization on kubernative stock that
00:28:22 [W] Also, we'll dive into advanced topics and clustered of scalar and horizontal put out descaler with custom metrics.
00:28:31 [W] And that is the end of the talk.
00:28:35 [W] Thank you very much for tuning in and now we will have some time for questions.
00:28:40 [W] Hello 123. Can you hear us?
00:28:49 [W] Mother can you hear me?
00:28:55 [W] Yes, I can hear you very well.
00:28:58 [W] So this setup works.
00:29:00 [W] Let us go through the question one by one.
00:29:05 [W] That's Resource Management like limits request have impact in auto-scaling.
00:29:10 [W] How should we configure limits request with auto scaling?
00:29:17 [W] this question I believe was partially I answered during vpa section and also in HPA so you
00:29:23 [W] always set to requests in your pots either and manually by putting some reasonable value there or by trusting VP recommendations that the reason for that is that
00:29:37 [W] At kubernative heavily depends on request when placing Pots if you put nothing in a request, it means that your pot no requirements and can be placed on any node which may not necessarily be
00:29:52 [W] Entirely true and if you happen to put too many pots on a single note.
00:29:58 [W] sitting on it
00:30:02 [W] I think I think you're breaking up.
00:30:12 [W] I think yeah, I think this is better.
00:30:21 [W] Okay.
00:30:26 [W] I haven't heard anything.
00:30:28 [W] But thank you for for the information.
00:30:30 [W] So limits are not that much needed.
00:30:33 [W] However, they can save you some time and save your cluster in case of what's going wild. For example, if your pot start using much more than is requested limits is the way of
00:30:47 [W] Keeping them in some rivers in the book constraints.
00:30:51 [W] So yeah, something creamy to is a good practice.
00:30:57 [W] It's not that much required as requests requests are very very very much required for auto skin.
00:31:03 [W] So I just wanted to add one more thing.
00:31:10 [W] So from vertical put out to scare perspective.
00:31:18 [W] We PA can recommend you both can set of your requests and limits and and as like Martin said like requests are for example, very important
00:31:29 [W] Just can I just like we said in the presentation?
00:31:35 [W] This is what cluster two scalar is looking at because this is what scheduler is using to play spots on the nose.
00:31:42 [W] Okay. Second question how HP is deployed?
00:32:02 [W] Is it a part of know to does every node in the cluster have it or is it a cluster of lever and my daughter goes all nodes and pods to within them. So this second option is true.
00:32:10 [W] It's a cluster white concept. It is controller runs on the master. It's a part of core kubernetes API. So it's
00:32:20 [W] Actually enabled on every class to Racine's 2015 or something like that.
00:32:26 [W] So you all have it.
00:32:38 [W] uses metric server or anything that implements metric API for getting metric. So it's important to keep metric server healthy just like we mentioned in the presentation.
00:32:44 [W] His BP always preferable to HPA mayadata.
00:32:56 [W] So I think actually BPA and HPA are tailored to different kinds of applications.
00:33:02 [W] We touched upon this a little bit in the presentation like Martin said for example, HPA is good for stateless workloads that also also when you have to handle
00:33:14 [W] Quick spikes because this is what we PA is not really tailored to do and vpa you can say you can always use it in the recommendation mode.
00:33:30 [W] So it's because it's not interfering with with your pods to take a look at what recommendations it's it gives but in general it really heavily depends on on your application and it
00:33:43 [W] Will be case-by-case basis.
00:33:45 [W] Okay, next question.
00:33:56 [W] Can you share the link to the PowerPoint presentation? We can probably put the PDF link somewhere will find the best place for it probably
00:34:08 [W] Like in the patent in the past Cube cons. We added the link to the presentation entry in the schedule.
00:34:19 [W] Probably this time.
00:34:20 [W] We'll do the same.
00:34:26 [W] Could you post the link to the VP repo here?
00:34:29 [W] But I believe you already did it.
00:34:31 [W] Yeah, I can make that question public for everyone to take a look at that link.
00:34:37 [W] If a pot is using note I operations as well.
00:34:52 [W] Can we use vpa to Monitor and scalar up and down? I hope so usage of odds.
00:34:57 [W] So not exactly sure what you mean by iot.
00:35:10 [W] IO operation as a signal it doesn't change I owe her a request in classical kubernative.
00:35:23 [W] You don't put iot we quests input specification and vpa at the current time in not working with online.
00:35:37 [W] memory
00:35:39 [W] So the answer probably is no.
00:35:44 [W] Yes.
00:35:44 [W] Sorry.
00:35:46 [W] Sorry for for the interruption.
00:35:48 [W] Yeah, I think the answer here is no the main factor is that I operations are not a resource first-class resource and kubernative.
00:36:01 [W] I think there are some sort of discussions to change this but I haven't really monitored closely and I think it would be more.
00:36:07 [W] And a change that affects the whole kubernative ecosystem and then auto-scaling could follow up.
00:36:16 [W] Will the slides be available afterwards?
00:36:24 [W] The answer is yes, probably Q conference will also publish the video.
00:36:29 [W] At least they did in the previous years.
00:36:32 [W] Can we enable vpa for crd objects?
00:36:38 [W] The answer is yes, your crd object has to implement scale sub resource.
00:36:52 [W] And I think this is actually nicely supported in in crd starting from I don't remember which kubernative version but probably 1.16.
00:37:01 [W] So if the if CR D support scale sub resource BPA is capable of scaling it.
00:37:08 [W] Mmm, are the slides available somewhere here? Yes, they will be somewhere available and they will probably also the video
00:37:26 [W] For off somewhere.
00:37:29 [W] I don't know exactly where how to come up with best resource config for a pod. Is there any open source tool available?
00:37:39 [W] I believe we covered it in this presentation. The answer is vpa.
00:37:41 [W] And there's another page of questions.
00:37:53 [W] Should we change our configuration for each cloud provider?
00:37:59 [W] Not sure exactly. What configuration are you referring to?
00:38:11 [W] I assume it's Pottery Quest and the answer is maybe and probably because one CPU May mean something different.
00:38:13 [W] Grant on let's say Amazon then it means on Google Cloud.
00:38:22 [W] So once one CPU may be more or less performant.
00:38:25 [W] everything depends on the cloud.
00:38:27 [W] I don't think that one CPU read but request is heavily standardizing kubernative.
00:38:40 [W] So yes, you should probably adjust your configuration a bit when you move from cloud one cloud provider to the other
00:38:42 [W] How does cluster autoscaler work with AWS Auto scaling group sits works?
00:38:53 [W] Part has been there for a belief like three years or something something like that.
00:38:58 [W] I would ask you to go to the cluster of the scalar repository. There is quite decent documentation about how to configure cluster autoscaler with AWS auto-scaling groups.
00:39:12 [W] Can we use vpa and cluster autoscaler together? If yes, then what are the use cases but
00:39:23 [W] All right, and I think this is the last question we can take here and then we're going to do move to the cube con maintainer and slack Channel and we'll continue answering so basically
00:39:39 [W] V PA plus cluster two scalar is an excellent combination because vpa makes sure that your resource requests are actually what your pods need. They really
00:39:54 [W] They're associated with what your pod is using and then cluster of the scalar will take those resource requests and decide based on if you need more space on this, but if you need more space or if we if it can actually scale.
00:40:08 [W] down the cluster
00:40:11 [W] Okay, so I think that's it for the life QA.
00:40:23 [W] Well, it's not us. We will continue on slack.
00:40:27 [W] So thank you all for joining us today.
00:40:37 [W] Kotas like Auntie. If you don't have any questions, we also are you will have some questions later.
00:40:44 [W] We strongly invite you to our weekly seek out of scaling meeting 7 p.m. PST for 7:00 a.m.
00:40:46 [W] 4 p.m. Standard European Time.
