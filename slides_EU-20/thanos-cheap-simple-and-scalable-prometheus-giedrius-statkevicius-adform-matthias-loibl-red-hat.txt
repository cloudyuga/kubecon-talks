Thanos: Cheap, Simple and Scalable Prometheus: JCTJ-8752 - events@cncf.io - Wednesday, August 19, 2020 8:19 AM - 119 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:05:01 [W] Hey everybody.
00:05:02 [W] Welcome.
00:05:03 [W] Hey everybody.
00:10:25 [W] Welcome to the finals introduction for Kube Connie. You and August 20 20 today. We will introduce you to Thomas. But first we want to make sure that all of you out there are
00:10:37 [W] In great staying safe, and I hope we hope that that you're still having fun attending the skip card.
00:10:47 [W] All right with that out of the way.
00:10:48 [W] Let's go.
00:10:53 [W] Let's get into it and let's start introducing Karma. So I'm Mateus lawyer.
00:10:55 [W] I'm a software engineer at Red Hat working on the observability team.
00:10:59 [W] They're on various openshift Integrations, for example with monitoring and I'm on contributor to tunnels the Prometheus operator Q, Prometheus.
00:11:10 [W] And I'm also the meeting organizer for the Berlin Prometheus meet up here in Berlin him give you some seniority systems engineer at that form part of the performance of herbs or ability services.
00:11:23 [W] So we provide probably fuse and of course will follow us kind of as a service to our development teams.
00:11:33 [W] also contribute to finest also maintain the small go Library, which lets you interact with the
00:11:39 [W] Fauna the graph Allah through its apis.
00:11:46 [W] So if you want to do that kind of stuff feel free to use it and contribute to it. And I also maintain a Blog at get schedules dot block.
00:11:51 [W] Alright, perfect. So as we said we're here to talk about sun or straight. So first some some numbers about Donna's that we want to like discuss with you.
00:12:04 [W] and since then we've grown significantly like in the beginning the project was started in November fully open source, and it like is currently part of the cncf sandbox and we're like actually
00:12:23 [W] In kubernative Martin actually a vote on the TOC for cncf is pending. So hopefully by the time you see this we might have already reached incubation incubation stage with Thanos. But since since
00:12:39 [W] They said we've still grown significantly, right?
00:12:49 [W] So we now have like 6,000 plus GitHub Stars, which is an increase of 1400.
00:12:58 [W] We have almost 80 new contributors, which has kind of crazy to think about. We also grew quite a lot. We have like basically doubled the amount of users and select the join our communication and discussions there and we also have
00:13:05 [W] Maintainers and three hours from now seven different companies on top of all of that. We also made sure that we have a proper governance in place because as a tense with some projects
00:13:21 [W] A proper governance in place because as a tense with some projects there are some companies really heavily involved with these projects and being employed by Red Hat.
00:13:30 [W] I can actually say that redhead is kind of like really strong and firm knows but we also want to make sure that we don't that we aren't able to overrule other opinions and voices in the community. So Thomas actually put a transparent
00:13:43 [W] And in some place where per company you would get at most two votes, right? All of that combined makes for a really great production users of Thanos these days so we know of 35 companies that have put
00:13:58 [W] Goes on the farmers website, but I guess they're quite some more but like these are the ones that we know about right?
00:14:08 [W] We're kind of like here to start with Prometheus. So how does like Prometheus and thousands actually integrate and like we already have media. So why do we like need Farmers to begin with right and
00:14:22 [W] That was actually integrate and like we already have media. So why do we like need Farmers to begin with right and they actually integrate and darkness Builds on top of of Prometheus.
00:14:27 [W] So first of all, why do we need monitoring?
00:14:32 [W] I doing Prometheus we're doing dance.
00:14:34 [W] So we want to collect and process then Aggregate and display almost like real-time quantitative data about a system such as clear accounts types.
00:14:42 [W] Errors, and we want to like process those times and serverless time. So it's it's quite significant to get this right and to make it work.
00:14:53 [W] So with this I'm heading the mic over to get Reyes to introduce him what to Prometheus.
00:15:05 [W] So when Building farmers we tried to stand on the shoulders of giants and then giant is Prometheus probably feels her as had lots and lots of improvements over the years. So first of all to get metrics from our own,
00:15:12 [W] Reviews the Prometheus client libraries and and Define what kind of metrics we want exposed.
00:15:23 [W] So there are four types of metrics and Prometheus move all of the data exposed. We need Prometheus to grab all of it.
00:15:32 [W] And this is the architecture of Prometheus.
00:15:34 [W] We will look at Prometheus from these four point of views. So we have the compactor which constantly compiles the data to make it more.
00:15:43 [W] The send request the Prometheus and get some data out of it.
00:15:57 [W] We have the script engine which actually goes to that metrics and point. It gets the metrics from it and puts it on the local storage.
00:16:07 [W] And finally we have probably the most important one and my humble opinion the rule and alert engine which allows you to aggregate some metrics data. So for
00:16:19 [W] people if you have some kind of curious that do a lot of computation or calculation, you most likely want to procure precalc with all of that and the rules engine permits you to do just that
00:16:33 [W] You are able to create new metrics from other metrics and alert. Again.
00:16:43 [W] Jay gives you the functionality to receive alerts some kind of messages when something goes wrong.
00:16:47 [W] Now.
00:16:54 [W] We're going to look like how to extract all of those components to scale up or maybe has here in this case.
00:16:55 [W] We have one Prometheus note that scrapes for different services on kubenetes that is running on the second note, but what happens if the Prometheus
00:17:04 [W] scripts, all of those Services goes down unfortunately, but in a lot of cases that Prometheus goes down goes down when we need it the most the typical way to solve this problem is to have more than one Prometheus server that scrapes the same services from that
00:17:19 [W] But there is a problem that's different from UCS servers will have a bit of different data different data at different times.
00:17:28 [W] here. We have one Prometheus instance and that rights to some local storage.
00:17:40 [W] But for example what happens if the local storage gets corrupted or we just lose that disk.
00:17:44 [W] We will lose all of the data for this problem to solve it.
00:17:48 [W] We usually do run multiple Prometheus servers.
00:17:51 [W] That get Federated into one in this case recording rules are used to pull limited amount of data from the clan Prometheus instances.
00:18:05 [W] And then all those metrics dumb jump to the main cluster from which you can retrieve the data, but what happens at a larger scale that might look like this and what happens is that the amount of data that
00:18:15 [W] Over the wire is doubled and this could be even worse if we have multiple Prometheus instances at the bottom on the client side.
00:18:31 [W] There's also this problem that as we keep adding different permissions instances on the client side for Central monitoring cluster will most likely run out of resources because we need more RAM
00:18:43 [W] Are not just from but of course over resources as well to store all of that data. And this is where Farmers comes in because we simply cannot scale that syrup. So improbable and 2017 started this project called Fun
00:18:58 [W] Written by bartek and Fabian so fun us is a set of components that can be combined to solve the aforementioned problems.
00:19:14 [W] So the farmers project has some aims defined. So the first goal of the farmers project.
00:19:23 [W] Yes, the be Prometheus compatible. This allows clients to seamlessly switch between farmers and Prometheus.
00:19:38 [W] The second goal is to have a global career you so we want to have like one point from which we can cover all of the metrics. Also. Ideally we would want to have a number that retention so that we
00:19:47 [W] Bull what has happened one year ago two years ago and to see kind of like Trends over time and the final goal is done something and compassion.
00:20:01 [W] It means that farmers performs the same compaction like that's the goal to perform the same compassion that profuse does and in addition Farmers also has the into performed on sampling now, let's take a look at
00:20:14 [W] So there is a primal fears set up again.
00:20:24 [W] So fundamental fundamental problem. Is that the central Prometheus and the monitoring coaster will simply not be able to keep up so much data as the client side is being scaled up.
00:20:33 [W] So instead of sending all that data all the time Central place.
00:20:39 [W] What if we had some kind of component which lets us find that data at at?
00:20:46 [W] Um instead of like this storing it in a central place by doing so we can scale up to a bunch of notes because we don't have to worry anymore about the
00:21:00 [W] Instead of sending all of that data all the time Central place.
00:21:01 [W] What if we had some kind of component which lets us find that data at at Curry time instead of like this store and get in a central place by doing so we can
00:21:02 [W] Node ingesting so so much data, so we have this mystery componentconfig.
00:21:12 [W] We can Curry from a bunch of profuse instances, but there is a problem.
00:21:20 [W] We don't know what kind of metrics do those Prometheus instances have.
00:21:22 [W] So we need another component that sits next to the Prometheus.
00:21:25 [W] Let's build that component and call it a cipher sidecar in addition to responding to those career.
00:21:31 [W] Is it can also provide us with metadata about what kind of metrics are available at all? And by using that data, the central monitoring can cluster can make decisions on where to send those carrots. We also want
00:21:46 [W] Do this to encode and decode the data and the most efficient way possible.
00:21:53 [W] We should use something like grpc.
00:21:59 [W] So at this point we have invented Farmers Farmers Korea components.
00:22:00 [W] It's in our Monitor and Koster and achieves the first goal of the farmers project. It implements the same from a face API, so you can switch between Prometheus and Farmers carry very easily.
00:22:12 [W] and there's also the sidecar which responds to requests over grpc and at proxies those queries to the previous instances, and it also has another point which provides us with method
00:22:28 [W] Very easily and there's also the sidecar which responds to requests over grpc and at proxies those queries to the previous instances, and it also has another point which provides us with
00:22:29 [W] Kind of metrics are available at all on those profuse instances grpc.
00:22:35 [W] methods are defined in a file called a protobuf.
00:22:40 [W] So this is kind of the main service that we provide we call the story pi and you can see that at the center.
00:22:48 [W] There are two main methods that info method which gives you the metadata information and the serious method which gives you serious data. It's also working.
00:22:58 [W] Mentioning that there is a farmer's deep left talk the last coupon some Diego we won't go into detail right now, but you can look at that talk as a consequence of this design Farmers Curry is actually fully stateless.
00:23:13 [W] So we've solved also another skill and problem.
00:23:16 [W] It means that we can scale Farmers Curry as we want.
00:23:25 [W] We can create a bunch of instances and point the user journey of them.
00:23:27 [W] How about scaling the profuse itself just like before
00:23:28 [W] We can obviously create a bunch of Prometheus servers and have them scraped the same information and have the same side car next to them proxying of those requests.
00:23:43 [W] We can use the same font as Curry and put it in front of one or more fun outside course do the same thing as the main cluster. But this time you won't have to send the request the same request to
00:23:54 [W] So we've made a lot of progress in scaling it up our previous instance, but we still haven't solved one problem.
00:24:10 [W] And that is that the local storage feature of the profuse instance can explode at any time and that will invite ably happen as you scale up your monitoring. So what if the
00:24:20 [W] Can explode at any time and that will invite ably happen as you scale up your monitoring. So what if the fungicide car could also upload all of the data that Prometheus produces to a central location?
00:24:26 [W] Also upload all of the data that Prometheus produces to a central location.
00:24:29 [W] And in fact, that's what it does.
00:24:32 [W] It allows us to avoid provisioning a fixed amount of storage for your trivia profuse instant.
00:24:42 [W] So introducing another component to our finest Fleet, that's the funnest or it's needed because once you upload all of the data to the remote object storage
00:24:51 [W] You need to somehow pull it down and to retrieve all of those metrics that were uploaded to the remote object storage by using the same story pi.
00:25:04 [W] We can connect this to find a scary and that's again one of the benefits of farmers.
00:25:16 [W] It's because all of those components implement the same API so we can mix and match all of those different components together and as we scale up and down or found a store instances because they're also stateless. There's this problem that
00:25:27 [W] funnest or cash has some information about metrics in the remote object storage to make the Keurig more efficient and slides previously shown last year.
00:25:42 [W] All of those different caches were not being shared between different feminist or instances.
00:25:48 [W] And that's why we added memcached support.
00:25:51 [W] So now we can store chunks information and tunnel store.
00:25:55 [W] For instance has because they're also stateless. There's this problem that
00:25:57 [W] funnest or cash has some information about metrics in the remote object storage to make the Keurig more efficient and slides previously shown last year.
00:25:58 [W] All of those different caches were not being shared between different feminist or instances.
00:25:59 [W] And that's why we added memcached support.
00:26:00 [W] So now we can store chunks information and ton of store.
00:26:01 [W] And like the shared between two or more instances all of the chunks information, and and finally we have that fourth goal to have done something and compaction.
00:26:09 [W] We need another component and that component is called fameless' compact no surprises there. So it takes the the compaction engine
00:26:22 [W] Finally, we have that fourth goal to have done something and compaction.
00:26:23 [W] We need another component and that component is called fameless' compact.
00:26:24 [W] no surprises there. So it takes the the compaction engine other of Prometheus and implements the same thing on top of the remote object storage.
00:26:29 [W] So you get the same benefits that Prometheus provides.
00:26:31 [W] But just on the remote object storage finally there is this new component that has been recently added.
00:26:41 [W] It's called replicate. So there are cases where you might not be able to store some data in some remote locations to do some kind of for example.
00:26:51 [W] Law such as the GDP are so for Family Circle K component lets you copy over testdb blocks from one object storage to another to recap first
00:27:07 [W] which implements the same problem facing API and connects double-story care components but forms that application and there is the fungicide color which proxies the request to the Prometheus and
00:27:23 [W] Blocks that it reduces to a remote object storage.
00:27:35 [W] There's fun a store which acts as a remote object storage adapter and response to the same story Pi curious.
00:27:38 [W] There's the funnest compact which performs compaction down sampling and also applies the retention policies and there's also the replicate component just told you about so it's
00:27:50 [W] X is a very nice tool and some cases very you want to copy data from one or troubled object storage to another and all of this is very nice, but there are cases where you cannot.
00:28:06 [W] Control the client side.
00:28:15 [W] Obviously most of the cases probably all unlike all of the cases you're able to control your own prodyna t service, but sometimes you cannot control the other side and that's where another componentconfig Lon that Murphy
00:28:26 [W] Aside and that's where another component comes in that my fears will tell you about.
00:28:28 [W] So in some cases you might actually not be able to like go from the central Prometheus and talk to the individual clusters. But you kind of have like an air gap situation where like the cluster her somewhere and you still want to have the sex Center a place where you can like
00:28:45 [W] A place where you can like have a global view of everything happening, right?
00:28:53 [W] So for that we might like think about new component where we can have these clusters kind of like sent their send their data to however if we just if we just do that and we
00:29:01 [W] We can like infinitely scale it up or like like say have like a hundred clusters are pushing their metrics to the central place where like ending up in the same situation as before where we kind of like take down our own system.
00:29:17 [W] So in computer science, we actually have a quite nice solution for this which is a hash ring, right so we can we can actually take these half rings and send like from various places the metrics so we actually take the name of the metric
00:29:29 [W] We hashed and and then that's kind of what we use to like consistently put the data into the individual notes in the hatchery in right?
00:29:45 [W] So we kind of like solve the problem of being able to handle the load. We can like horizontally scale our system and we can at this point just by putting more compute in terms of CPU and memory
00:29:54 [W] Each other we can kind of like handle more load efficiently.
00:30:04 [W] However, if one of those notes goes down, we're still in bad luck because then we start losing the data of that one node, right and to prevent this we can actually do something which is called replication so we can all the data that comes in
00:30:13 [W] We start losing the date of that one node, right and to prevent this we can actually do something which is called replication.
00:30:16 [W] So we can all the data that comes in we can kind of like in like our case.
00:30:25 [W] For example, we just replicate three-time. So what we do is we say okay this time series that that should be start we take this and we want to store it like on three individual separate nodes and then we are already
00:30:32 [W] Three times.
00:30:32 [W] So what we do is we say okay this time series that that should be start we take this and we want to store it like on three individual separate nodes and then we are already able to say okay if two of these
00:30:34 [W] If two of these nodes are accepting our request and storing the data, then we're in a good position.
00:30:43 [W] So we might not even care about the third one which is like the perfect world.
00:30:55 [W] We obviously always have this third one, but we can lose one and thus we can always be up in just data. Even why for example rolling out of there is some problem or like we shipped a bed deployment and like it takes down
00:30:58 [W] That's going to be rolled up first, but not a very we still are able to save the data in two of the three replicas, right?
00:31:09 [W] So that's really nice and then we can put that hair strength either into the monitoring cluster. We can even like put it somewhere else and I have like a griffon with a query component talk to that hash in the head shrink Lego
00:31:23 [W] Implement the same story Pi so query components which Farmers receiver it needs to ask for specific metrics and then can serve the actual requests.
00:31:38 [W] Okay, but now that we have introduced this hash rate, which is really nice.
00:31:50 [W] We can actually take the same principle of the sidecar and like the sidecar the thumb of South Koreans actually looking at the file system and every two hours Prometheus were right a new mutable block out of the red hat love to disk and then the cycle will
00:31:56 [W] Take that block and shift to object storage and the same is true for the thumb side for the farmers receivers.
00:32:04 [W] The founders receivers are essentially a headless Prometheus and that can kind of adjust data and uses the Prometheus 10 serious database like everything is kind of the same under the hood and it will also every two hours cut a block and then take that and
00:32:16 [W] Just and then the cycle will take that block and shipped to object storage and the same is true for the thumb side for the farmers receivers. The founders receivers essentially a headless Prometheus and that can kind of adjust data and uses the
00:32:18 [W] As to the same object storage so that your thumb aside Karis my ship to the same like all the data is kind of like coming together at Central place that kind of like gives you the ability to to store these metrics for a very long time.
00:32:32 [W] What we call the phone has received component. As I said there was actually today the talk by Lucas from my team on adjusting a million metrics without knows receiv.
00:32:48 [W] Unfortunately that happened before the stock.
00:32:50 [W] So you might want to go and check that talk out as a recording.
00:33:01 [W] Maybe you were able to catch it. But the last thing that I want to like quickly mention is the final screw component. So with all of this we are still
00:33:04 [W] still unable up until now to kind of like alert on this or like like new metrics with recording rules on top of this entire system, right?
00:33:14 [W] So that's where the thumbs ruler comes in and that's kind of like the same component that we know from Prometheus. But as a separate component, you can deploy this and then this is actually pointed to Thanos Korea. So it will actually go through the entire entire saying
00:33:27 [W] Proton the so like like new metrics with recording rules on top of this entire system, right?
00:33:29 [W] So that's where the thumbs ruler comes in and that's kind of like the same component that we know from Prometheus. But as a separate component, you can deploy this and then this is actually pointed to Thanos Korea. So it will actually go through the entire entire saying
00:33:31 [W] bring the entire system as any other final square root component and transparently be able to actually evaluate rules and kind of like know when like an alertness is about to Fire and sent this to an alert manager,
00:33:43 [W] The nice thing is that you can do this across all the clusters, for example, so like for every single instance of Prometheus should still there. But now if you see some problem across all the Clusters, that's really the beauty now that you
00:33:59 [W] With the phone's folder and like across all the Clusters a lot in it.
00:34:10 [W] All right, let's recap. So again Prometheus at the at the very bottom.
00:34:17 [W] We already know about the query component of them has but now we also added a new component called the thumbs receive component to like be able to create these are strings and kind of like receive a metric sent from a remote place or
00:34:25 [W] I'm gonna call the farmers receive component to like be able to create these are shrinks and kind of like receive a metric sent from a remote place remote Prometheus.
00:34:30 [W] We already know about the store. So we reuse that one because we already had it and it's quite nicely integrated with the receiver.
00:34:36 [W] And then there's the real component.
00:34:38 [W] So recapping we kind of started with this like one application doing it all and it's really nice and it like that's how it should be if you want to alert kind of like in a proper fashion for the like
00:34:53 [W] We kind of started with this like one application doing it all and it's really nice and like that's how it should be. If you want to alert kind of like in a proper fashion for the like notes and
00:34:54 [W] Goat so close to your targets as possible and we had the query engine and he's like different other components but we kind of like taking the time and split those up. So we now have the curry component and thermostat like graph on
00:35:08 [W] Screw component has right itself can talk to we now have the site power and the receiver that are talking to either Prometheus that scrapes actually targets or can actually receive metrics from the outside.
00:35:24 [W] So that's how we like ingest data.
00:35:29 [W] We actually at the pot mm replaced the local storage with object storage and then we actually had the compact in store which were kind of like those components inside of Prometheus and we kind of
00:35:41 [W] of made them able to talk to object storage to compact on those and down sample with the data coming from object storage. And finally we had the rule and alert engine in Prometheus, and we now have the thermos rule out for that and that can again
00:35:54 [W] API talk to alert manager. So really like taking all of these bits and pieces that were then Prometheus and made them horizontally scalable.
00:36:04 [W] So if all of this is quite enough for you, there's a cat a coda which will walk you through the individual steps and setting up Farmers locally.
00:36:16 [W] get really some some hands-on experience with them.
00:36:18 [W] If you want to deploy found out there like four projects one of which is keep Thanos which I'm maintainer of and it's super similar to keep Prometheus.
00:36:33 [W] It's compatible with Prometheus and you can like kind of like integrate both of them and there's a hand chart which we have in the thymus Community organization now trying to kind of like combined and bring together maintainers of different
00:36:43 [W] Working like the singer nice chart that is like supporting most use cases.
00:36:53 [W] Then there's a night Thomas Operator by Banzai Cloud, which can also check out and finally there's a nice project at the docker compose project for Farmers, which allows you to set up basic sounds locally for development.
00:37:03 [W] So in the recent month, we actually had quite some participants for Google summer of code and Community Bridge Quixote actually implemented the late deletions and that's already been kind of like done but on top of that we currently have
00:37:18 [W] Project the summer.
00:37:27 [W] So the first one being how schita she's working on a per series metrics retention which allows you to delete metrics if they get too old for example, like in general it delete specific metrics with Thanos then Prime is working on.
00:37:36 [W] Retching and extending the funnel cri-o would react so similar to Prometheus where we go to a react based UI same work as being done for Thanos and then which is working on versioning the website doc. So for each Thomas
00:37:52 [W] Versioning the website talks. So for each Thanos version we should have like separate dogs.
00:37:56 [W] that like are pretty nice pretty nicely browser button discoverable than the ashes working on per request query tracking and limiting so you can kind of like guard your infrastructure
00:38:09 [W] Up with bed queries and finally Sonia's working on improving the capicola two turrets that were mentioned earlier to even add more nice tutorials there for all of you to work with and then there were these two
00:38:25 [W] Who talks first one? We already mentioned and the second one was on scaling Prometheus and how they got some farmers in the cortex.
00:38:39 [W] So cortex dominoes are ever evolving and like stealing stealing but like level rating on on various components and functionality and you really should check out that talk.
00:38:46 [W] It's quite interesting what's going on there?
00:38:49 [W] And with that we're done. Thank you and have questions.
00:38:51 [W] All right.
00:39:00 [W] I hope we are life.
00:39:03 [W] All right, cool everybody.
00:39:07 [W] Um, so we have a couple of questions we can start addressing. Obviously first thing that like if there are any questions after we're done with the Q&A feel free to continue chatting with us on select in the
00:39:18 [W] Um, and as kind of a first announcement after the talk, we can kind of like briefly touched on this during during the talk, which was recorded like three months three weeks ago in those
00:39:35 [W] Actually reached consensus on Thanos reaching incubation state. So there's going to be a blog post on the cncf Block as far as I know later today some somewhere like Pacific
00:39:50 [W] At noon or something. So essentially for everyone in Europe like later this evening.
00:39:57 [W] there will be an official announcement.
00:40:10 [W] So yeah, like darkness is officially incubating and hopefully we can we can even improve fairness for everybody in the future even more and have an event even more striving Community around the project and product.
00:40:14 [W] Anything to add k3s now, that's some news. Okay.
00:40:18 [W] Yeah exactly.
00:40:20 [W] It's for a long time.
00:40:21 [W] Yep.
00:40:22 [W] Exactly.
00:40:24 [W] So yeah, I really pumped about this for sure.
00:40:29 [W] Okay, so we have a couple of questions. So is there a tutorial anywhere where you set up this Central less profound Aya and finest stack with a block storage from scratch Gary
00:40:39 [W] So you can find the I guess the tutorials and cut the code should suffice especially after after the recent improvements to them by our community burst student
00:40:56 [W] They should walk you through everything like and if you have any more questions, you can feel free to come and ask any questions that you have.
00:41:08 [W] Yeah, and we are continuously improving those.
00:41:14 [W] There's anything kind of missing in the Katakana tutorials as give you a set. And obviously those are open source.
00:41:20 [W] Those are part of our documentation.
00:41:25 [W] So there's anything missing feel free to open an issue or a pull request and prove those so it's better for you and for everybody.
00:41:28 [W] All right.
00:41:32 [W] So how does Thanos compared to M3 M3 allows for exemplars and integration with jaeger.
00:41:42 [W] So actually I'm not too familiar with three like we talked to them every now and then but I never like really is it myself I can I can tell you about Thanos so Darkness itself is instrumented with jaeger.
00:41:52 [W] I have is that if you run Thanos essentially we have liked races throughout the system if their queries coming into Farmers, then we were we were obviously Trace them and those really helped a lot in the past.
00:42:08 [W] Trying to figure out like performance bottlenecks and improving query performance for Thon has itself.
00:42:22 [W] So yeah, like we don't store any any traces with with Thanos.
00:42:23 [W] It's purely a metric system, but we integrate it.
00:42:27 [W] Store any any traces with with Thanos.
00:43:38 [W] it's purely a metric system, but we integrate it.
00:43:39 [W] We are integrating with a Jaeger more like as a user and then for example nurse I know that there are a lot of like discussions happening in kind of like for Thanos Upstream Prometheus and
00:43:40 [W] Yes, because even if you're not using all of the futures for ability to have one API that you can set Curry stew is nice because you get all of the
00:43:53 [W] Previously and I think this Tyson a bit to divert question where someone asked is there any point in using funds for a single cluster of dozens of notes and hundreds of apps I
00:44:09 [W] As you know, when you're a star probably feels it has to do some things and if you have really huge Prometheus serverless that it can take quite some time and fun us plays nicely with that and that it allows you to charge your permit fuse
00:44:25 [W] by the number of metrics that the targets provide
00:44:33 [W] yeah, exactly and like I mean you get like all the benefits of Thanos like long-term storage and like being able to ship those blocks to to to object storage which kind of like makes it makes
00:44:48 [W] This the retention on Prometheus itself.
00:44:55 [W] So you might even like free up some SSD storage on on the on the volume for Prometheus itself, and these kind of like benefits. They are still there.
00:45:07 [W] So yeah, if that's something you want to want to do then it's even find for example cluster. I would say as well.
00:45:09 [W] To do do you already have another question?
00:45:17 [W] I like to kind of like a recover a question either cash or Cash asks does LTS fun us hashicorp tax removed the necessity of provisioning persistent volumes for Prometheus or it still
00:45:29 [W] so I in my opinion it's and we recommend users to still add some persistent volumes to Prometheus because the data even if you're using remote, right it's not
00:45:47 [W] Talk hiccups can happen, but obviously it doesn't appear at the other end immediately.
00:45:55 [W] So there are still some delay between when your permission is gets the new data and when it appears on Farmers receive another side card that delays even longer the bread default block
00:46:08 [W] Missed two hours. So that's how long your Prometheus instance should be able to store data.
00:46:16 [W] And you it's nice that sidecar will retry uploading the blocks that haven't been uploaded yet when it comes back again.
00:46:27 [W] Yeah, exactly. Totally second that and I think we're actually out of time.
00:46:36 [W] I just got the one minute morning.
00:46:37 [W] It's a couple of seconds ago.
00:46:43 [W] If you again if you want to talk to us go on the maintainer select Channel, we also have like outside of Kube condoms just a thunderous channel on the cncf select.
00:46:49 [W] So feel free to reach out there Twitter followed panels on Twitter and yeah keep in touch.
00:46:57 [W] Hopefully you haven't the funnest.
00:46:58 [W] Exactly exactly.
00:47:00 [W] .
00:47:02 [W] Yeah. All right.
00:47:04 [W] So thanks for tuning in everybody.
