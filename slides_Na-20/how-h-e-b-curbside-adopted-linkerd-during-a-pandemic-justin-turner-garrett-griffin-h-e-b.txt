How H-E-B Curbside Adopted Linkerd During a Pandemic: BUGY-2088 - events@cncf.io - Thursday, November 19, 2020 5:42 PM - 26 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hi, my name is Garrett Griffin and I am a senior engineer and Technical lead for the infrastructure and platform for digital fulfillment.
00:00:07 [W] Hi, my name is Justin Turner. I'm the engineering leader responsible for digital fulfillment at HEB.
00:00:14 [W] Thing about HIV curbside fulfillment systems and how on our modernization journey we accelerated things by adopting linkerd D.
00:00:25 [W] It's going to be a technical fact-based talk in the hopes that our adoption of linkerd e and the technical aspects of it benefit you if you're thinking about adopting a servicemeshcon.
00:00:41 [W] Additionally will talk about how we've this factored into our pandemic response.
00:00:47 [W] A little bit about H-E-B.
00:00:49 [W] It's a texas-based retailer that was established in 1905 as several hundred locations across Texas and Mexico and it's deeply ingrained in the spirit of Texas. You can talk to almost any text in and they're going
00:01:04 [W] how they love their H-E-B
00:00:51 [W] one of the services that we offer is curbside and Home Delivery this allows our customers to go shop online submit their grocery order and we do this shopping on their behalf in store and then hand it off to them at the curb or take it to their door.
00:01:08 [W] We do this through a set of fulfillment tools called fast.
00:01:11 [W] This is a mobile and web application that allows our in-store Partners to shop substitute manage orders navigate through the store and ultimately hand it off to the customer.
00:01:26 [W] We started early on with a monolith when the curbside was a proof of concept.
00:01:32 [W] This is how it began life and we started to run into challenges as we scaled up the business struggling to deliver quickly lots of risk associated with change and ultimately reliability issues
00:01:48 [W] On our modernization journey to break into microservices and if you want to learn more about the modernization journey itself or our reliability efforts. You can check out the the talks
00:01:56 [W] the slide
00:01:50 [W] early February we were very close to the finish line of our modernization efforts. We had all our services mostly complete with parity enough to start rolling into stores and we were aiming for May to start doing
00:02:05 [W] This is when things changed they change for me they change for you our customers in the world.
00:02:10 [W] covid hit this also changed the nature of the service that curbside provides. We went from being a nice to have convenience to a critical resource for slowing the spread of
00:02:25 [W] X's and a Lifeline for immunocompromised customers that want to reduce the risk associated with grocery shopping.
00:02:34 [W] this changed our priorities we want from primarily being focused on finishing up services to really reinforcing and enhancing the monolith to handle the oncoming load that we were anticipating as well as enhancing with features and
00:02:50 [W] Gaston finishing up services to really reinforcing and enhancing the monolith to handle the oncoming load that we were anticipating as well as enhancing with features and functionality like best available products
00:03:10 [W] Like best available products to handle inventory shortages snaps so that every customer that is around a curbside can utilize it and a variety of other features that helped us respond to covid.
00:03:26 [W] The side effect of this was that it pushed out our timeline.
00:03:30 [W] It was the right thing to do for Texans.
00:03:32 [W] But ultimately we still believe that the monolith was not the system that we needed at curbside anymore.
00:03:38 [W] And we wanted to before more features were added and the growing list of needs.
00:03:45 [W] we knew we needed to bring the timeline closer so that we could build the best system for Texans and really focus in on what matters.
00:03:57 [W] We did this through a couple of measures we reorganized yourselves to get through the work faster.
00:04:03 [W] We really leaned into our chaosmesh generic approach to make sure that what we were building was resilient. And then we adopted linkerd e is our servicemeshcon brought it further forward from our roadmap, which we were intending to get to
00:04:18 [W] Out and went ahead and prioritized it. This would help us solve. Our the hypothesis would is that this would help us solve some of the challenges we were running into as we built
00:04:33 [W] Is and the operational muscles around supporting them helping us solve a early observability challenges and ultimately like some of the retries and
00:04:38 [W] and ultimately like some of the retries and networking that we were starting to really have to wrap our head around to make sure that this was a reliable system when we started our Cloud Journey
00:04:52 [W] When we started our Cloud Journey we knew there would be challenges ahead of us.
00:04:56 [W] We knew we were going to split our monolith and a microservices and with that comes a lot of added complexity to start.
00:05:05 [W] We now had another layer of complexity within our own ecosystem having to worry about State and security at a whole new level on top of that our ci/cd system had begun to show us age while is a great solution for a while and become a little bit
00:05:20 [W] Servicemeshcon you'll eat over 80 servers and with that schema management had become more difficult as we have another team to help us in applying those changes which of course would require coordination. But now that we were getting a fresh start
00:05:35 [W] Those changes which of course would require coordination, but now that we were getting a fresh start we could start a very evaluating these things.
00:05:40 [W] What can we automate to simplify our ci/cd system when we be able to take a more iterative approach to schema changes even better.
00:05:50 [W] Could we apply those changes in our deployment pipeline since we're attempting to be completely stateless, but there be anything to help us with retries and time outside between services.
00:06:02 [W] where there any tools that could help us visualize all of this with research. We knew that a servicemeshcon is in our future but we had to figure out the
00:06:13 [W] Planning and going down the what if Rabbit Hole can only get you so far. So he set out to just do it. We defined our domains and decided on a service of split out of the monolith.
00:06:24 [W] It didn't take us long to figure out just how much is changing.
00:06:29 [W] We completely overhauled our ci/cd system branching strategy how we work through and thought about backwards compatibility with contracts that schema changes all the while learning new tools.
00:06:40 [W] Tools like Docker and kubernative we even began to learn new things about tools we use for a long time like how the jvm would act once it's containerized.
00:06:51 [W] It'd be fun.
00:06:52 [W] Just learn how hungry Java can be with memory.
00:06:56 [W] We thought this Cloud thing was supposed to be easy and solve all of our problems, but we realized pretty quickly that we could probably use a little bit of extra help.
00:07:05 [W] Here's the disclaimer friends a service mission is not for everyone while you get a lot of things out of the box you now have this extra layer of complexity.
00:07:14 [W] You have to ask yourself. If you really going to use the dashboards care about the metric scrapers or if you just really want em TLS. If you're only needing one of those things, you're just hurting yourself by going all in there's plenty of tools out there if you want
00:07:29 [W] After our research and reviewing our needs a mess just made sense.
00:07:27 [W] So we set out to integrate and see what would happen.
00:07:30 [W] We saw some pretty heavy deal breakers with some of them though at the time.
00:07:35 [W] We couldn't create our own internal load balancer as it wanted to handle that and take care of it for us.
00:07:41 [W] Others went. Let us create our own Ingress solution as it was built into one of the services it provided.
00:07:48 [W] There's also a case of a minor version update breaking our services causing a considerable amount of down time and time to fix it.
00:07:58 [W] Then we came to linkerd e it was lightweight plug into the existing ecosystem didn't foresee any Ingress Solutions on us and we could still use third parties tooling.
00:08:08 [W] These are made here was for us already.
00:08:10 [W] So we decided to plug it in and see how well it played in the sandbox.
00:08:16 [W] Out of the gate.
00:08:17 [W] We were pretty impressed with the documentation or proof of concept went almost textbook by the docks provided but the COI we ran our checks to make sure it would integrate Randon install command and he just started working.
00:08:29 [W] We did see side effects for any of our unmatched services. So we integrated.
00:08:36 [W] We injected the sidecar proxy into an existing service and we began to get that almost instant feedback.
00:08:42 [W] It worked a little too well to the point of being suspicious, but we were liking what we're seeing at the time.
00:08:48 [W] So we press forward.
00:08:49 [W] Out of the gate. We were pretty impressed with the documentation or proof of concept went almost textbook by the docks provided but the CLI, we ran our checks to make sure it would integrate Brandon install command and it just started working.
00:09:00 [W] We did see side effects for any of our unmatched services. So we integrated.
00:09:06 [W] We injected the sidecar proxy into an existing service and we began to get that almost instant feedback.
00:09:13 [W] It worked a little too well to the point of being suspicious, but we're liking what we're seeing at the time. So we press forward.
00:09:22 [W] But the POC going so smooth we decided to open this up and meshmark rest of the services for a particular namespace out of the box. We started seeing the gold metrics and a service map starting with starting to get us excited.
00:09:38 [W] During this time. We managed to find out the hidden gems of linkerd e we met with buoyant and the amount of help they gave us was staggering.
00:09:45 [W] They've been with us pretty much every step of the way since we decided we needed a servicemeshcon great having them along during this journey.
00:09:53 [W] Also the slack Community has been awesome to interact with keeping an eye on the channels has helped to stay ahead of issues when upgrading or seeing other potential pitfalls people have come across during their everyday lives with the mesh if you decide to
00:10:07 [W] With linkerd E. I strongly suggest having an engineer or to join the community and interact with them.
00:10:13 [W] It definitely won't be something you are going to regret.
00:10:18 [W] Now that we're hooked, we needed to productionize the system.
00:10:21 [W] We needed to enable High availability mode for production and wanted to keep everything in code AS is the standard are clusters were already written in terraform.
00:10:31 [W] So by this point we weren't strangers the concept. Thankfully there's already a Helm chart written for linkerd e
00:10:38 [W] This is a standard. Our clusters were already written in terraform. So by this point we weren't strangers the concept.
00:10:45 [W] Thankfully there's already a Helm chart written for linkerd e
00:10:50 [W] but there's a slight hiccup. We ran into during an upgrade we found out we couldn't just upgrade the linkerd E image when moving from one version to the next we had to upgrade the helm chart version as well which make contract changes that
00:11:05 [W] I'd upgrading has been pretty paint us.
00:11:09 [W] by the way, though read the documentation closely as problems can arise between the chair and keyboard a line was missed while reading and we may have marked R Coupe system namespace for proxy injection,
00:11:24 [W] which freely wouldn't be an issue until you do something crazy like bringing up a new cluster which will begin to struggle to become healthy because it's going to wait on linkerd E proxy injector to become healthy,
00:11:39 [W] Which is going to wait on the critical systems become healthy, which is waiting on linkerd E.
00:11:44 [W] So on and so forth it became a fun class is debugging but could have been avoided.
00:11:52 [W] Now that we were productionize we're feeling pretty hungry for more features.
00:11:56 [W] But of course growing pains are bound to happen while dipping our toes in the world of canaries.
00:12:01 [W] We found that sometimes our schemas were not as backwards compatible as we thought as in they weren't and things were breaking so accommodations and new mindsets had to be made and upgrades to our charts would follow but more on that one later before we
00:12:16 [W] Get to that point we had to get our service profiles up and running and linkerd e a service profile is a resource that gives you fine grain information for scraping.
00:12:25 [W] Basically, you make the route using the power of break X and linkerd E takes from they're getting reports that / root basis.
00:12:33 [W] Thanks to this.
00:12:33 [W] You can get those sweet sweet timeouts and retries.
00:12:37 [W] Problem is that you have to generate the profile come through five hundred lines of the Amal add in what you want for the retry or timeout and then deploy it from there.
00:12:47 [W] We wanted to take it a step further while still leveraging what we had and keeping a Services engineer involvement minimal.
00:12:54 [W] setup should be out of the box when they spin up a new service and it should only have to change values one place to leverage the features from linkerd e
00:13:04 [W] The other problem we started to face was we have this scary New Concept out there that until now the engineers didn't really have to mess with it, which is existed in they got a lot of cool stuff out of the box.
00:13:15 [W] So whenever something would break now that they've integrated with these new things we'd start to get messages like is linkerd e down
00:13:24 [W] we would go and check it would come out to be that there was an issue pulling an image or there's a config change.
00:13:30 [W] So the proxy sidecar would never get healthy which would trigger alerts for the different team. So they would come to us and said that the service is seem so we need to find a way to build knowledge and confidence in our Engineers for using linkerd e
00:13:48 [W] Thankfully one of the first things we did in the beginning of the journey was created a template project that all the other services were created from using an open source tool and became plug-and-play to get a new service up and running with all of our tooling already built in
00:14:04 [W] So now we ensure consistency across all of the services.
00:14:08 [W] So when new tools had to be added it was a pretty painless process and there's no guessing where everyone was one of the shared libraries.
00:14:16 [W] We were already using with spring Fox which would give us Swagger docks.
00:14:20 [W] We found an open source tool that would take our Swagger's spec and output the service profile.
00:14:26 [W] We made a few changes to the client baked it into our docker buildpacks.
00:14:33 [W] Now in one place our Engineers could get retries and timeouts couple this with our custom chart and meshing the service became a two-line affair when they were ready.
00:14:44 [W] They would flip the switch in their charts and the service would be meshmark the next appointment leveraging Flagger another third-party tool we're able to also add Canary deployments. So as long as the service was meshmark
00:15:04 [W] And Having learned our lessons we added in a way for the engineers to be able to skip the analysis and deploy the service and the manor previous two canaries existing.
00:15:13 [W] This would be in cases where there's a breaking change going out or some kind of incompatible schema change.
00:15:22 [W] Now it's time to build a confidence throughout the team's when we found the check commandant's linkerd ecl.
00:15:28 [W] I knew it'd be pretty easy to automate and alert based on the output with 14 lines of bash and a Cron job.
00:15:34 [W] We were able to create alerts based on the health of linkerd e the next steps will be the self-heal based on that output.
00:15:41 [W] most likely for things like certificate expiration a big part of our development process in our group is introducing chaosmesh the services breaking things and new and exciting ways and non prod there's a certain kind of Glee
00:15:56 [W] As captured in an engineer's heart when they're told they're allowed to break something deliberately.
00:16:00 [W] So linkerd e we came up with our hypothesis and our testing criteria, and we went to work we're able to test and verify that if we scale the pods to hundreds of instances the proxy to still be injected while doing this we end up discovering a
00:16:16 [W] Action issue completely unrelated linkerd e we're able to remediate it and keep on going.
00:16:22 [W] We are also curious what would happen if while we were scaling service pods with the traffic being distributed correctly if the linkerd E destination pause where a less than desired replica state?
00:16:34 [W] Through our tooling we're able to watch the traffic being directed without any scaling interference are big fears were around the proxy injector and the control plane though.
00:16:45 [W] We're able to bring down the injector and still have our function our services function.
00:16:50 [W] They would not be able to scale or deploy new versions, but they're still working. This brought us relief in that we knew we would have time to fix the issue If This Were to occur occur in production.
00:17:04 [W] Finally if we brought linkerd e completely down we were able to verify similar results as the proxy injector being down there still be issues with scaling deployments, but the services would still be able to
00:17:19 [W] UK this was the level of peace of mind that we are looking for this is what was going to let us sleep at night and it's hard to put a value on that.
00:17:29 [W] the winds we get from linkerd E have been pretty awesome canaries are now in place by most of our services giving us another layer of protection when deploying we've created the gating possible to be able to begin deploying during business hours which in the past was unheard of
00:17:45 [W] Was great about this integration is it is what third party and it was just another instance of it. Just working with linkerd e some of the immediate benefits we saw was during an active incident where we were able to verify
00:18:00 [W] immediately in our dashboard that our services were receiving the traffic correctly sending out our requests but never getting anything back because of this is this was proven and we were able to work with the teams necessary to resolve the issue at hand
00:18:15 [W] The benefits are made even better by the fact that it is not disrupted our workflow.
00:18:20 [W] We continue to document as we did before add a couple extra lines and we've set up a the necessary items same day fixes.
00:18:29 [W] become easier and faster because we got these Protections in place now, we're also able to reduce the amount of Bigbang deployments which were always error Pro.
00:18:41 [W] To my team this journey has definitely been worth it.
00:18:47 [W] The benefit of all of the efforts that Garrett just spoke to our that we were successfully able to accelerate the completion of our microservices.
00:18:54 [W] We went to our first store in early July and since then have been rolling out to the remainder of the company.
00:19:02 [W] This is allowed us to do our part to make sure stores were there for our customers in the in their time of need.
00:19:09 [W] We've been working on building our operational competency of our new microservices while taking key throughput off of the monolith that still in place which has helped with our reliability and resiliency.
00:19:23 [W] We've been learning how to best observe support and deliver safely and quickly to our new Services.
00:19:31 [W] Thanks to the new tools that are available to us with our linkerd e servicemeshcon.
00:19:38 [W] Thank you for attending today. If you have any questions or want to discuss further, feel free to reach out to Garrett or myself directly.
00:19:45 [W] If you want to learn more about what we're doing at HEB check out digital H-E-B.com for a Blog of several of our technical efforts. Again, thank you, and we appreciate you.
00:20:05 [W] Hi everyone.
00:20:08 [W] Hope you enjoyed the talk and we have our first question in here as a local San Antonio Texan.
00:20:17 [W] I was an early adopter of the service and continue to use it. Now.
00:20:22 [W] My question is what metrics have you all been able to compare between this time 2019 versus 2020 today, so
00:20:34 [W] A lot of the like business metrics are around order volume, which rose exponentially.
00:20:41 [W] There's also you know technical metrics such as our throughput timing of operations, and then there's the team and project metrics for our four key metrics
00:20:57 [W] our of our health around RS re practices and then also the technical metrics of you know, our up time and service Health versus what we were at it 2019 and if you look at the
00:21:12 [W] Resiliency presentation we did before we do a super deep dive on that.
00:21:20 [W] So the next question, this is cool.
00:21:24 [W] Do you have a logging Stacks set up Garrett?
00:21:27 [W] You want to speak to that?
00:21:28 [W] Yeah, essentially what we have is we're on gcp.
00:21:34 [W] So a lot of our logs just go straight there straight from standard out and we also use datadog for that one.
00:21:41 [W] The really big important thing is you have to have structured logging or else. You're just going to get lost in the noise if you have
00:21:49 [W] Form of ability to find by the trace ID and all of that you're good to go.
00:21:54 [W] So taking the time in the beginning when you set up your services to do that is very important.
00:22:02 [W] The next question is if you had stayed with the monolith, what would the covid response have been like?
00:22:07 [W] Well, we had stabilized the monolith and we had to spend a lot of time investing in it with the covid response as well because we weren't fully rolled out with the microservices that said us taking that key
00:22:22 [W] Traffic off helped with our up time our performance.
00:22:27 [W] So the it would have been choppy or Waters had we not gone down this path and ultimately some of the things that we've delivered lately this far into the pandemic would have taken much longer for us to deliver had
00:22:42 [W] I had to put them into the monolith.
00:22:50 [W] Next how did you narrow down your choices to linkerd e Garrett?
00:22:58 [W] We just we tried out a bunch of them.
00:23:01 [W] We went out there and
00:23:06 [W] we just kind of see okay, is this woman really work for us?
00:23:09 [W] What's the pain that up look like if it goes down with it look like with the braid back up and after this trying a couple of meshes linkerd e was just the most simple one and it gave us everything we needed out of the box,
00:23:24 [W] So it just kind of makes sense for us to go without.
00:23:30 [W] Yeah, the Simplicity was the big draw for us.
00:23:34 [W] We didn't we made the choice to go ahead and adopt that early.
00:23:39 [W] But if we had, you know done that haphazardly or taken on too much complexity at that point the journey, it would have made everything worse and made it take longer and then introduced a ton of risk and linkerd E was
00:23:54 [W] Super simple for us to get started with to understand it didn't introduce any performance problems.
00:24:00 [W] It gave us exactly the features. We needed to give us the resilience and confidence to move forward with minimal amount of effort and overhead. So that's from a strategic perspective why we also chose it.
00:24:15 [W] The next question all things considered.
00:24:17 [W] What is the biggest win that you consider from?
00:24:19 [W] Adopting a servicemeshcon? Ticular linkerd e
00:24:25 [W] anything I've seen so far as the observability once we were able to get this all set up.
00:24:32 [W] Especially with the packet tracking that you're able to do with linkerd e is so great. Like we were we were in an active incident.
00:24:40 [W] We went to the dashboard and immediately.
00:24:43 [W] We saw that our package from just getting dropped.
00:24:45 [W] We were sending them out correctly, but we never got a response back.
00:24:47 [W] back. So our meantime to innocence was really sped up because of this and we were able to help the other team find a resolution really quickly.
00:24:59 [W] Yeah, it's it's really nice to have a live tap of data. That doesn't it's very clear what's going on in your system at least from a golden metrics perspective
00:25:15 [W] that also enabled what I view as most valuable it enabled us to start moving Faster by using those metrics to drive some of our Canary analysis with our deployments and may it
00:25:31 [W] Significantly improved our change safety our ability to roll back, you know with that Canary analysis.
00:25:40 [W] We've been frequently deploying to these Services now since pushing them out and since we've done got this in place, when we do experience an issue, we don't know that our users have ever seen it it just safely backs off
00:25:55 [W] And that's I know that you don't have to have linkerd e but it made it really easy for us to have linkerd e and to have those metrics available.
00:26:09 [W] It looks like there aren't any more questions.
00:26:16 [W] so if y'all want join us over in the slack Channel
00:26:23 [W] on the main Coupe con Slack
00:26:27 [W] Yeah, the pop up just came up.
00:26:28 [W] But thank you all for attending and you know, we feel free to reach out to get her myself. If you have any further questions after the conference take care of yourselves and be safe out there.
00:26:39 [W] there. Thank you.
