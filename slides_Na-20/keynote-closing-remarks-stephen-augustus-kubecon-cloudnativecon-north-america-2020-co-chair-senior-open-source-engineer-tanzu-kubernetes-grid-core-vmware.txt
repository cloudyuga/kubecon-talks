Day 2 Keynotes: HEVK-7917 - events@cncf.io - Thursday, November 19, 2020 10:39 AM - 224 minutes

Participant: wordly [W] English (US)
Participant: wordly [W0] English (US)
Participant: wordly [W1] English (US)

Transcription for wordly [W]

00:00:00 [W] Kubernative will be boring as soon as we can make it boring.
00:00:03 [W] Maybe after I retire it's already boring.
00:00:06 [W] So I think we're well on our journey towards kubernative being boring.
00:00:10 [W] It'll only be boring when it's done.
00:00:12 [W] It'll only be done when it's no longer relevant.
00:00:59 [W] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:13:59 [W1] Kubernative will be boring as soon as we can make it boring.
02:14:02 [W1] Maybe after I retire it's already boring.
02:14:06 [W1] So I think we're well on our journey towards kubernative being boring.
02:14:09 [W1] It'll only be boring when it's done.
02:14:11 [W1] It'll only be done when it's no longer relevant.
02:15:10 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:18:49 [W1] Kubernative will be boring as soon as we can make it boring.
02:18:52 [W1] Maybe after I retire it's already boring.
02:18:56 [W1] So I think we're well on our journey towards kubernative being boring.
02:18:59 [W1] It'll only be boring when it's done.
02:19:01 [W1] It'll only be done when it's no longer relevant.
02:20:00 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:20:49 [W1] Hello.
02:20:50 [W1] Hello everyone Cube con Cloud knative con virtual attendees.
02:20:55 [W1] I hope all of you had a great opportunity to check out the content from yesterday.
02:20:59 [W1] We've got I hope you met some new people hung out in slack the hallway chat.
02:21:06 [W1] I know that I got an opportunity to meet a lot of new folks as well as Place among us later in the night.
02:21:12 [W1] So I hope you're having fun. And we're going to get started with the day to Keynotes. So kicking us off my lovely co-chair Constance caramanolis.
02:21:24 [W1] This constants is a principal engineer at Splunk.
02:21:29 [W1] And yeah, she is going to be talking to you today about the opentelemetry collector and how it empowers end users.
02:21:37 [W1] So check it out.
02:21:48 [W1] Hi again, everyone.
02:21:50 [W1] I'm Constance.
02:21:51 [W1] caramanolis. And today I will be talking to you about the opentelemetry collector and how it empowers and users.
02:22:01 [W1] Now you're probably wondering what is opentelemetry.
02:22:03 [W1] It is a merger of open senses and opentracing from early 2019.
02:22:08 [W1] All right, these were to project successful projects.
02:22:12 [W1] They're working on very similar goals. And the decision was done to merge these into one project to unify our resources.
02:22:20 [W1] It is a collection of tools apis and sdks and yes that actually means that there are their support and several languages actual libraries that you can
02:22:31 [W1] Use the adopt in your and applications today for more details in terms of the instrumentation side today.
02:22:40 [W1] We focus on tooling I highly suggest that you watched last year's keynote with Liz Fang Jones and Sarah Novotny introducing opentelemetry and also some Kook Connie you talks.
02:22:55 [W1] Now why opentelemetry now the moment observability is here as we build more complex systems.
02:23:04 [W1] We need to better understand this our systems and so it is the time for a project that focuses on merging multiple Telemetry formats metrics traces actually logs.
02:23:19 [W1] and it is all about prioritizing end users and as you'll see in our landing page we talked about it being a vendor agnostic because unfortunately when it comes to Telemetry and the resulting back ends vendor lock-in has happened,
02:23:35 [W1] It depends on where you are in your car knative Journey if you are lucky and or an early adopter of cognitive vendor lock-in might not be a strong story for you. But for meant I can guarantee that for many of you watching today vendor lock-in is a pain.
02:23:50 [W1] and this actually hinders your ability to choose the proper back ends for you because vendors will, you know will provide better tooling and better instrumentation that works better with their back end, but this isn't
02:24:06 [W1] The way in the future that is not the point of cloud native and so opentelemetry is partial is strong addressing that by making sure to provide vendor agnostic instrumentation tooling.
02:24:17 [W1] Now you're probably thinking great another migration and I don't blame you. It is annoying to think about another migration, right?
02:24:26 [W1] You're probably thinking about like a job servicemeshcon Burnett. He's like what about containers you security so many things working there. And so I deserve this face now I am here to tell you that in terms of adopting and migraine opentelemetry.
02:24:42 [W1] Retrieve it isn't an all-or-nothing.
02:24:43 [W1] There is a way for you to start adopting opentelemetry collect opentelemetry and specifically The Collector without changing much of your existing infrastructure.
02:24:52 [W1] And so the path forward is oh The opentelemetry Collector now, you're probably wondering what is it?
02:25:01 [W1] It's an executable that receives State Telemetry data transforms it and sends the data long. It works on metrics and traces.
02:25:10 [W1] It supports several popular open source protocols.
02:25:13 [W1] I want to call this out because right opentelemetry does have its own protocol for spec for a PS4 metrics and traces but we do acknowledge that there have been other protocols for metrics and traces before
02:25:28 [W1] And so to successfully adopt opentelemetry doesn't mean that you have to use our protocol right away it you can still use your existing protocols as I'll show in a little bit.
02:25:39 [W1] And it's a pluggable architecture.
02:25:40 [W1] If you like. The code base is pretty nice and clean.
02:25:43 [W1] They're very clear interfaces. So you can Implement, you know, either new ways to get data in transform the data and Export it out.
02:25:51 [W1] There are two ways.
02:25:53 [W1] You can deploy it one as an agent sidecar or as a standalone binary instance, you know examples for my why you might want to write it as an agent is that if you want to do sampling decisions or off if you have a high throughput servicemeshcon,
02:26:08 [W1] And you want to offload the Telemetry as soon as possible to some other binary so you don't create a bottleneck in the application. That's one way or you can just have it a standalone instance because maybe you adding some extra information to your metrics and traces after it before for the long trip back
02:26:24 [W1] Either one it is all the same binary.
02:26:28 [W1] Now the internals of The Collector we said it before but if there's three parts would getting data in and the formal term is the receivers and so receivers do is that once you get something with an once
02:26:43 [W1] Within a certain protocol it gets translated into internal format and this internal format the processors consume this internal for format and then transforms the data optionally little spoilers in a bit and
02:26:58 [W1] Then it forwards data along to whatever for Mike what?
02:27:04 [W1] Now we're using you migrations is a few complex things about it to think about it one is that you can have multiple Telemetry workloads, right different teams might have used different protocols.
02:27:15 [W1] They might be sending these different back ends.
02:27:18 [W1] ends. It's a little complicated there depending you know, when the Telemetry is added the quality of the Telemetry can differ.
02:27:28 [W1] So you want to standardize this and then controlling where the data goes?
02:27:32 [W1] It's grades. You generate all this Telemetry but controlling where it goes in the end is so important.
02:27:39 [W1] Now, let's tackle the first thing managing multiple workloads.
02:27:42 [W1] This is naturally a simplified service Services application depiction.
02:27:48 [W1] And so all these services use only one Matrix format and some things to Prometheus back end and now they're actually two different at racing protocol Zipkin any acre in this case and you know
02:28:03 [W1] Services right.
02:28:04 [W1] They might have these values hard-coded what makes it really hard is say if you're the observability team is how do you manage all of these right?
02:28:11 [W1] It's if you don't provide a central way for teams to send their traces to some data collection.
02:28:18 [W1] Yes, the collector it's hard to audit where things are going.
02:28:22 [W1] So what I'm suggesting to you today is that you can use a collector to maintain all of your workflows. Now, there's a few ways you can do it one is that you can use a collector to create
02:28:33 [W1] in this case since we had three different protocols one for metrics into for traces three different pipelines one for Prometheus one for Zipkin.
02:28:40 [W1] Sorry go back and one for jaeger.
02:28:43 [W1] But also if you wanted to say, you know, what we're tired of having two different backends for traces.
02:28:50 [W1] We're going to merge them.
02:28:52 [W1] What's really cool about the collector right was hinted at in the pipeline's diagram.
02:28:57 [W1] is that since the port supports both receivers? It can see on one pipeline.
02:29:01 [W1] You can say I want to support Zipkin and Jaeger and then it will convert both of those into the internal data format. And then after once it goes through the rest of the pipeline exporting whatever format you want it to be this allows for you to have multiple protocols.
02:29:16 [W1] Most for Telemetry data and send it to one back end or you know, maybe something even cooler the end.
02:29:23 [W1] Now, how does this new work out of like, you know, you have these protocols and a new service comes along and this you service owners are super excited about opentelemetry and they want to use the opentelemetry format now
02:29:38 [W1] Kind of like what we did with is it going to Jaeger Trace pipeline?
02:29:41 [W1] We can do the exact same thing.
02:29:42 [W1] We can add the oh tell protocol to the receivers for the metric and traces and just send the data alone.
02:29:49 [W1] This means that your service owners don't need to change protocols.
02:29:53 [W1] when you can control the data, yes, it does require changing where the data is, you know exporting to but it also means to that you don't have to force everyone to upgrade to the latest instrumentation right to the latest library or
02:30:07 [W1] all this gives you power to choose.
02:30:10 [W1] Hey, maybe we only upgrade the latest Services. Right only new Services uses latest protocol or you know, this one service it's needs to be re-written.
02:30:20 [W1] Let's do that one there.
02:30:21 [W1] This gives you power to choose when you adopt a new protocol.
02:30:25 [W1] Now let's talk about standardizing data. We have you know, now we can we get all our data working through but now we're seeing that the quality of data is and so great right and a few causes for this different Source different quality of data can be different languages, you
02:30:41 [W1] Naming for that maybe earlier data that doesn't isn't as rich as newer data because we've asked as we play around with systems for longer. We learn to ask better questions meanings evolved a term that we might have used for five years ago doesn't make sense
02:30:57 [W1] More right maybe you've added more regions. And before there were no region is used to use default and accidents happen as best as we try sometimes, you know, like Pai leaks right something like that and we'd way to catch that.
02:31:13 [W1] Yes, as you all know the collector does all of these things.
02:31:16 [W1] so it's called the a trees processor and this example, I'm going to be operating on traces and so say I'm missing an attribute right?
02:31:25 [W1] I have an earlier version of the application that wasn't setting environment before we just had one environment now, we actually have testing staging production.
02:31:36 [W1] And so using this processor I can say let me add this attribute environment prodyna.
02:31:43 [W1] And now my data will look similar, you know from your stuff.
02:31:48 [W1] And then I realized looking a little bit more.
02:31:50 [W1] Oh, oops.
02:31:51 [W1] you know what I got into a little joke saying that the west coast is the best coast and so I set region to Best Coast and it didn't make it wasn't as much of an issue back then because we're only deployed on the west coast, but now we actually have east coast and yes
02:32:06 [W1] East Coast is really cool too. You know, it can be the best coast.
02:32:10 [W1] So the best is Coast together, so I want to change Best Coast to West Coast.
02:32:16 [W1] And you know, I'm looking at the state of little bit more and oops. Sorry Steven.
02:32:20 [W1] I'll beat your name in logging, you know in setting these attributes.
02:32:26 [W1] And so that's not okay.
02:32:27 [W1] So I'm actually going to delete anything with username for my Trace to look like my span to look like this.
02:32:34 [W1] Now this is giving you just a few examples of how you can tweak existing data. And this is all done in the collector.
02:32:41 [W1] I haven't given any llamo files here because I think we're all a little bit tired of Gamal the moment, but this is all done with a few lines of animal.
02:32:50 [W1] This doesn't require application to change anything. This allows you to say the central observability team to control the quality of the data without pushing changes on to the application.
02:33:02 [W1] And the last part 14 your data along.
02:33:05 [W1] All right, we're getting got data in multiple receivers.
02:33:08 [W1] We have enriched it standardize the data and now we want to send it on out work.
02:33:12 [W1] And so right now in this example here, we only have we only sent one back end.
02:33:17 [W1] But what happens maybe if you had a hack-a-thon and two teams came up with two different back ends and you want to compare these back and side by side.
02:33:25 [W1] Well, this is great is that with similar to the receivers? You have multiple receivers we support multiple.
02:33:31 [W1] Orders and so with a few lines from few lines again will code that saving your eyes from at the moment is that you can tell the collector.
02:33:40 [W1] Hey send everything, you know, send all these traces to back in 1 2 & 3 and this has absolutely no implication on your application.
02:33:48 [W1] Yes implication your application. Absolutely none.
02:33:52 [W1] Now the statement I want to make a microphone want to make an even stronger statement is that this gives you the power to choose whatever backend you want, right? This is partially targeting the vendor lock-in is that if you can control how you collect all your data, you can also
02:34:07 [W1] Control where your data goes and this actually holds I know it's funny coming from me because I do work for a vendor but this holds us vendor accountable to providing a great back end for you, right if you have the power to choose where your data goes
02:34:22 [W1] We have responsibilities do to make sure that you have a bad experience with your data.
02:34:26 [W1] And so that's why I think that this is actually the coolest thing about opentelemetry collector is that this puts the power in your hands?
02:34:33 [W1] So to reiterate some of the things we saw today from complex operations to ease your operations, we can go from instead of you know, very disjointed ways to managing multiple Telemetry workflows.
02:34:48 [W1] It's a collector you can have multiple pipelines and multiple protocol support so you can
02:34:51 [W1] Do that with one binary one deployment if you want to gamble you can standardize a quality to climb a tree using the attributes processor.
02:34:59 [W1] All right, this is once again, no code changes and you get to control where your Telemetry goes.
02:35:05 [W1] This is leveraging multiple exporters and multiple protocol support now a few other takeaways is that it's not a requirement for you to use the opentelemetry protocols to Leverage The Collector.
02:35:18 [W1] There are a lot of really cool features in the collector that I highly stressed you're looking looking into if you're trying to collect and manage all of your telemetry.
02:35:29 [W1] There's also provides a way for you to adopt adopt new Telemetry protocols and only change what is needed right by say having one pipeline that accepts multiple protocols.
02:35:41 [W1] You can then say I keep everything, you know anything before a certain date only using protocol, you know Legacy protocol anything going forward using opentelemetry protocol.
02:35:51 [W1] This gives you more power to choose.
02:35:53 [W1] It's easily extendable write anything that is missing add it right. If you have a custom protocol crate, you know, it's an easy to add a new receiver and exporter to support your custom protocols.
02:36:08 [W1] And processors do more of the modify attributes.
02:36:10 [W1] They can do sampling memory limiting which protects the collector from booming because you don't know losing your Telemetry batching or retrying queued like standing with a kubernative metadata.
02:36:22 [W1] There are quite a few processes that are able to enrich the data and also control how data flows through
02:36:30 [W1] So I'm hoping that your little bit less annoyed with the call to migration and that this may be provides a path forward for you to look into opentelemetry.
02:36:40 [W1] So the collector and also instrumentation without thinking without it having to be a huge migration and only tensely updating small things that need to be updated.
02:36:50 [W1] It's maybe a little happier with this concept.
02:36:52 [W1] We are working towards a g a and you can
02:36:58 [W1] Find The Collector repository here collector and contribute their and there's a meeting every second Wednesday.
02:37:04 [W1] Thank you. Everyone. Keep Cloud native connected everywhere.
02:37:07 [W1] Bye.
02:37:11 [W1] Welcome back.
02:37:12 [W1] Thank you so much constants for that awesome.
02:37:15 [W1] And opentelemetry collector.
02:37:17 [W1] There's some great chatter happening in the Keynotes channel on slack as well.
02:37:21 [W1] So check that out including a link to a list following Jones is talk.
02:37:25 [W1] So next up.
02:37:27 [W1] are we have non davidov?
02:37:30 [W1] Ananda is a lead data scientist and distinguished technologist at hpe and she will be discussing how to leverage open source projects such as Spire Opa and Envoy to provide A fine grain policy for overlay for your
02:37:45 [W1] Your machine learning pipelines take it away.
02:37:57 [W1] Hi, welcome everyone.
02:37:58 [W1] This talk is about scaling machine Learning Without compromising privacy and security.
02:38:05 [W1] Thank you cncf for giving me this opportunity.
02:38:09 [W1] and thank you audience for your attention.
02:38:12 [W1] My name is Amanda, which I Dave I lead product management at hpe s Barrel container platform and machine learning Ops. My background is in technology transformation at medium to large Enterprises specialized.
02:38:27 [W1] And big data and machine learning operations over the next few minutes.
02:38:32 [W1] We will take a look at the complexities of the machine learning pipeline. What are some of the possible touch points?
02:38:42 [W1] And areas of concern and what are some of the technologies that are being used to mitigate risks and lower the exposure in machine learning by plants?
02:38:53 [W1] This picture is from a famous paper by Google where it talks about the technical debt and the various stages of machine learning that most people associate machine learning with algorithms
02:39:08 [W1] The MLK chord or the MLS self because there's a lot that happens before during and after this may run on one or many communities clusters depending on the deployments at
02:39:23 [W1] Stammers there are various actors.
02:39:26 [W1] There is where various personas that interact with different stages of the pipeline and there are services within kubenetes and Ingress for users to access those services
02:39:41 [W1] And also there are external entity. If you look at this picture, this is our representative architecture of a large financial institution that has a pretty large footprint of machine learning activities that happen across
02:39:56 [W1] Sweetie systems and across different groups, right? You have data pre-processing done by your data engineers. And then you have the actual machine learning process done by your data scientists
02:40:11 [W1] And then a deployment of a model so not getting into the detail of this.
02:40:19 [W1] What's of concern for us in the next few minutes is that there are several different processes as you can see here this perimeter there is end attack surface multiple different kubernative
02:40:34 [W1] These clusters probably in different trusted domain and then you have a number of different external systems that have to be accessed and for those who are familiar with security and kubernative by default.
02:40:48 [W1] There is really no security that's enabled. So if you do a threat model for this
02:40:54 [W1] You can see that the graph of how a user accesses the system and what are all the touch points they go through for various activities.
02:41:05 [W1] What services are talking to other services?
02:41:08 [W1] So you have authenticating and authorizing to individual service by users. Then you have to establish trust between Services tenants and you know across sites if needed.
02:41:24 [W1] Because of external systems that are accessed as part of this pipeline you have an increased attack surface, especially with data access.
02:41:34 [W1] Now you have privacy concerns here. Our path to resolution is happening in multiple phases using best of the build Technologies such as ldap and I DC connector as you can see here.
02:41:49 [W1] And M TLS between systems.
02:41:54 [W1] Using you know jot and spiffe E identities.
02:42:00 [W1] For preventing attacks surface.
02:42:03 [W1] This is something we are paying special attention to with admission control various parts creating policies and network controls.
02:42:15 [W1] And for extended policies such as limiting your authorized Docker registry and things like that. There are Opa Rego policies.
02:42:26 [W1] For phase two there is experimentation going on to establish more transitive identity between users services and also external resources that are outside of the Cabela's Network and for
02:42:41 [W1] Relation between on-prem and Cloud trust remains. We are looking at spiffe a federation.
02:42:48 [W1] Thank you and looking forward to seeing you on another talks.
02:42:52 [W1] Very exciting.
02:42:56 [W1] Thank you nan de next up. We have Jonathan beri.
02:43:00 [W1] Jonathan is the founder of stealth and today he will be talking discussing the need of how and where networking Protocols are supported. It's uncover roadblocks.
02:43:10 [W1] So Jonathan take it away.
02:43:23 [W1] Hi everyone in this session.
02:43:26 [W1] We'll be discussing protocols specifically networking protocols.
02:43:31 [W1] I'll cover some of the challenges we faced upon protocols to production and where we as a community to make it easier.
02:43:38 [W1] Let's go.
02:43:42 [W1] My name is Jonathan beri, and I work on an iot startup. You can find me on Twitter at berry berry kicks if you want to chat about protocols or anything else.
02:43:55 [W1] We should probably begin our discussion with HTTP as it is the networking protocol.
02:44:01 [W1] Most people are familiar with its the lingua Franca of the web and a large percentage of web services are built using http.
02:44:13 [W1] Now once you have your HD TV service, you're not done.
02:44:17 [W1] You need a bunch of things to deploy to production and in the cloud things like load balancing routing security and observability.
02:44:29 [W1] Since HTTP is a popular protocol different projects and operators provide these things for HTTP out of the box.
02:44:39 [W1] Which is great since you would otherwise have to build these things yourself.
02:44:44 [W1] But HTTP isn't the only protocol you use all the time.
02:44:51 [W1] There's also DNS. Everyone's favorite service you use it when you want to configure a server to master domain name or enable services within a cluster to discover each other.
02:45:03 [W1] But what if you wanted to create a DNS service?
02:45:06 [W1] Turns out this is a thing that people want to do.
02:45:11 [W1] Well, you may not have realized that DNS is actually a suite of protocols and when you use services like Amazon Route 53 or Cordina s those are actually implementations of DNS protocols.
02:45:28 [W1] Dennis like HTTP has similar things it needs in order to be deployed to production.
02:45:36 [W1] However, since DNS is a less common protocol compared to http.
02:45:41 [W1] It isn't supported out-of-the-box by many of the cloud upon objects. And therefore you have to build a lot of these things 14s yourself.
02:45:51 [W1] And that makes it way harder to build your own solution that implements DNS.
02:45:59 [W1] And there are a lot of application and Industry protocols that are designed to fill a specific purpose.
02:46:08 [W1] I've listed a few examples here, but they range from synchronizing game state to streaming webrtc to the internet of things and actually iot is what first got me interested in exploring the topic of implementing protocols in the first place.
02:46:27 [W1] See, there are a ton of iot protocols designed specifically for the needs of connecting physical devices to each other and to the internet.
02:46:38 [W1] Some are optimized for power somewhere optimized for bandwidth others Implement standardized industrial consumer control planes.
02:46:47 [W1] Some are built on the Internet Protocol like UDP and TCP. While others simply can't use IP.
02:46:54 [W1] As a start-up we're interested in implementing many of these protocols in our Cloud solution.
02:47:02 [W1] the challenge my startup phases in bringing iot protocols to production is similar to gns in that these Protocols are less common than HTTP a lot less in fact and therefore aren't supporting easily by many of the projects
02:47:17 [W1] Services and clouds. We might want to leverage their for we're faced with the challenge of implementing load balancing routing Etc from scratch all by yourself. That means we can't leverage the wealth of
02:47:32 [W1] source for this community or take advantage of great solution providers
02:47:39 [W1] But I did say before the challenge of implementing protocols isn't specific to iot.
02:47:45 [W1] I want to see if other people in the community are also implementing new protocols and see what I can learn from them.
02:47:52 [W1] So I started talking to other folks like game server devs working on Project Argo Nats webrtc maintainers working on Pion Telco folks from the network servicemeshcon Direct in others.
02:48:06 [W1] Turns out they're all asking a similar fundamental question.
02:48:13 [W1] How can we Implement any protocol in a cloud native way?
02:48:17 [W1] This is clearly a community challenge something bigger than my startup or iot.
02:48:26 [W1] I'm a p.m.
02:48:27 [W1] So I did what we do best start a dock can go it to it now at bit ly / Beyond http.
02:48:35 [W1] It's a living doc.
02:48:37 [W1] that survey is a growing list of cloud native projects that might be used as part of a solution that implements a networking protocol it tries to hit identify which projects are specifically focusing on supporting non HTTP protocols and suggestions on where they can add additional
02:48:53 [W1] And we implementers might need feedback in contributions are greatly welcome.
02:48:59 [W1] I started the dock over a year ago and since that time contributors and maintainers have helped shave it at the same time projects across the cloud native ecosystem have added up streamed features that make it easier to implement new protocols.
02:49:14 [W1] His progress is both exciting and encouraging.
02:49:19 [W1] There's more projects than I have time to discuss in this session, but I wanted to highlight a few I think will be used by the most amount of those.
02:49:27 [W1] communities Envoy servicemeshcon face cloudevents and Network Services
02:49:36 [W1] Let's start with kubernetes.
02:49:39 [W1] Kubernative has a concept of Ingress which is an object that exposes networking traffic from outside the cluster to services within the current stable Ingress implementation supports HTTP go figure within the
02:49:55 [W1] Say it group has been hard at work developing the new Gateway guy to replace Ingress.
02:50:01 [W1] Like Ingress a Gateway routes traffic to a service that supports more protocols like UDP and TCP among other things with a goal to enable protocol implementers to create custom gateways in the future.
02:50:14 [W1] They already have demos you can try and it may be in an alpha look released by the time you're watching this report.
02:50:21 [W1] The envoy as a popular networking proxy by its nature needs to support a protocol in order to act as a proxy for said protocol.
02:50:31 [W1] Envoy has always had native support for HTTP, but due to its popularity began implementing support for protocols like redis and postgres albeit in a one-off fashion.
02:50:44 [W1] Over the last year or two, the team has added UDP and TCP listeners, which make it possible for protocol implementers to use Envoy to proxy any protocol based on those layer for Primitives which covers a lot of potential protocols.
02:51:00 [W1] Also with the introduction of webassembly support Envoy has now made it even easier for more people to implement custom protocols in the language of their choice.
02:51:11 [W1] One more thing since Envoy is used as a basis for other projects like servicemeshcon and observability tools and now becomes easier for projects like sto and Prometheus to support custom protocols in the future.
02:51:27 [W1] Next I'd like to highlight servicemeshcon face for SMI, which is developing a standard for servicemeshcon kubenetes.
02:51:37 [W1] One aspect of a servicemeshcon it manages traffic between services within Foster.
02:51:44 [W1] SMI defines a concept of traffic specs a list of subjects that Define how traffic flows between the mesh.
02:51:52 [W1] There are already some specs for HTTP TCP and UDP.
02:51:57 [W1] The intent is for protocol implementers to Define their own traffic specs and eventually contribute them Upstream to SLI. Where make sense.
02:52:07 [W1] You might notice some parallels between the Gateway API for Nettie's and the traffic spec here in SMI.
02:52:14 [W1] Gateway's are about traffic coming into the cluster while traffic specs are internal traffic Within.
02:52:21 [W1] So there's a potential the two projects to work well together.
02:52:25 [W1] I'm personally excited to see how these two efforts collaborate in the future.
02:52:31 [W1] Cloudevents is a specification for describing event data in common formats to provide interoperability across Services platforms and systems.
02:52:40 [W1] It defines how to serialize events in different formats like Json and protocols which of course include http.
02:52:49 [W1] There's already some specs for protocols like Kafka mqtt and Nats and cleared documentation on how to define custom / proprietary protocols.
02:53:01 [W1] Cloudevents is already being used today in projects. Like he knative idly will become more commonplace among event oriented microservices.
02:53:08 [W1] So having a way to define custom protocol bindings will make it easier for services that leverage non-http protocols to interoperate.
02:53:20 [W1] The last project I'm going to touch on is Network servicemeshcon fused with the other kind of service for the servicemeshcon your face.
02:53:29 [W1] Nsmcon to extend the existing kubernative networking model, but address networking use cases.
02:53:35 [W1] They need to go deeper in the stack like el34 L 2 layers or even ethernet frames themselves inspired by sto network is servicemeshcon the concepts of a servicemeshcon L2 L3 payloads.
02:53:50 [W1] Hence the name.
02:53:51 [W1] Telco 's and the cncf Telecom User Group are big fans of nsmcon.
02:53:56 [W1] One reason is how it enables them to implement cellular specific protocols in a cloud knative way 5G is repelling the Telecom industry fully embracing what they call Cloud Network functions many of the
02:54:11 [W1] Oceans have been classically implemented in very expensive Hardware found in cell towers would take a long time to physically operate with Cloud native functions Telecom operators.
02:54:21 [W1] to build these capabilities as services that can be more easily upgraded over time.
02:54:27 [W1] Lots of the functions are specific Niche protocols that are part of the operation of 5G networks and nsmcon has the means to configure them on top of case moist.
02:54:39 [W1] I'm only scratching the surface of what nsmcon fun fact because servicemeshcon Sly Castillo operate up the stack at L7 L4 and nsmcon LP and below.
02:54:50 [W1] You can use a servicemeshcon NSM. There's a bunch of advantages when combining these two concepts.
02:54:57 [W1] I don't have time to dig into that now, but the innocent grew have some examples.
02:55:03 [W1] with that I want to conclude with what you could do to participate if you're trying such a great something with protocols other than HTTP, share your story documenting use cases both real world and aspirational has been
02:55:18 [W1] The operation 5G networks and nsmcon has the means to configure them on top of case moist.
02:55:26 [W1] I'm only scratching the surface of what nsmcon fun fact because servicemeshcon is like sto operate up the stack at L7 L4 and nsmcon LP and below.
02:55:37 [W1] You can use a servicemeshcon NSM. There's a bunch of advantages when combining these two concepts.
02:55:44 [W1] I don't have time to dig into that now, but the innocent proof have some examples.
02:55:49 [W1] with that I want to conclude with what you can do to participate if you're trying such a great something with protocols other than HTTP, share your story documenting use cases both real world and aspirational has been the
02:57:49 [W1] If tool to help improve protocol support create issues hop on slack. The cncf poor communities is a great place or just hit me up on Twitter.
02:57:58 [W1] And if you're a project maintainer try to identify where your projects currently assume traffic is HD and look for places for accessibility. Oh and asking you is what they think.
02:58:11 [W1] And that's all I got.
02:58:12 [W1] Thank you. Everyone.
02:58:14 [W1] Here's a link to that dock one more time, and you can reach me on Twitter at Heretics. Cheers.
02:58:22 [W1] Thank you Jonathan for that wonderful presentation again harkening back to the end-users reach out to your end users and think about their use cases so you can build for that next up. We have vijoy append a feature is
02:58:37 [W1] Presentation again harkening back to the end-users reach out to your end users and think about their use cases so you can build for that next up. We have vijoy Pandey futurewei is a VP of engineering for emerging Technologies
02:59:00 [W1] Paying for emerging Technologies and incubation at Cisco and he will be discussing the challenges of making your API secure and and the waist overcome them.
02:59:19 [W1] Hello, folks.
02:59:21 [W1] I'm vijoy and welcome to Q Khan and I'm here to ask Marvin about where my security apis are.
02:59:28 [W1] It's also an ask for the community to step up our application security game. But before we begin we have to ask so who is Marvin?
02:59:38 [W1] If you haven't read The Hitchhiker's Guide to the Galaxy go and get a copy now.
02:59:43 [W1] Marvin is a robot with a brain the size of a planet by his own account.
02:59:49 [W1] He's at least fifty thousand times smarter than a human but he's being made to park cars pick papers check airlocks.
02:59:58 [W1] not really job satisfaction worthy since he is built with genuine people personality or GBP Tech. He also has human emotions. So he gets depressed with all of this mindless work.
03:00:11 [W1] but most importantly he is a Paranoid Android and all of these properties make him very suited for the State of Affairs around API security as you shall find out but first a little bit of historical context around
03:00:27 [W1] Knative as we all know a pocket textures are changing from monolithic that metal ovhcloud.
03:00:43 [W1] Over the last 3 Cube cons. We have discussed how the network needed to evolve to handle this transformation to Cloud native through the network servicemeshcon. Jecht. And today we are looking at this Cloud knative developer as she is building
03:00:58 [W1] Moving fast utilizing the Trove of apis from public Cloud providers such as providers and other services she is faced with the dependency graph complexity of a model globally distributed.
03:00:57 [W1] Microsoft is based app Security in such an architecture is a modern brain size problem.
03:01:03 [W1] By the way. This is the Microsoft this dependency graph of the Monza banking app and as she uses these globally distributed apis, which are either home.
03:01:13 [W1] All groan or from external providers the quality and security of these remote Services is often unknown.
03:01:20 [W1] The pivot on API reputation is not inherently part of the ci/cd process which might put your customers data eventually at risk.
03:01:31 [W1] This rest causes all kinds of unwanted Behavior developers want convenience and velocity security a sorry sack of sand sea Source pound the table for reviews and are still unsure whether the mm API is being used are safe or compliant.
03:01:47 [W1] From One Cloud ideas are always wanted that insecure code has been pushed to prod and all of these reviews and Gates and meetings just add complexity and opacity to the simple job of developing and deploying fast.
03:01:57 [W1] The simple job of developing and deploying fast.
03:01:41 [W1] Seeing all of this Marvin the Paranoid Android with the weight of the world on his shoulders steps in with this depressing Court.
03:01:49 [W1] Wouldn't it be nice if there really was a modern with a PlanetSide brain who could be observing the entire application lifecycle fund repo to runtime and identify the risks of using
03:02:04 [W1] All the teams concerned report that risk to security a sorry sack of sand sea salts and maybe even remediate that risk to customer Data before it happens.
03:02:13 [W1] Now that is indeed a modern sighs problem, but do not underestimate Marvel he could simultaneously plan the entire planets military strategy and solve all of these major problems of the universe
03:02:28 [W1] I didn't Pablo also compose a number of lot of eyes if you could do all of that. He could surely solve for the application and API security problem.
03:02:38 [W1] So what would the app lifecycle look like? If we had this Marvin the developer would take Marvin curated apis and move fast fast fast.
03:02:47 [W1] Marvin would notify security teams of possible issues and mediations continuously and cloud form teams could rest easy knowing they can report compliance in a real-time matter.
03:03:02 [W1] How would we train this morning?
03:03:04 [W1] What are these boundary conditions?
03:03:05 [W1] What are its parameters first and foremost we have to realize that it's all about protecting a customer's data.
03:03:13 [W1] The open internet is really the runtime for all modern Cloud native apps security attacks risk, both clients users and other services other applications and the new parameter of apps and security is really
03:03:28 [W1] Is down to an API or data object.
03:03:28 [W1] What would be some of the apps with questions we could ask this Marvin?
03:03:33 [W1] Is the correct in-house service image or artifact being used in creating a new app are we integrating with compliant third-party apis are we tracking RPC calls to make sure we do not import never is data and
03:03:48 [W1] these to ensure safety for elapsed
03:01:50 [W1] some of the broader questions we have to ask ourselves as the community could be if a distributed app does get broken into do we have all the tooling to drive meantime to detect and debug down can be formally model and verify
03:02:05 [W1] Logic in our business apps even across layers.
03:02:09 [W1] Can we the community help spread the word on API misuse?
03:02:14 [W1] If we aren't able to step up to the plate and answer these questions, I'm afraid Marvin has this to say to you.
03:02:22 [W1] Thank you for listening.
03:02:22 [W1] Please reach out to me.
03:02:24 [W1] I would love to continue the conversation visit us at the Cisco Booth where we have demos on app Security app and in for observability cloud native networking a lot more and we are highly and we would love to have you on board. Thank you.
03:02:41 [W1] Awesome presentation from vijoy.
03:02:45 [W1] I hope we can all please Marvin through moving forward when we're designing our apis.
03:02:49 [W1] So next up. We have a void doc to change to 10 ski why Doc is a staff engineer at Google and also one of the co-chairs for kubernative Sig scalability today.
03:03:03 [W1] He will be discussing improvements made to kubernative A's and SED that unlock
03:03:08 [W1] massive clusters running with 15,000 nodes. So enjoy
03:03:23 [W1] Hello Cube Khan. My name is Victor twins key. I work in Google and I'm part of this amazing kubernative Community for almost six years at this point.
03:03:33 [W1] There's I'm the TL of scalability and in connection to that during this presentation.
03:03:34 [W1] I'm going to talk about example features that allow us to run kubernative clusters with 15,000 notes.
03:03:43 [W1] But let's start with making this explicit.
03:03:47 [W1] Kubernative clusters with 15,000 nodes are already a thing.
03:03:53 [W1] As you might have heard earlier this year buyer crop science and Google together published an interesting blog post.
03:04:02 [W1] You can read their how thanks to running on g k clusters with fifteen thousand nodes with almost a quarter million, of course and over 1.5 petabyte-scale Fram
03:04:17 [W1] Process roughly 15 billion genotypes per hour as part of the pipeline responsible for deciding which seeds which are the final product should be Advanced for further experiments in their R&D Department.
03:04:20 [W1] Couple months later during joint presentation Twitter and Google together described how we approached validating that the applications that they are currently running on their messes Aurora clusters
03:04:35 [W1] Plated and run on g k clusters with 15,000 nodes.
03:04:37 [W1] But the goal of this presentation is to show that this work matters for all of you.
03:04:43 [W1] Even if your clusters are order or maybe even orders of magnitude smaller.
03:04:51 [W1] However, we need to start with understanding what scalability really means for kubernative.
03:04:57 [W1] Even though we tend to use node count or cluster size as a proxy for the overall scale.
03:05:03 [W1] It's actually much more complex than that.
03:05:07 [W1] scalability is a multi-dimensional problem with thousands of Dimensions such as number of services number of volumes pot turn and so on even though they in many use cases
03:05:22 [W1] Together with cluster size you need to acknowledge that not count is not the only thing that matters.
03:05:31 [W1] So, how did we approach scaling kubernative to the next level which 15,000 notes certainly is?
03:05:40 [W1] The core principle for any scalability or performance work is don't optimize blindly and always sort of real-life problems.
03:05:51 [W1] Almost every optimization is making a system a little bit more complex.
03:05:55 [W1] So it's super important to keep that in mind.
03:06:00 [W1] According to this rule. We first found users for whom running such a massive clusters would have actual benefits.
03:06:10 [W1] We started with understanding their use cases but also their motivations and as you may suspect those are the ones that I already mentioned.
03:06:20 [W1] The first one was buyer cropscience who is refusing kubernative to run embarrassingly parallel Isabel batch computations.
03:06:28 [W1] For them larger clusters immediately translate to having results faster.
03:06:34 [W1] In addition to that given that the users are generally data scientists who don't want to understand the underlying infrastructure.
03:06:43 [W1] They want to make it as simple as possible. Ideally a single cluster.
03:06:47 [W1] Migrate they microservices apps to kubernative.
03:06:48 [W1] As I already mentioned they are currently running them on my so Sarah clusters, which can handle even 40 to 60 thousand notes in a single cluster.
03:06:59 [W1] So while migrating to kubernative they would like to avoid the need to suddenly manage an order of magnitude more clusters.
03:07:08 [W1] In addition to that they would also like to unified a setup and run application that they are currently running differently like some stressful apps also and kubernative.
03:07:23 [W1] At this point you're probably thinking that you are wasting your time.
03:07:27 [W1] I'm saying that we focused on two users.
03:07:30 [W1] Doesn't that sound simple by the way, and your workloads probably aren't even similar to what they are running.
03:07:39 [W1] However, just like landing on the moon push the frontiers of Technology. It's the same with our scalability work.
03:07:47 [W1] The improvements we did not only push the scalability limits, but they are primarily making all clusters more reliable and more performant.
03:07:57 [W1] So now I'm going to describe a couple improvements to show how they make your life better.
03:08:05 [W1] Sting your time.
03:08:04 [W1] I'm saying that we focused on two users.
03:08:07 [W1] doesn't that sound simple by the way and your workloads probably aren't even similar to what they are running. However, just like landing on the moon push the frontiers of Technology.
03:08:19 [W1] It's the same with our scalability work.
03:08:24 [W1] The improvements we did not only push the scalability limits, but they are primarily making all clusters more reliable and more performant.
03:08:35 [W1] So now I'm going to describe a couple improvements to show how they make your life better.
03:08:43 [W1] So let's start with that CD the main activity Improvement that is worth mentioning is concurrent reads.
03:08:52 [W1] Before this change every read operation on that CD was blocking all other operations both read and write for the entire time of its processing.
03:09:03 [W1] Thanks to this Improvement.
03:09:05 [W1] We are now blocking other operations only for a very short period of time just to grab a copy on write pointer to the current state.
03:09:17 [W1] What is crucial for large clusters?
03:09:19 [W1] It's not the only place where it helps.
03:09:31 [W1] 20 notes
03:09:34 [W1] thanks to this Improvement. When you are listing custom resources, you will no longer observe spikes in the API code latencies, even if your customers small.
03:09:47 [W1] And it's already available in SCD 3.4.
03:09:51 [W1] So let's now make a step up and talk about a pi server and API machinery.
03:09:57 [W1] As you know watches The crucial part of our API, but no request runs forever.
03:10:04 [W1] So what happens when it times out, especially if it's a very selective watch that selects only a small fraction of all objects of a given type.
03:10:15 [W1] To resume the watch we are using resource version of the last object that we received yet.
03:10:22 [W1] But many other changes might have happened in the meantime.
03:10:27 [W1] And now we need to process all of them again.
03:10:31 [W1] simply because we didn't have a way to Signal the client that they have already been processed.
03:10:38 [W1] So introduced a concept of what bookmark it's basically a new event type.
03:10:46 [W1] That you can receive the awards that basically tells you we processed everything up to a resource version X. So if you didn't receive anything, it simply means nothing much your selector.
03:10:58 [W1] There's nothing specific to 15,000 not clusters. It helps for four clusters with thousand or even hundred nodes.
03:11:07 [W1] And it just happens out of the box because Q black that is watching its own thoughts is a perfect example of very selective watch that benefits a lot from this Improvement.
03:11:20 [W1] And it's already ga-in kubernative to 117.
03:11:25 [W1] However, improvements were needed across the whole stack. So let's take a look into an example from the networking area.
03:11:33 [W1] For those not familiar with endpoints API the endpoints object contains information about all back ends of a given service.
03:11:44 [W1] So it's size is proportional to the number of PODS behind that service.
03:11:50 [W1] That has many consequences. But let's think about kubeflow oxy which is an agent running on every single node in the cluster that is responsible for programming in cluster networking.
03:12:03 [W1] In order to do that. It's watching for changes of every single endpoints object in the cluster.
03:12:11 [W1] So imagine that you have a service with 5,000 Parts, the corresponding endpoints object will have optimistically around one megabyte.
03:12:21 [W1] Is that a pi server has to send 5 gigabytes of data for a single change of that object.
03:12:19 [W1] And for the rolling upgrade of that service, it would be 25 terabytes of data to mitigate that we introduced the concept of endpoints eyes.
03:12:31 [W1] That allows us to share the information about the endpoints of a given service into multiple endpoints likes objects.
03:12:40 [W1] Thanks to that we can significantly reduce the load on the control plane and amount of data that IP a server has to serialize and send.
03:12:50 [W1] the pieces of that solution when the batter in kubernative 119
03:12:56 [W1] So let's take a look into one more example this time from the storage area and let's talk about secrets and config Maps.
03:13:04 [W1] As you probably know whenever any of those changes cubelets are reflecting those changes to all ports that are mounting them.
03:13:11 [W1] But in order to do that, they are opening a watch for every single secret and conflict map that they pods are using
03:13:22 [W1] That might translate even to hundreds or hundreds of thousands of additional watches in the system.
03:13:29 [W1] Optimizing them would bring a lot of complexity to the API machinery. And as I mentioned before we should always be solving real life problems.
03:13:38 [W1] It appeared that they don't mutate majority of Secrets and config maps at all.
03:13:36 [W1] So we introduced the concept of immutability to secret and config map API when explicitly marked as immutable by the user the contents cannot be changed, but cubelets also don't need
03:13:51 [W1] Which is vastly reducing the load on the control plane.
03:13:38 [W1] As in the previous examples there is nothing specific to large clusters and you can take advantage of it. Even if your cluster is small.
03:13:47 [W1] And it's already better in kubernative 119.
03:13:52 [W1] However, we were also looking outside. Of course kubernative. We worked closely with goal and community on optimizing its memory locator.
03:14:03 [W1] It may be surprising to many of you but lock contention at the level of go memory allocator is actually one of the bottlenecks that we really suffer from.
03:14:14 [W1] While some optimizations have already landed in newer versions of go even more are coming.
03:14:21 [W1] And this benefits not just kubernative this benefits everyone who is writing their applications in go.
03:14:29 [W1] I described a couple improvements and all of them as well as tens or maybe even hundreds of others were done in Upstream kubernative.
03:14:37 [W1] However, that doesn't immediately mean that every kubernative distribution will scale to 15,000 note clusters in order to work.
03:14:49 [W1] Your ecosystem has to work at that scale to that includes the underlying infrastructure both computer and networking you need
03:14:59 [W1] Includes Auto scaling logging and monitoring control plane upgrades and many other things.
03:15:04 [W1] Based on GK experience. I can say that it's a huge effort to make all of them work.
03:15:11 [W1] So kubernative improvements are necessary, but they don't solve all the problems for you.
03:15:19 [W1] There's one more question that we should answer here, which is how do we know when we can stop?
03:15:27 [W1] Fortunately, the answer for this question is fairly simple. As soon as we meet our slos service level objectives. You can think about them as system level metrics with fresh holes.
03:15:41 [W1] While the concept they cover in kubernative are still fairly basic like API call latencies or pot startup time.
03:15:49 [W1] They greatly correlate with user experience.
03:15:51 [W1] So to summarize scalability work matters for almost everyone because scalability is much more than just the cluster size.
03:16:04 [W1] The improvements we did to push the scalability limits are also or maybe even primarily making smaller clusters more reliable and more performant.
03:16:16 [W1] So if your Caster didn't work because of some scalability or performance issues in the past.
03:16:21 [W1] It's probably time to re-evaluate it.
03:16:18 [W1] Unfortunately, we don't have more time. So I will just mention that with upcoming releases. You can expect even more improvements and extending the portfolio of use cases supported by clusters with fifteen thousand nodes and with that.
03:16:33 [W1] Much for staying with me.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you. I am always intrigued about the scalability issues and how kind of everything interacts especially because sink scalability often has to work Upstream in go to get things done and it and it mingles with
03:16:47 [W1] Upstream in go to get things done and it and it mingles with the with the work that we do in Sig release.
03:16:51 [W1] So thank you for that exciting presentation.
03:16:54 [W1] Next up. We have David sutiya the last keynote for the morning last but certainly not least David is a senior devops engineer at go spot check and today he'll talk about how go spot-check and has
03:17:09 [W1] Together buildpacks Helm opentelemetry Prometheus Envoy linkerd e and grpc to create a smoother experience for developers.
03:17:17 [W1] So you're going to get like the whole Cloud native ecosystem or a good portion of it.
03:17:22 [W1] So check it out.
03:17:34 [W1] Hi, this is more power less pain building an internal platform with cncf tools. And I am nervous. My name is Dave sutiya though. And this is actually a sequel to a talk. I gave last year called balancing power and pain.
03:17:49 [W1] Which I gave a Tony rib, and it was about moving a single application from a platform as a service over into kubernative.
03:17:54 [W1] He's and when we finish that talk a bunch of people came said oh, so you build a platform to deploy apps and we said no we know we moved a nap.
03:18:05 [W1] the platforms coming could come talk to us next year. So this is next year.
03:18:11 [W1] Who am I I'm a senior devops engineer. I build clusters and deployed databases and right utility applications and I helped with architecture and this year.
03:18:20 [W1] I've been leading the effort to make a more developed developer friendly platform and inside of our company and that was prompted by one of our lead developers at a hackathon last winter coming up and saying I don't really know what my projects going to be other than figuring out why this is all so hard
03:18:35 [W1] Here. I've been leading the effort to make a more developed developer friendly platform and inside of our company and that was prompted by one of our lead developers at a hack-a-thon last winter coming up and saying I don't really know what my projects going to be other than figuring out why this
03:18:56 [W1] Make it not so hard.
03:18:57 [W1] So that was a catalyst for saying okay we can operate now, but it's not pleasant. We need to work on the developer experience and make this better.
03:19:07 [W1] I work for a company called go spot check.
03:19:09 [W1] It's a field execution app where you have sort of had data collection missions that you fill out on a mobile application that go back for near real-time sort of business intelligence analysis.
03:19:20 [W1] We as just for context on our sighs. We've got about 40 engineers and QA folks and we started as a rails monolith and we've sort of branched out in the last couple of years to go microservices and node-based Cloud functions we
03:19:35 [W1] Have sort of had data collection missions that you fill out on a mobile application that go back for near real-time sort of business intelligence analysis.
03:19:43 [W1] We as just for context on our sighs. We've got about 40 engineers and QA folks and we started as a rails monolith and we've sort of branched out in the last couple of years to go microservices and node-based Cloud functions we
03:20:43 [W1] And couch and Kafka.
03:20:45 [W1] We have a scholar based data pipeline.
03:20:49 [W1] So lots of different Technologies, which is really why Cloud native made sense for us as we migrated out because you know, we believed it would end up supporting supporting everything that we did.
03:21:01 [W1] So bit of context on why we're building this platform why cncf tools?
03:21:06 [W1] Well, we started on Heroku platform as a service. We outgrew it and kudos to Heroku for enabling us to get too big for Heroku.
03:21:14 [W1] We love you guys.
03:21:14 [W1] We decided to move to kubernative future-proof ourselves and for a lot more detail on that, please watch last year's talk but
03:21:23 [W1] One of the reasons for moving to kubernative was the ecosystem.
03:21:26 [W1] We figure we're going to go to a place where we have a lot of surrounding tools that are going to fill the gaps of utility that we need.
03:21:35 [W1] Maybe not right now, but but soon or in the future as well as now and we figured that it would be a long-term bet to pay off in terms of interoperability and efficiency and I do think it is paying off.
03:21:48 [W1] but there was struggle going from you know a monolith on an all-in-one platform with one or two teams that were very close to lots of teams working on lots of services in a distributed environment with open source tooling has been difficult
03:22:04 [W1] We decided to move to kubernative future-proof ourselves and for a lot more detail on that, please watch last year's talk but.
03:22:11 [W1] One of the reasons for moving to kubernative was the ecosystem.
03:22:14 [W1] We figure we're going to go to a place where we have a lot of surrounding tools that are going to fill the gaps of utility that we need.
03:22:23 [W1] Maybe not right now, but but soon or in the future as well as now and we figured that it would be a long-term bet to pay off in terms of interoperability and efficiency and I do think it is paying off.
03:22:36 [W1] but there was struggle going from you know a monolith on an all-in-one platform with one or two teams that were very close to lots of teams working on lots of services in a distributed environment with open source tooling has been difficult
03:24:13 [W1] I could come up with is from my experience as a parent.
03:24:16 [W1] It is really felt like going from the Slick stroller system to a bunch of toddlers poking each other in the heart. There have been there have been growing pains. One of the biggest difficulties we've had was just resourcing were as pretty, you know,
03:24:31 [W1] Midsize company my team the operations team is 2.2 people the point to being my manager who tries really hard to be an icy, but has to manage and you know, we used to be the devops team now with the Ops Team because
03:24:46 [W1] To do now that we can't really do any of the dev stuff.
03:24:45 [W1] We had a platform team at one point is, you know being dispersed out into the feature teams.
03:24:51 [W1] And so there's actually no resources dedicated to building this platform.
03:24:55 [W1] It's all just been done with whatever we can sort of grab from people and get them to contribute.
03:25:02 [W1] And it's just been time.
03:25:03 [W1] It's taken us about two and a half years to figure out.
03:25:05 [W1] what is our platform even need to have much less. How will build it and how will you know what pieces we can put together to compose it and then just to do the iterative process of we moved in at now.
03:25:15 [W1] We've moved Lots.
03:25:16 [W1] What's missing?
03:25:17 [W1] do we need right and going through that sort of Discovery process?
03:25:21 [W1] And so there's been this pendulum from Simplicity to complexity and and then sort of back in and into the middle that I want to talk about when this was going to be a breakout session.
03:25:31 [W1] I had all the logos here is going to do a deep dive into the the stack that we created but the deeper message for me going through this is that the cncf has lots of options for every kind of tool.
03:25:42 [W1] There's probably more than one implementation and you can really build and compose a stack out of whatever you want that fits for your needs. So so less than paying attention to the choice.
03:25:50 [W1] We made you know, it's more that it's possible to do this composition of a platform now.
03:25:56 [W1] The ecosystem is moving so fast.
03:25:57 [W1] The tooling is being developed so fast that in six months, everything is going to be a hundred percent easier.
03:26:04 [W1] I'll get into my case study for that in a little bit.
03:26:06 [W1] But but I've been saying this over and over again and I feel like it's starting to change.
03:26:13 [W1] So we started the the pendulum swing of Simplicity and simplicity is great, but it has limits we were able to deploy really quickly.
03:26:21 [W1] Everything was sort of there for us, but we hit this point where we had maxed out the number of servers. The the postgres configuration was slowing us down and we needed to have more control and customization over our environment.
03:26:32 [W1] So we swung the pendulum all the way over to the other side of power and complexity.
03:26:38 [W1] We had the ability to do whatever we wanted, but you know,
03:26:42 [W1] Like the dog catching the car. What what do we actually want is is the question we end up having to answer and do we have the resources to do it?
03:26:51 [W1] All right going to that side we get into kubernative and we realize oh we need metrics. Okay.
03:26:59 [W1] Well great.
03:26:59 [W1] Prometheus is there for that and we need to be able to see what's happening between our systems well-distributed tracing is therefore that Jaeger is there.
03:27:05 [W1] Okay cool.
03:27:06 [W1] We got that covered. But then you get into the smaller nitty-gritty things of how do we actually secure this properly and
03:27:12 [W1] and hey, I would expect that if I change my environment variables the pods going to restart but that's not actually true.
03:27:18 [W1] How do we get Pottery starts when the config changes and there's just these and and and and it starts to feel overwhelming.
03:27:26 [W1] And and and and it starts to feel overwhelming.
03:27:31 [W1] And that's why I'm so excited that things are kind of swinging into this happy medium where the toolings there.
03:27:36 [W1] It takes less work to get the tools working together.
03:27:41 [W1] And I think critically the vendor support is catching up. I'm going to give you some some concrete examples of this.
03:27:48 [W1] Observability a couple years ago.
03:27:51 [W1] We were running our own Prometheus and Jaeger rail support for both was meager or non-existent.
03:27:56 [W1] So our observability infrastructure and up getting split between an APM solution in the cloud knative stuff.
03:28:00 [W1] Some of the cloud native stuff was behind a VPN.
03:28:03 [W1] Some of it wasn't most of our Engineers had ever been on a VPN before so that was confusing for them and the cloud native stuff was falling over occasionally or commonly because we didn't have dedicated eyes on it because we were doing 50 things trying to set all the
03:28:18 [W1] And and stuff up, so it wasn't a great experience now. We actually have a vendor who fully supports Cloud native stuff.
03:28:26 [W1] It's not an external metric that's going to get charged at five times the cost and they give us a Helm chart.
03:28:31 [W1] We deploy the helm chart.
03:28:33 [W1] It sends out Prometheus and opentelemetry we send our stuff there. It forwards it onto a central place where everyone can look at it and we've got great tooling around it.
03:28:42 [W1] It's been a fantastic experience. That's only really been possible in the last year year and a half.
03:28:48 [W1] Mesh in increases and other this was my case study for if you can wait six months. You should three years ago.
03:28:54 [W1] Adding because it was just going to do it for me, but it was still hard to deploy and it was really complex may be more complex than we needed. And and so it's grown to now most of these things like we use linkerd e and you can deploy a production configuration of linkerd e with a single CLI command
03:29:09 [W1] This is just astounding and it has made it so much easier to be in the space and to play with things and get things out quickly.
03:29:11 [W1] The standards are getting better.
03:29:14 [W1] And so interoperability is actually becoming a real possible thing. Right? We use Ambassador for our Gateway and it plays very nicely with linkerd e both of those play nicely with the open tracing and opentelemetry so we can get observability into all this
03:29:29 [W1] It's nice.
03:29:29 [W1] It's really nice.
03:29:31 [W1] Building and deploying.
03:29:33 [W1] This is probably the number one value add we've had this year for our develop that developer experience. I mentioned in that, you know, when that when that lead was talking about how hard everything was he was really referring to I got to do 17 a multi has and I gotta write this Docker file
03:29:48 [W1] Tain all of it and we've cut all that down by abstracting and out with standardized Helm charts that we've written for our Organization for Argo micro service or a rails service and now they just have to put in some values and deploy it it's getting
03:30:00 [W1] Just have to put in some values and deploy it it's getting much closer to being a pleasant platform for dr. Files were using buildpacks. Now no more dockerfile Strider maintain, ironically enough.
03:30:12 [W1] We're using Heroku is buildpacks Tak so as we migrate out the last long tail of our applications.
03:30:18 [W1] We don't even have to change the proc files its seamless to just get things out from where they were and deploy them into kubernative zits.
03:30:26 [W1] It's been real Pleasant and made things much faster.
03:30:29 [W1] And the thing I'm most excited about honestly is actually the marketplace that is being created.
03:30:34 [W1] I've actually gotten surprise from people when I say that we want to buy and not build and I think that's partially because a lot of the most vocal people in this environment are the big players. And so thanks to coupon for
03:30:49 [W1] Created I've actually gotten surprise from people when I say that we want to buy and not build and I think that's partially because a lot of the most vocal people in this environment are the big players. And so thanks to coupon for
03:31:14 [W1] It's like so to give this perspective but you know, there's a space between the five-person Oregon the 5,000 person org that where there's room.
03:31:23 [W1] We want to buy and not build we're too big for the platform as a service but I was talking to the VP of a Fortune 500 tech company and his team that is extending tekton is twice as big as my team, right they can build we need to buy because we can't be
03:31:38 [W1] A little bit about the process of how we built this platform.
03:31:44 [W1] The first most important thing that we did was we treated it like a product your internal devtools our product any platform you build inside is a product.
03:31:53 [W1] You have to treat it like one.
03:31:54 [W1] I think one of the biggest pitfalls of platform teams that I've seen is that they build things for themselves and platform teams tend to be composed of the wonky assist engineer's or some of them and you know, it ends up being a complex doctor or
03:32:08 [W1] Castration stack that is powered by make files written in Vim to do local development. And I'm one of those lucky people.
03:32:16 [W1] I like make I like them don't at me, but that's not what everyone consuming that platform wants to use.
03:32:23 [W1] And so I got I didn't get a product manager but I got part-time from a product manager for 2 months who helped me do interviews of our internal users of our engineer's and QA and support and we put together personas and
03:32:38 [W1] Then we were and then he kind of downloaded into my brain a bunch of product management, you know info and strategies and stuff that we've used over the rest of the year to help inform what we build.
03:32:50 [W1] The next thing we've done that's been really critical is sort of harnessing Conway's law.
03:32:55 [W1] Personas, we created from various people.
03:32:51 [W1] So, you know, we had a Persona of the junior engineer and the like the senior engineer that just wants to ship stuff and isn't really into configuration and getting their hands dirty and devops stuff.
03:33:01 [W1] Those people need to be represented on a group that's working on.
03:33:04 [W1] How do we do observability or how do we you know work on how do we improve our local development?
03:33:08 [W1] So those groups have defined most of the work to be done as an operations team. We stood back and one more the library for
03:33:17 [W1] Tools here are all the things we know about that will let you local deploy into the cluster, right and and then you guys can go research this and y'all can figure out what you want to use and then they're able to become.
03:33:33 [W1] And make the choices which are really impactful for people actually wanted to use that since we don't have anyone any dedicated team to actually implementing that team has a board. It generates we generate stories and then people pick up those stories as they can to
03:33:48 [W1] We'll pick up those stories as they can to write the script that will you know install all the tooling you need to interact with our servicemeshcon something.
03:33:55 [W1] So the final thing I want to talk about is just the promise of this ecosystem that I think is starting to become fulfilled.
03:34:04 [W1] Last year my favorite keynote was from Bryan Liles the when is it kubernative is going to have its rails moment and and he call that my favorite quote which is that kubernative is is necessarily complex.
03:34:15 [W1] I don't think we're quite at a rails moment. But as a relatively early adopter, I think we're getting a lot closer.
03:34:23 [W1] It's not exactly the stroller.
03:34:24 [W1] I showed earlier but but it's getting to be a little bit more like this necessarily complex squirrel feeder, you know, it does a lot of stuff that is
03:34:33 [W1] Important it we still had to put it together.
03:34:36 [W1] We had to compose it but it looks and feels a lot more professional and it's a lot more pleasant to use.
03:34:45 [W1] So my Mantra is changing its now if you can wait six months, you might actually be good to go. Right now. The the tooling is at a point in terms of maturity and interoperability where you can dive in as a midsize organ compose something that that is going to serve your needs
03:35:00 [W1] Off that is important it we still had to put it together.
03:35:05 [W1] We had to compose it but it looks and feels a lot more professional and it's a lot more pleasant to use.
03:35:13 [W1] So my Mantra is changing its now if you can wait six months, you might actually be good to go. Right now. The the tooling is at a point in terms of maturity and interoperability where you can dive in as a midsize organ compose something that that is going to serve your needs
03:36:02 [W1] And one of things I'm most excited about is seeing the actual platform as a service here every year because I really truly believe that if you are a five person or gue should not be in kubenetes. You should be on a platform as a service but now you can still go places be like
03:36:18 [W1] But now you can still go places be like we use containers and kubernative is because behind the scenes you actually are.
03:36:26 [W1] Thank you for watching.
03:36:27 [W1] I'm at the develop Nick and the regular places since I'm not about to step down from a platform Podium into the crowd of my peers and chat, which I really miss.
03:36:36 [W1] If you didn't get something out of this that you wish you did or you want more detail.
03:36:40 [W1] I would be overjoyed if you reached out to me.
03:36:41 [W1] I love talking about this stuff.
03:36:42 [W1] Thank you so much for watching.
03:36:55 [W1] Yes connecting with a crowd even when virtual thank you.
03:36:59 [W1] Thank you David So as we go into the close of day to Keynotes. I just wanted to give a few closing remarks.
03:37:07 [W1] So, you know at the end of the day event today, we're going to have a variety of interactive experiences for you to enjoy such as Marvel Cinematic Universe trivia computer and video game history trivia and
03:37:22 [W1] Virtual Escape rooms. So police kept check out the schedule and RSVP to participate. So as David had mentioned being able to connect with all of the attendees is super important.
03:37:35 [W1] So take some time for the next 15 minutes.
03:37:38 [W1] All of the keynote speakers are going to be hanging out in the keynote slack Channel.
03:37:43 [W1] There's been lots of chatter already.
03:37:45 [W1] So if you want to connect to anyone and chat a little bit more deeply about about what they've talked about today head to
03:37:51 [W1] to number pound to - Keep Calm Keynotes. So the breakout sessions will begin at 2:55 u.s.
03:38:04 [W1] Eastern.
03:38:04 [W1] So take a break hang out in slack and then come join us for for more.
03:38:10 [W1] Awesome Cube con Cloud knative Khan. Thank you all.
03:38:19 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
03:39:29 [W1] Kubernative will be boring as soon as we can make it boring.
03:39:32 [W1] Maybe after I retire it's already boring.
03:39:36 [W1] So I think we're well on our journey towards kubernative being boring.
03:39:40 [W1] It'll only be boring when it's done.
03:39:42 [W1] It'll only be done when it's no longer relevant.
03:43:20 [W1] our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because
03:43:30 [W1] solving a complex problem
03:44:30 [W1] Kubernative will be boring as soon as we can make it boring.
03:44:34 [W1] Maybe after I retire it's already boring.
03:44:37 [W1] So I think we're well on our journey towards kubernative being boring.
03:44:41 [W1] It'll only be boring when it's done.
03:44:43 [W1] It'll only be done when it's no longer relevant.

Transcription for wordly [W0]

00:00:00 [W] Kubernative will be boring as soon as we can make it boring.
00:00:03 [W] Maybe after I retire it's already boring.
00:00:06 [W] So I think we're well on our journey towards kubernative being boring.
00:00:10 [W] It'll only be boring when it's done.
00:00:12 [W] It'll only be done when it's no longer relevant.
00:00:59 [W] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:13:59 [W1] Kubernative will be boring as soon as we can make it boring.
02:14:02 [W1] Maybe after I retire it's already boring.
02:14:06 [W1] So I think we're well on our journey towards kubernative being boring.
02:14:09 [W1] It'll only be boring when it's done.
02:14:11 [W1] It'll only be done when it's no longer relevant.
02:15:10 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:18:49 [W1] Kubernative will be boring as soon as we can make it boring.
02:18:52 [W1] Maybe after I retire it's already boring.
02:18:56 [W1] So I think we're well on our journey towards kubernative being boring.
02:18:59 [W1] It'll only be boring when it's done.
02:19:01 [W1] It'll only be done when it's no longer relevant.
02:20:00 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:20:49 [W1] Hello.
02:20:50 [W1] Hello everyone Cube con Cloud knative con virtual attendees.
02:20:55 [W1] I hope all of you had a great opportunity to check out the content from yesterday.
02:20:59 [W1] We've got I hope you met some new people hung out in slack the hallway chat.
02:21:06 [W1] I know that I got an opportunity to meet a lot of new folks as well as Place among us later in the night.
02:21:12 [W1] So I hope you're having fun. And we're going to get started with the day to Keynotes. So kicking us off my lovely co-chair Constance caramanolis.
02:21:24 [W1] This constants is a principal engineer at Splunk.
02:21:29 [W1] And yeah, she is going to be talking to you today about the opentelemetry collector and how it empowers end users.
02:21:37 [W1] So check it out.
02:21:48 [W1] Hi again, everyone.
02:21:50 [W1] I'm Constance.
02:21:51 [W1] caramanolis. And today I will be talking to you about the opentelemetry collector and how it empowers and users.
02:22:01 [W1] Now you're probably wondering what is opentelemetry.
02:22:03 [W1] It is a merger of open senses and opentracing from early 2019.
02:22:08 [W1] All right, these were to project successful projects.
02:22:12 [W1] They're working on very similar goals. And the decision was done to merge these into one project to unify our resources.
02:22:20 [W1] It is a collection of tools apis and sdks and yes that actually means that there are their support and several languages actual libraries that you can
02:22:31 [W1] Use the adopt in your and applications today for more details in terms of the instrumentation side today.
02:22:40 [W1] We focus on tooling I highly suggest that you watched last year's keynote with Liz Fang Jones and Sarah Novotny introducing opentelemetry and also some Kook Connie you talks.
02:22:55 [W1] Now why opentelemetry now the moment observability is here as we build more complex systems.
02:23:04 [W1] We need to better understand this our systems and so it is the time for a project that focuses on merging multiple Telemetry formats metrics traces actually logs.
02:23:19 [W1] and it is all about prioritizing end users and as you'll see in our landing page we talked about it being a vendor agnostic because unfortunately when it comes to Telemetry and the resulting back ends vendor lock-in has happened,
02:23:35 [W1] It depends on where you are in your car knative Journey if you are lucky and or an early adopter of cognitive vendor lock-in might not be a strong story for you. But for meant I can guarantee that for many of you watching today vendor lock-in is a pain.
02:23:50 [W1] and this actually hinders your ability to choose the proper back ends for you because vendors will, you know will provide better tooling and better instrumentation that works better with their back end, but this isn't
02:24:06 [W1] The way in the future that is not the point of cloud native and so opentelemetry is partial is strong addressing that by making sure to provide vendor agnostic instrumentation tooling.
02:24:17 [W1] Now you're probably thinking great another migration and I don't blame you. It is annoying to think about another migration, right?
02:24:26 [W1] You're probably thinking about like a job servicemeshcon Burnett. He's like what about containers you security so many things working there. And so I deserve this face now I am here to tell you that in terms of adopting and migraine opentelemetry.
02:24:42 [W1] Retrieve it isn't an all-or-nothing.
02:24:43 [W1] There is a way for you to start adopting opentelemetry collect opentelemetry and specifically The Collector without changing much of your existing infrastructure.
02:24:52 [W1] And so the path forward is oh The opentelemetry Collector now, you're probably wondering what is it?
02:25:01 [W1] It's an executable that receives State Telemetry data transforms it and sends the data long. It works on metrics and traces.
02:25:10 [W1] It supports several popular open source protocols.
02:25:13 [W1] I want to call this out because right opentelemetry does have its own protocol for spec for a PS4 metrics and traces but we do acknowledge that there have been other protocols for metrics and traces before
02:25:28 [W1] And so to successfully adopt opentelemetry doesn't mean that you have to use our protocol right away it you can still use your existing protocols as I'll show in a little bit.
02:25:39 [W1] And it's a pluggable architecture.
02:25:40 [W1] If you like. The code base is pretty nice and clean.
02:25:43 [W1] They're very clear interfaces. So you can Implement, you know, either new ways to get data in transform the data and Export it out.
02:25:51 [W1] There are two ways.
02:25:53 [W1] You can deploy it one as an agent sidecar or as a standalone binary instance, you know examples for my why you might want to write it as an agent is that if you want to do sampling decisions or off if you have a high throughput servicemeshcon,
02:26:08 [W1] And you want to offload the Telemetry as soon as possible to some other binary so you don't create a bottleneck in the application. That's one way or you can just have it a standalone instance because maybe you adding some extra information to your metrics and traces after it before for the long trip back
02:26:24 [W1] Either one it is all the same binary.
02:26:28 [W1] Now the internals of The Collector we said it before but if there's three parts would getting data in and the formal term is the receivers and so receivers do is that once you get something with an once
02:26:43 [W1] Within a certain protocol it gets translated into internal format and this internal format the processors consume this internal for format and then transforms the data optionally little spoilers in a bit and
02:26:58 [W1] Then it forwards data along to whatever for Mike what?
02:27:04 [W1] Now we're using you migrations is a few complex things about it to think about it one is that you can have multiple Telemetry workloads, right different teams might have used different protocols.
02:27:15 [W1] They might be sending these different back ends.
02:27:18 [W1] ends. It's a little complicated there depending you know, when the Telemetry is added the quality of the Telemetry can differ.
02:27:28 [W1] So you want to standardize this and then controlling where the data goes?
02:27:32 [W1] It's grades. You generate all this Telemetry but controlling where it goes in the end is so important.
02:27:39 [W1] Now, let's tackle the first thing managing multiple workloads.
02:27:42 [W1] This is naturally a simplified service Services application depiction.
02:27:48 [W1] And so all these services use only one Matrix format and some things to Prometheus back end and now they're actually two different at racing protocol Zipkin any acre in this case and you know
02:28:03 [W1] Services right.
02:28:04 [W1] They might have these values hard-coded what makes it really hard is say if you're the observability team is how do you manage all of these right?
02:28:11 [W1] It's if you don't provide a central way for teams to send their traces to some data collection.
02:28:18 [W1] Yes, the collector it's hard to audit where things are going.
02:28:22 [W1] So what I'm suggesting to you today is that you can use a collector to maintain all of your workflows. Now, there's a few ways you can do it one is that you can use a collector to create
02:28:33 [W1] in this case since we had three different protocols one for metrics into for traces three different pipelines one for Prometheus one for Zipkin.
02:28:40 [W1] Sorry go back and one for jaeger.
02:28:43 [W1] But also if you wanted to say, you know, what we're tired of having two different backends for traces.
02:28:50 [W1] We're going to merge them.
02:28:52 [W1] What's really cool about the collector right was hinted at in the pipeline's diagram.
02:28:57 [W1] is that since the port supports both receivers? It can see on one pipeline.
02:29:01 [W1] You can say I want to support Zipkin and Jaeger and then it will convert both of those into the internal data format. And then after once it goes through the rest of the pipeline exporting whatever format you want it to be this allows for you to have multiple protocols.
02:29:16 [W1] Most for Telemetry data and send it to one back end or you know, maybe something even cooler the end.
02:29:23 [W1] Now, how does this new work out of like, you know, you have these protocols and a new service comes along and this you service owners are super excited about opentelemetry and they want to use the opentelemetry format now
02:29:38 [W1] Kind of like what we did with is it going to Jaeger Trace pipeline?
02:29:41 [W1] We can do the exact same thing.
02:29:42 [W1] We can add the oh tell protocol to the receivers for the metric and traces and just send the data alone.
02:29:49 [W1] This means that your service owners don't need to change protocols.
02:29:53 [W1] when you can control the data, yes, it does require changing where the data is, you know exporting to but it also means to that you don't have to force everyone to upgrade to the latest instrumentation right to the latest library or
02:30:07 [W1] all this gives you power to choose.
02:30:10 [W1] Hey, maybe we only upgrade the latest Services. Right only new Services uses latest protocol or you know, this one service it's needs to be re-written.
02:30:20 [W1] Let's do that one there.
02:30:21 [W1] This gives you power to choose when you adopt a new protocol.
02:30:25 [W1] Now let's talk about standardizing data. We have you know, now we can we get all our data working through but now we're seeing that the quality of data is and so great right and a few causes for this different Source different quality of data can be different languages, you
02:30:41 [W1] Naming for that maybe earlier data that doesn't isn't as rich as newer data because we've asked as we play around with systems for longer. We learn to ask better questions meanings evolved a term that we might have used for five years ago doesn't make sense
02:30:57 [W1] More right maybe you've added more regions. And before there were no region is used to use default and accidents happen as best as we try sometimes, you know, like Pai leaks right something like that and we'd way to catch that.
02:31:13 [W1] Yes, as you all know the collector does all of these things.
02:31:16 [W1] so it's called the a trees processor and this example, I'm going to be operating on traces and so say I'm missing an attribute right?
02:31:25 [W1] I have an earlier version of the application that wasn't setting environment before we just had one environment now, we actually have testing staging production.
02:31:36 [W1] And so using this processor I can say let me add this attribute environment prodyna.
02:31:43 [W1] And now my data will look similar, you know from your stuff.
02:31:48 [W1] And then I realized looking a little bit more.
02:31:50 [W1] Oh, oops.
02:31:51 [W1] you know what I got into a little joke saying that the west coast is the best coast and so I set region to Best Coast and it didn't make it wasn't as much of an issue back then because we're only deployed on the west coast, but now we actually have east coast and yes
02:32:06 [W1] East Coast is really cool too. You know, it can be the best coast.
02:32:10 [W1] So the best is Coast together, so I want to change Best Coast to West Coast.
02:32:16 [W1] And you know, I'm looking at the state of little bit more and oops. Sorry Steven.
02:32:20 [W1] I'll beat your name in logging, you know in setting these attributes.
02:32:26 [W1] And so that's not okay.
02:32:27 [W1] So I'm actually going to delete anything with username for my Trace to look like my span to look like this.
02:32:34 [W1] Now this is giving you just a few examples of how you can tweak existing data. And this is all done in the collector.
02:32:41 [W1] I haven't given any llamo files here because I think we're all a little bit tired of Gamal the moment, but this is all done with a few lines of animal.
02:32:50 [W1] This doesn't require application to change anything. This allows you to say the central observability team to control the quality of the data without pushing changes on to the application.
02:33:02 [W1] And the last part 14 your data along.
02:33:05 [W1] All right, we're getting got data in multiple receivers.
02:33:08 [W1] We have enriched it standardize the data and now we want to send it on out work.
02:33:12 [W1] And so right now in this example here, we only have we only sent one back end.
02:33:17 [W1] But what happens maybe if you had a hack-a-thon and two teams came up with two different back ends and you want to compare these back and side by side.
02:33:25 [W1] Well, this is great is that with similar to the receivers? You have multiple receivers we support multiple.
02:33:31 [W1] Orders and so with a few lines from few lines again will code that saving your eyes from at the moment is that you can tell the collector.
02:33:40 [W1] Hey send everything, you know, send all these traces to back in 1 2 & 3 and this has absolutely no implication on your application.
02:33:48 [W1] Yes implication your application. Absolutely none.
02:33:52 [W1] Now the statement I want to make a microphone want to make an even stronger statement is that this gives you the power to choose whatever backend you want, right? This is partially targeting the vendor lock-in is that if you can control how you collect all your data, you can also
02:34:07 [W1] Control where your data goes and this actually holds I know it's funny coming from me because I do work for a vendor but this holds us vendor accountable to providing a great back end for you, right if you have the power to choose where your data goes
02:34:22 [W1] We have responsibilities do to make sure that you have a bad experience with your data.
02:34:26 [W1] And so that's why I think that this is actually the coolest thing about opentelemetry collector is that this puts the power in your hands?
02:34:33 [W1] So to reiterate some of the things we saw today from complex operations to ease your operations, we can go from instead of you know, very disjointed ways to managing multiple Telemetry workflows.
02:34:48 [W1] It's a collector you can have multiple pipelines and multiple protocol support so you can
02:34:51 [W1] Do that with one binary one deployment if you want to gamble you can standardize a quality to climb a tree using the attributes processor.
02:34:59 [W1] All right, this is once again, no code changes and you get to control where your Telemetry goes.
02:35:05 [W1] This is leveraging multiple exporters and multiple protocol support now a few other takeaways is that it's not a requirement for you to use the opentelemetry protocols to Leverage The Collector.
02:35:18 [W1] There are a lot of really cool features in the collector that I highly stressed you're looking looking into if you're trying to collect and manage all of your telemetry.
02:35:29 [W1] There's also provides a way for you to adopt adopt new Telemetry protocols and only change what is needed right by say having one pipeline that accepts multiple protocols.
02:35:41 [W1] You can then say I keep everything, you know anything before a certain date only using protocol, you know Legacy protocol anything going forward using opentelemetry protocol.
02:35:51 [W1] This gives you more power to choose.
02:35:53 [W1] It's easily extendable write anything that is missing add it right. If you have a custom protocol crate, you know, it's an easy to add a new receiver and exporter to support your custom protocols.
02:36:08 [W1] And processors do more of the modify attributes.
02:36:10 [W1] They can do sampling memory limiting which protects the collector from booming because you don't know losing your Telemetry batching or retrying queued like standing with a kubernative metadata.
02:36:22 [W1] There are quite a few processes that are able to enrich the data and also control how data flows through
02:36:30 [W1] So I'm hoping that your little bit less annoyed with the call to migration and that this may be provides a path forward for you to look into opentelemetry.
02:36:40 [W1] So the collector and also instrumentation without thinking without it having to be a huge migration and only tensely updating small things that need to be updated.
02:36:50 [W1] It's maybe a little happier with this concept.
02:36:52 [W1] We are working towards a g a and you can
02:36:58 [W1] Find The Collector repository here collector and contribute their and there's a meeting every second Wednesday.
02:37:04 [W1] Thank you. Everyone. Keep Cloud native connected everywhere.
02:37:07 [W1] Bye.
02:37:11 [W1] Welcome back.
02:37:12 [W1] Thank you so much constants for that awesome.
02:37:15 [W1] And opentelemetry collector.
02:37:17 [W1] There's some great chatter happening in the Keynotes channel on slack as well.
02:37:21 [W1] So check that out including a link to a list following Jones is talk.
02:37:25 [W1] So next up.
02:37:27 [W1] are we have non davidov?
02:37:30 [W1] Ananda is a lead data scientist and distinguished technologist at hpe and she will be discussing how to leverage open source projects such as Spire Opa and Envoy to provide A fine grain policy for overlay for your
02:37:45 [W1] Your machine learning pipelines take it away.
02:37:57 [W1] Hi, welcome everyone.
02:37:58 [W1] This talk is about scaling machine Learning Without compromising privacy and security.
02:38:05 [W1] Thank you cncf for giving me this opportunity.
02:38:09 [W1] and thank you audience for your attention.
02:38:12 [W1] My name is Amanda, which I Dave I lead product management at hpe s Barrel container platform and machine learning Ops. My background is in technology transformation at medium to large Enterprises specialized.
02:38:27 [W1] And big data and machine learning operations over the next few minutes.
02:38:32 [W1] We will take a look at the complexities of the machine learning pipeline. What are some of the possible touch points?
02:38:42 [W1] And areas of concern and what are some of the technologies that are being used to mitigate risks and lower the exposure in machine learning by plants?
02:38:53 [W1] This picture is from a famous paper by Google where it talks about the technical debt and the various stages of machine learning that most people associate machine learning with algorithms
02:39:08 [W1] The MLK chord or the MLS self because there's a lot that happens before during and after this may run on one or many communities clusters depending on the deployments at
02:39:23 [W1] Stammers there are various actors.
02:39:26 [W1] There is where various personas that interact with different stages of the pipeline and there are services within kubenetes and Ingress for users to access those services
02:39:41 [W1] And also there are external entity. If you look at this picture, this is our representative architecture of a large financial institution that has a pretty large footprint of machine learning activities that happen across
02:39:56 [W1] Sweetie systems and across different groups, right? You have data pre-processing done by your data engineers. And then you have the actual machine learning process done by your data scientists
02:40:11 [W1] And then a deployment of a model so not getting into the detail of this.
02:40:19 [W1] What's of concern for us in the next few minutes is that there are several different processes as you can see here this perimeter there is end attack surface multiple different kubernative
02:40:34 [W1] These clusters probably in different trusted domain and then you have a number of different external systems that have to be accessed and for those who are familiar with security and kubernative by default.
02:40:48 [W1] There is really no security that's enabled. So if you do a threat model for this
02:40:54 [W1] You can see that the graph of how a user accesses the system and what are all the touch points they go through for various activities.
02:41:05 [W1] What services are talking to other services?
02:41:08 [W1] So you have authenticating and authorizing to individual service by users. Then you have to establish trust between Services tenants and you know across sites if needed.
02:41:24 [W1] Because of external systems that are accessed as part of this pipeline you have an increased attack surface, especially with data access.
02:41:34 [W1] Now you have privacy concerns here. Our path to resolution is happening in multiple phases using best of the build Technologies such as ldap and I DC connector as you can see here.
02:41:49 [W1] And M TLS between systems.
02:41:54 [W1] Using you know jot and spiffe E identities.
02:42:00 [W1] For preventing attacks surface.
02:42:03 [W1] This is something we are paying special attention to with admission control various parts creating policies and network controls.
02:42:15 [W1] And for extended policies such as limiting your authorized Docker registry and things like that. There are Opa Rego policies.
02:42:26 [W1] For phase two there is experimentation going on to establish more transitive identity between users services and also external resources that are outside of the Cabela's Network and for
02:42:41 [W1] Relation between on-prem and Cloud trust remains. We are looking at spiffe a federation.
02:42:48 [W1] Thank you and looking forward to seeing you on another talks.
02:42:52 [W1] Very exciting.
02:42:56 [W1] Thank you nan de next up. We have Jonathan beri.
02:43:00 [W1] Jonathan is the founder of stealth and today he will be talking discussing the need of how and where networking Protocols are supported. It's uncover roadblocks.
02:43:10 [W1] So Jonathan take it away.
02:43:23 [W1] Hi everyone in this session.
02:43:26 [W1] We'll be discussing protocols specifically networking protocols.
02:43:31 [W1] I'll cover some of the challenges we faced upon protocols to production and where we as a community to make it easier.
02:43:38 [W1] Let's go.
02:43:42 [W1] My name is Jonathan beri, and I work on an iot startup. You can find me on Twitter at berry berry kicks if you want to chat about protocols or anything else.
02:43:55 [W1] We should probably begin our discussion with HTTP as it is the networking protocol.
02:44:01 [W1] Most people are familiar with its the lingua Franca of the web and a large percentage of web services are built using http.
02:44:13 [W1] Now once you have your HD TV service, you're not done.
02:44:17 [W1] You need a bunch of things to deploy to production and in the cloud things like load balancing routing security and observability.
02:44:29 [W1] Since HTTP is a popular protocol different projects and operators provide these things for HTTP out of the box.
02:44:39 [W1] Which is great since you would otherwise have to build these things yourself.
02:44:44 [W1] But HTTP isn't the only protocol you use all the time.
02:44:51 [W1] There's also DNS. Everyone's favorite service you use it when you want to configure a server to master domain name or enable services within a cluster to discover each other.
02:45:03 [W1] But what if you wanted to create a DNS service?
02:45:06 [W1] Turns out this is a thing that people want to do.
02:45:11 [W1] Well, you may not have realized that DNS is actually a suite of protocols and when you use services like Amazon Route 53 or Cordina s those are actually implementations of DNS protocols.
02:45:28 [W1] Dennis like HTTP has similar things it needs in order to be deployed to production.
02:45:36 [W1] However, since DNS is a less common protocol compared to http.
02:45:41 [W1] It isn't supported out-of-the-box by many of the cloud upon objects. And therefore you have to build a lot of these things 14s yourself.
02:45:51 [W1] And that makes it way harder to build your own solution that implements DNS.
02:45:59 [W1] And there are a lot of application and Industry protocols that are designed to fill a specific purpose.
02:46:08 [W1] I've listed a few examples here, but they range from synchronizing game state to streaming webrtc to the internet of things and actually iot is what first got me interested in exploring the topic of implementing protocols in the first place.
02:46:27 [W1] See, there are a ton of iot protocols designed specifically for the needs of connecting physical devices to each other and to the internet.
02:46:38 [W1] Some are optimized for power somewhere optimized for bandwidth others Implement standardized industrial consumer control planes.
02:46:47 [W1] Some are built on the Internet Protocol like UDP and TCP. While others simply can't use IP.
02:46:54 [W1] As a start-up we're interested in implementing many of these protocols in our Cloud solution.
02:47:02 [W1] the challenge my startup phases in bringing iot protocols to production is similar to gns in that these Protocols are less common than HTTP a lot less in fact and therefore aren't supporting easily by many of the projects
02:47:17 [W1] Services and clouds. We might want to leverage their for we're faced with the challenge of implementing load balancing routing Etc from scratch all by yourself. That means we can't leverage the wealth of
02:47:32 [W1] source for this community or take advantage of great solution providers
02:47:39 [W1] But I did say before the challenge of implementing protocols isn't specific to iot.
02:47:45 [W1] I want to see if other people in the community are also implementing new protocols and see what I can learn from them.
02:47:52 [W1] So I started talking to other folks like game server devs working on Project Argo Nats webrtc maintainers working on Pion Telco folks from the network servicemeshcon Direct in others.
02:48:06 [W1] Turns out they're all asking a similar fundamental question.
02:48:13 [W1] How can we Implement any protocol in a cloud native way?
02:48:17 [W1] This is clearly a community challenge something bigger than my startup or iot.
02:48:26 [W1] I'm a p.m.
02:48:27 [W1] So I did what we do best start a dock can go it to it now at bit ly / Beyond http.
02:48:35 [W1] It's a living doc.
02:48:37 [W1] that survey is a growing list of cloud native projects that might be used as part of a solution that implements a networking protocol it tries to hit identify which projects are specifically focusing on supporting non HTTP protocols and suggestions on where they can add additional
02:48:53 [W1] And we implementers might need feedback in contributions are greatly welcome.
02:48:59 [W1] I started the dock over a year ago and since that time contributors and maintainers have helped shave it at the same time projects across the cloud native ecosystem have added up streamed features that make it easier to implement new protocols.
02:49:14 [W1] His progress is both exciting and encouraging.
02:49:19 [W1] There's more projects than I have time to discuss in this session, but I wanted to highlight a few I think will be used by the most amount of those.
02:49:27 [W1] communities Envoy servicemeshcon face cloudevents and Network Services
02:49:36 [W1] Let's start with kubernetes.
02:49:39 [W1] Kubernative has a concept of Ingress which is an object that exposes networking traffic from outside the cluster to services within the current stable Ingress implementation supports HTTP go figure within the
02:49:55 [W1] Say it group has been hard at work developing the new Gateway guy to replace Ingress.
02:50:01 [W1] Like Ingress a Gateway routes traffic to a service that supports more protocols like UDP and TCP among other things with a goal to enable protocol implementers to create custom gateways in the future.
02:50:14 [W1] They already have demos you can try and it may be in an alpha look released by the time you're watching this report.
02:50:21 [W1] The envoy as a popular networking proxy by its nature needs to support a protocol in order to act as a proxy for said protocol.
02:50:31 [W1] Envoy has always had native support for HTTP, but due to its popularity began implementing support for protocols like redis and postgres albeit in a one-off fashion.
02:50:44 [W1] Over the last year or two, the team has added UDP and TCP listeners, which make it possible for protocol implementers to use Envoy to proxy any protocol based on those layer for Primitives which covers a lot of potential protocols.
02:51:00 [W1] Also with the introduction of webassembly support Envoy has now made it even easier for more people to implement custom protocols in the language of their choice.
02:51:11 [W1] One more thing since Envoy is used as a basis for other projects like servicemeshcon and observability tools and now becomes easier for projects like sto and Prometheus to support custom protocols in the future.
02:51:27 [W1] Next I'd like to highlight servicemeshcon face for SMI, which is developing a standard for servicemeshcon kubenetes.
02:51:37 [W1] One aspect of a servicemeshcon it manages traffic between services within Foster.
02:51:44 [W1] SMI defines a concept of traffic specs a list of subjects that Define how traffic flows between the mesh.
02:51:52 [W1] There are already some specs for HTTP TCP and UDP.
02:51:57 [W1] The intent is for protocol implementers to Define their own traffic specs and eventually contribute them Upstream to SLI. Where make sense.
02:52:07 [W1] You might notice some parallels between the Gateway API for Nettie's and the traffic spec here in SMI.
02:52:14 [W1] Gateway's are about traffic coming into the cluster while traffic specs are internal traffic Within.
02:52:21 [W1] So there's a potential the two projects to work well together.
02:52:25 [W1] I'm personally excited to see how these two efforts collaborate in the future.
02:52:31 [W1] Cloudevents is a specification for describing event data in common formats to provide interoperability across Services platforms and systems.
02:52:40 [W1] It defines how to serialize events in different formats like Json and protocols which of course include http.
02:52:49 [W1] There's already some specs for protocols like Kafka mqtt and Nats and cleared documentation on how to define custom / proprietary protocols.
02:53:01 [W1] Cloudevents is already being used today in projects. Like he knative idly will become more commonplace among event oriented microservices.
02:53:08 [W1] So having a way to define custom protocol bindings will make it easier for services that leverage non-http protocols to interoperate.
02:53:20 [W1] The last project I'm going to touch on is Network servicemeshcon fused with the other kind of service for the servicemeshcon your face.
02:53:29 [W1] Nsmcon to extend the existing kubernative networking model, but address networking use cases.
02:53:35 [W1] They need to go deeper in the stack like el34 L 2 layers or even ethernet frames themselves inspired by sto network is servicemeshcon the concepts of a servicemeshcon L2 L3 payloads.
02:53:50 [W1] Hence the name.
02:53:51 [W1] Telco 's and the cncf Telecom User Group are big fans of nsmcon.
02:53:56 [W1] One reason is how it enables them to implement cellular specific protocols in a cloud knative way 5G is repelling the Telecom industry fully embracing what they call Cloud Network functions many of the
02:54:11 [W1] Oceans have been classically implemented in very expensive Hardware found in cell towers would take a long time to physically operate with Cloud native functions Telecom operators.
02:54:21 [W1] to build these capabilities as services that can be more easily upgraded over time.
02:54:27 [W1] Lots of the functions are specific Niche protocols that are part of the operation of 5G networks and nsmcon has the means to configure them on top of case moist.
02:54:39 [W1] I'm only scratching the surface of what nsmcon fun fact because servicemeshcon Sly Castillo operate up the stack at L7 L4 and nsmcon LP and below.
02:54:50 [W1] You can use a servicemeshcon NSM. There's a bunch of advantages when combining these two concepts.
02:54:57 [W1] I don't have time to dig into that now, but the innocent grew have some examples.
02:55:03 [W1] with that I want to conclude with what you could do to participate if you're trying such a great something with protocols other than HTTP, share your story documenting use cases both real world and aspirational has been
02:55:18 [W1] The operation 5G networks and nsmcon has the means to configure them on top of case moist.
02:55:26 [W1] I'm only scratching the surface of what nsmcon fun fact because servicemeshcon is like sto operate up the stack at L7 L4 and nsmcon LP and below.
02:55:37 [W1] You can use a servicemeshcon NSM. There's a bunch of advantages when combining these two concepts.
02:55:44 [W1] I don't have time to dig into that now, but the innocent proof have some examples.
02:55:49 [W1] with that I want to conclude with what you can do to participate if you're trying such a great something with protocols other than HTTP, share your story documenting use cases both real world and aspirational has been the
02:57:49 [W1] If tool to help improve protocol support create issues hop on slack. The cncf poor communities is a great place or just hit me up on Twitter.
02:57:58 [W1] And if you're a project maintainer try to identify where your projects currently assume traffic is HD and look for places for accessibility. Oh and asking you is what they think.
02:58:11 [W1] And that's all I got.
02:58:12 [W1] Thank you. Everyone.
02:58:14 [W1] Here's a link to that dock one more time, and you can reach me on Twitter at Heretics. Cheers.
02:58:22 [W1] Thank you Jonathan for that wonderful presentation again harkening back to the end-users reach out to your end users and think about their use cases so you can build for that next up. We have vijoy append a feature is
02:58:37 [W1] Presentation again harkening back to the end-users reach out to your end users and think about their use cases so you can build for that next up. We have vijoy Pandey futurewei is a VP of engineering for emerging Technologies
02:59:00 [W1] Paying for emerging Technologies and incubation at Cisco and he will be discussing the challenges of making your API secure and and the waist overcome them.
02:59:19 [W1] Hello, folks.
02:59:21 [W1] I'm vijoy and welcome to Q Khan and I'm here to ask Marvin about where my security apis are.
02:59:28 [W1] It's also an ask for the community to step up our application security game. But before we begin we have to ask so who is Marvin?
02:59:38 [W1] If you haven't read The Hitchhiker's Guide to the Galaxy go and get a copy now.
02:59:43 [W1] Marvin is a robot with a brain the size of a planet by his own account.
02:59:49 [W1] He's at least fifty thousand times smarter than a human but he's being made to park cars pick papers check airlocks.
02:59:58 [W1] not really job satisfaction worthy since he is built with genuine people personality or GBP Tech. He also has human emotions. So he gets depressed with all of this mindless work.
03:00:11 [W1] but most importantly he is a Paranoid Android and all of these properties make him very suited for the State of Affairs around API security as you shall find out but first a little bit of historical context around
03:00:27 [W1] Knative as we all know a pocket textures are changing from monolithic that metal ovhcloud.
03:00:43 [W1] Over the last 3 Cube cons. We have discussed how the network needed to evolve to handle this transformation to Cloud native through the network servicemeshcon. Jecht. And today we are looking at this Cloud knative developer as she is building
03:00:58 [W1] Moving fast utilizing the Trove of apis from public Cloud providers such as providers and other services she is faced with the dependency graph complexity of a model globally distributed.
03:00:57 [W1] Microsoft is based app Security in such an architecture is a modern brain size problem.
03:01:03 [W1] By the way. This is the Microsoft this dependency graph of the Monza banking app and as she uses these globally distributed apis, which are either home.
03:01:13 [W1] All groan or from external providers the quality and security of these remote Services is often unknown.
03:01:20 [W1] The pivot on API reputation is not inherently part of the ci/cd process which might put your customers data eventually at risk.
03:01:31 [W1] This rest causes all kinds of unwanted Behavior developers want convenience and velocity security a sorry sack of sand sea Source pound the table for reviews and are still unsure whether the mm API is being used are safe or compliant.
03:01:47 [W1] From One Cloud ideas are always wanted that insecure code has been pushed to prod and all of these reviews and Gates and meetings just add complexity and opacity to the simple job of developing and deploying fast.
03:01:57 [W1] The simple job of developing and deploying fast.
03:01:41 [W1] Seeing all of this Marvin the Paranoid Android with the weight of the world on his shoulders steps in with this depressing Court.
03:01:49 [W1] Wouldn't it be nice if there really was a modern with a PlanetSide brain who could be observing the entire application lifecycle fund repo to runtime and identify the risks of using
03:02:04 [W1] All the teams concerned report that risk to security a sorry sack of sand sea salts and maybe even remediate that risk to customer Data before it happens.
03:02:13 [W1] Now that is indeed a modern sighs problem, but do not underestimate Marvel he could simultaneously plan the entire planets military strategy and solve all of these major problems of the universe
03:02:28 [W1] I didn't Pablo also compose a number of lot of eyes if you could do all of that. He could surely solve for the application and API security problem.
03:02:38 [W1] So what would the app lifecycle look like? If we had this Marvin the developer would take Marvin curated apis and move fast fast fast.
03:02:47 [W1] Marvin would notify security teams of possible issues and mediations continuously and cloud form teams could rest easy knowing they can report compliance in a real-time matter.
03:03:02 [W1] How would we train this morning?
03:03:04 [W1] What are these boundary conditions?
03:03:05 [W1] What are its parameters first and foremost we have to realize that it's all about protecting a customer's data.
03:03:13 [W1] The open internet is really the runtime for all modern Cloud native apps security attacks risk, both clients users and other services other applications and the new parameter of apps and security is really
03:03:28 [W1] Is down to an API or data object.
03:03:28 [W1] What would be some of the apps with questions we could ask this Marvin?
03:03:33 [W1] Is the correct in-house service image or artifact being used in creating a new app are we integrating with compliant third-party apis are we tracking RPC calls to make sure we do not import never is data and
03:03:48 [W1] these to ensure safety for elapsed
03:01:50 [W1] some of the broader questions we have to ask ourselves as the community could be if a distributed app does get broken into do we have all the tooling to drive meantime to detect and debug down can be formally model and verify
03:02:05 [W1] Logic in our business apps even across layers.
03:02:09 [W1] Can we the community help spread the word on API misuse?
03:02:14 [W1] If we aren't able to step up to the plate and answer these questions, I'm afraid Marvin has this to say to you.
03:02:22 [W1] Thank you for listening.
03:02:22 [W1] Please reach out to me.
03:02:24 [W1] I would love to continue the conversation visit us at the Cisco Booth where we have demos on app Security app and in for observability cloud native networking a lot more and we are highly and we would love to have you on board. Thank you.
03:02:41 [W1] Awesome presentation from vijoy.
03:02:45 [W1] I hope we can all please Marvin through moving forward when we're designing our apis.
03:02:49 [W1] So next up. We have a void doc to change to 10 ski why Doc is a staff engineer at Google and also one of the co-chairs for kubernative Sig scalability today.
03:03:03 [W1] He will be discussing improvements made to kubernative A's and SED that unlock
03:03:08 [W1] massive clusters running with 15,000 nodes. So enjoy
03:03:23 [W1] Hello Cube Khan. My name is Victor twins key. I work in Google and I'm part of this amazing kubernative Community for almost six years at this point.
03:03:33 [W1] There's I'm the TL of scalability and in connection to that during this presentation.
03:03:34 [W1] I'm going to talk about example features that allow us to run kubernative clusters with 15,000 notes.
03:03:43 [W1] But let's start with making this explicit.
03:03:47 [W1] Kubernative clusters with 15,000 nodes are already a thing.
03:03:53 [W1] As you might have heard earlier this year buyer crop science and Google together published an interesting blog post.
03:04:02 [W1] You can read their how thanks to running on g k clusters with fifteen thousand nodes with almost a quarter million, of course and over 1.5 petabyte-scale Fram
03:04:17 [W1] Process roughly 15 billion genotypes per hour as part of the pipeline responsible for deciding which seeds which are the final product should be Advanced for further experiments in their R&D Department.
03:04:20 [W1] Couple months later during joint presentation Twitter and Google together described how we approached validating that the applications that they are currently running on their messes Aurora clusters
03:04:35 [W1] Plated and run on g k clusters with 15,000 nodes.
03:04:37 [W1] But the goal of this presentation is to show that this work matters for all of you.
03:04:43 [W1] Even if your clusters are order or maybe even orders of magnitude smaller.
03:04:51 [W1] However, we need to start with understanding what scalability really means for kubernative.
03:04:57 [W1] Even though we tend to use node count or cluster size as a proxy for the overall scale.
03:05:03 [W1] It's actually much more complex than that.
03:05:07 [W1] scalability is a multi-dimensional problem with thousands of Dimensions such as number of services number of volumes pot turn and so on even though they in many use cases
03:05:22 [W1] Together with cluster size you need to acknowledge that not count is not the only thing that matters.
03:05:31 [W1] So, how did we approach scaling kubernative to the next level which 15,000 notes certainly is?
03:05:40 [W1] The core principle for any scalability or performance work is don't optimize blindly and always sort of real-life problems.
03:05:51 [W1] Almost every optimization is making a system a little bit more complex.
03:05:55 [W1] So it's super important to keep that in mind.
03:06:00 [W1] According to this rule. We first found users for whom running such a massive clusters would have actual benefits.
03:06:10 [W1] We started with understanding their use cases but also their motivations and as you may suspect those are the ones that I already mentioned.
03:06:20 [W1] The first one was buyer cropscience who is refusing kubernative to run embarrassingly parallel Isabel batch computations.
03:06:28 [W1] For them larger clusters immediately translate to having results faster.
03:06:34 [W1] In addition to that given that the users are generally data scientists who don't want to understand the underlying infrastructure.
03:06:43 [W1] They want to make it as simple as possible. Ideally a single cluster.
03:06:47 [W1] Migrate they microservices apps to kubernative.
03:06:48 [W1] As I already mentioned they are currently running them on my so Sarah clusters, which can handle even 40 to 60 thousand notes in a single cluster.
03:06:59 [W1] So while migrating to kubernative they would like to avoid the need to suddenly manage an order of magnitude more clusters.
03:07:08 [W1] In addition to that they would also like to unified a setup and run application that they are currently running differently like some stressful apps also and kubernative.
03:07:23 [W1] At this point you're probably thinking that you are wasting your time.
03:07:27 [W1] I'm saying that we focused on two users.
03:07:30 [W1] Doesn't that sound simple by the way, and your workloads probably aren't even similar to what they are running.
03:07:39 [W1] However, just like landing on the moon push the frontiers of Technology. It's the same with our scalability work.
03:07:47 [W1] The improvements we did not only push the scalability limits, but they are primarily making all clusters more reliable and more performant.
03:07:57 [W1] So now I'm going to describe a couple improvements to show how they make your life better.
03:08:05 [W1] Sting your time.
03:08:04 [W1] I'm saying that we focused on two users.
03:08:07 [W1] doesn't that sound simple by the way and your workloads probably aren't even similar to what they are running. However, just like landing on the moon push the frontiers of Technology.
03:08:19 [W1] It's the same with our scalability work.
03:08:24 [W1] The improvements we did not only push the scalability limits, but they are primarily making all clusters more reliable and more performant.
03:08:35 [W1] So now I'm going to describe a couple improvements to show how they make your life better.
03:08:43 [W1] So let's start with that CD the main activity Improvement that is worth mentioning is concurrent reads.
03:08:52 [W1] Before this change every read operation on that CD was blocking all other operations both read and write for the entire time of its processing.
03:09:03 [W1] Thanks to this Improvement.
03:09:05 [W1] We are now blocking other operations only for a very short period of time just to grab a copy on write pointer to the current state.
03:09:17 [W1] What is crucial for large clusters?
03:09:19 [W1] It's not the only place where it helps.
03:09:31 [W1] 20 notes
03:09:34 [W1] thanks to this Improvement. When you are listing custom resources, you will no longer observe spikes in the API code latencies, even if your customers small.
03:09:47 [W1] And it's already available in SCD 3.4.
03:09:51 [W1] So let's now make a step up and talk about a pi server and API machinery.
03:09:57 [W1] As you know watches The crucial part of our API, but no request runs forever.
03:10:04 [W1] So what happens when it times out, especially if it's a very selective watch that selects only a small fraction of all objects of a given type.
03:10:15 [W1] To resume the watch we are using resource version of the last object that we received yet.
03:10:22 [W1] But many other changes might have happened in the meantime.
03:10:27 [W1] And now we need to process all of them again.
03:10:31 [W1] simply because we didn't have a way to Signal the client that they have already been processed.
03:10:38 [W1] So introduced a concept of what bookmark it's basically a new event type.
03:10:46 [W1] That you can receive the awards that basically tells you we processed everything up to a resource version X. So if you didn't receive anything, it simply means nothing much your selector.
03:10:58 [W1] There's nothing specific to 15,000 not clusters. It helps for four clusters with thousand or even hundred nodes.
03:11:07 [W1] And it just happens out of the box because Q black that is watching its own thoughts is a perfect example of very selective watch that benefits a lot from this Improvement.
03:11:20 [W1] And it's already ga-in kubernative to 117.
03:11:25 [W1] However, improvements were needed across the whole stack. So let's take a look into an example from the networking area.
03:11:33 [W1] For those not familiar with endpoints API the endpoints object contains information about all back ends of a given service.
03:11:44 [W1] So it's size is proportional to the number of PODS behind that service.
03:11:50 [W1] That has many consequences. But let's think about kubeflow oxy which is an agent running on every single node in the cluster that is responsible for programming in cluster networking.
03:12:03 [W1] In order to do that. It's watching for changes of every single endpoints object in the cluster.
03:12:11 [W1] So imagine that you have a service with 5,000 Parts, the corresponding endpoints object will have optimistically around one megabyte.
03:12:21 [W1] Is that a pi server has to send 5 gigabytes of data for a single change of that object.
03:12:19 [W1] And for the rolling upgrade of that service, it would be 25 terabytes of data to mitigate that we introduced the concept of endpoints eyes.
03:12:31 [W1] That allows us to share the information about the endpoints of a given service into multiple endpoints likes objects.
03:12:40 [W1] Thanks to that we can significantly reduce the load on the control plane and amount of data that IP a server has to serialize and send.
03:12:50 [W1] the pieces of that solution when the batter in kubernative 119
03:12:56 [W1] So let's take a look into one more example this time from the storage area and let's talk about secrets and config Maps.
03:13:04 [W1] As you probably know whenever any of those changes cubelets are reflecting those changes to all ports that are mounting them.
03:13:11 [W1] But in order to do that, they are opening a watch for every single secret and conflict map that they pods are using
03:13:22 [W1] That might translate even to hundreds or hundreds of thousands of additional watches in the system.
03:13:29 [W1] Optimizing them would bring a lot of complexity to the API machinery. And as I mentioned before we should always be solving real life problems.
03:13:38 [W1] It appeared that they don't mutate majority of Secrets and config maps at all.
03:13:36 [W1] So we introduced the concept of immutability to secret and config map API when explicitly marked as immutable by the user the contents cannot be changed, but cubelets also don't need
03:13:51 [W1] Which is vastly reducing the load on the control plane.
03:13:38 [W1] As in the previous examples there is nothing specific to large clusters and you can take advantage of it. Even if your cluster is small.
03:13:47 [W1] And it's already better in kubernative 119.
03:13:52 [W1] However, we were also looking outside. Of course kubernative. We worked closely with goal and community on optimizing its memory locator.
03:14:03 [W1] It may be surprising to many of you but lock contention at the level of go memory allocator is actually one of the bottlenecks that we really suffer from.
03:14:14 [W1] While some optimizations have already landed in newer versions of go even more are coming.
03:14:21 [W1] And this benefits not just kubernative this benefits everyone who is writing their applications in go.
03:14:29 [W1] I described a couple improvements and all of them as well as tens or maybe even hundreds of others were done in Upstream kubernative.
03:14:37 [W1] However, that doesn't immediately mean that every kubernative distribution will scale to 15,000 note clusters in order to work.
03:14:49 [W1] Your ecosystem has to work at that scale to that includes the underlying infrastructure both computer and networking you need
03:14:59 [W1] Includes Auto scaling logging and monitoring control plane upgrades and many other things.
03:15:04 [W1] Based on GK experience. I can say that it's a huge effort to make all of them work.
03:15:11 [W1] So kubernative improvements are necessary, but they don't solve all the problems for you.
03:15:19 [W1] There's one more question that we should answer here, which is how do we know when we can stop?
03:15:27 [W1] Fortunately, the answer for this question is fairly simple. As soon as we meet our slos service level objectives. You can think about them as system level metrics with fresh holes.
03:15:41 [W1] While the concept they cover in kubernative are still fairly basic like API call latencies or pot startup time.
03:15:49 [W1] They greatly correlate with user experience.
03:15:51 [W1] So to summarize scalability work matters for almost everyone because scalability is much more than just the cluster size.
03:16:04 [W1] The improvements we did to push the scalability limits are also or maybe even primarily making smaller clusters more reliable and more performant.
03:16:16 [W1] So if your Caster didn't work because of some scalability or performance issues in the past.
03:16:21 [W1] It's probably time to re-evaluate it.
03:16:18 [W1] Unfortunately, we don't have more time. So I will just mention that with upcoming releases. You can expect even more improvements and extending the portfolio of use cases supported by clusters with fifteen thousand nodes and with that.
03:16:33 [W1] Much for staying with me.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you. I am always intrigued about the scalability issues and how kind of everything interacts especially because sink scalability often has to work Upstream in go to get things done and it and it mingles with
03:16:47 [W1] Upstream in go to get things done and it and it mingles with the with the work that we do in Sig release.
03:16:51 [W1] So thank you for that exciting presentation.
03:16:54 [W1] Next up. We have David sutiya the last keynote for the morning last but certainly not least David is a senior devops engineer at go spot check and today he'll talk about how go spot-check and has
03:17:09 [W1] Together buildpacks Helm opentelemetry Prometheus Envoy linkerd e and grpc to create a smoother experience for developers.
03:17:17 [W1] So you're going to get like the whole Cloud native ecosystem or a good portion of it.
03:17:22 [W1] So check it out.
03:17:34 [W1] Hi, this is more power less pain building an internal platform with cncf tools. And I am nervous. My name is Dave sutiya though. And this is actually a sequel to a talk. I gave last year called balancing power and pain.
03:17:49 [W1] Which I gave a Tony rib, and it was about moving a single application from a platform as a service over into kubernative.
03:17:54 [W1] He's and when we finish that talk a bunch of people came said oh, so you build a platform to deploy apps and we said no we know we moved a nap.
03:18:05 [W1] the platforms coming could come talk to us next year. So this is next year.
03:18:11 [W1] Who am I I'm a senior devops engineer. I build clusters and deployed databases and right utility applications and I helped with architecture and this year.
03:18:20 [W1] I've been leading the effort to make a more developed developer friendly platform and inside of our company and that was prompted by one of our lead developers at a hackathon last winter coming up and saying I don't really know what my projects going to be other than figuring out why this is all so hard
03:18:35 [W1] Here. I've been leading the effort to make a more developed developer friendly platform and inside of our company and that was prompted by one of our lead developers at a hack-a-thon last winter coming up and saying I don't really know what my projects going to be other than figuring out why this
03:18:56 [W1] Make it not so hard.
03:18:57 [W1] So that was a catalyst for saying okay we can operate now, but it's not pleasant. We need to work on the developer experience and make this better.
03:19:07 [W1] I work for a company called go spot check.
03:19:09 [W1] It's a field execution app where you have sort of had data collection missions that you fill out on a mobile application that go back for near real-time sort of business intelligence analysis.
03:19:20 [W1] We as just for context on our sighs. We've got about 40 engineers and QA folks and we started as a rails monolith and we've sort of branched out in the last couple of years to go microservices and node-based Cloud functions we
03:19:35 [W1] Have sort of had data collection missions that you fill out on a mobile application that go back for near real-time sort of business intelligence analysis.
03:19:43 [W1] We as just for context on our sighs. We've got about 40 engineers and QA folks and we started as a rails monolith and we've sort of branched out in the last couple of years to go microservices and node-based Cloud functions we
03:20:43 [W1] And couch and Kafka.
03:20:45 [W1] We have a scholar based data pipeline.
03:20:49 [W1] So lots of different Technologies, which is really why Cloud native made sense for us as we migrated out because you know, we believed it would end up supporting supporting everything that we did.
03:21:01 [W1] So bit of context on why we're building this platform why cncf tools?
03:21:06 [W1] Well, we started on Heroku platform as a service. We outgrew it and kudos to Heroku for enabling us to get too big for Heroku.
03:21:14 [W1] We love you guys.
03:21:14 [W1] We decided to move to kubernative future-proof ourselves and for a lot more detail on that, please watch last year's talk but
03:21:23 [W1] One of the reasons for moving to kubernative was the ecosystem.
03:21:26 [W1] We figure we're going to go to a place where we have a lot of surrounding tools that are going to fill the gaps of utility that we need.
03:21:35 [W1] Maybe not right now, but but soon or in the future as well as now and we figured that it would be a long-term bet to pay off in terms of interoperability and efficiency and I do think it is paying off.
03:21:48 [W1] but there was struggle going from you know a monolith on an all-in-one platform with one or two teams that were very close to lots of teams working on lots of services in a distributed environment with open source tooling has been difficult
03:22:04 [W1] We decided to move to kubernative future-proof ourselves and for a lot more detail on that, please watch last year's talk but.
03:22:11 [W1] One of the reasons for moving to kubernative was the ecosystem.
03:22:14 [W1] We figure we're going to go to a place where we have a lot of surrounding tools that are going to fill the gaps of utility that we need.
03:22:23 [W1] Maybe not right now, but but soon or in the future as well as now and we figured that it would be a long-term bet to pay off in terms of interoperability and efficiency and I do think it is paying off.
03:22:36 [W1] but there was struggle going from you know a monolith on an all-in-one platform with one or two teams that were very close to lots of teams working on lots of services in a distributed environment with open source tooling has been difficult
03:24:13 [W1] I could come up with is from my experience as a parent.
03:24:16 [W1] It is really felt like going from the Slick stroller system to a bunch of toddlers poking each other in the heart. There have been there have been growing pains. One of the biggest difficulties we've had was just resourcing were as pretty, you know,
03:24:31 [W1] Midsize company my team the operations team is 2.2 people the point to being my manager who tries really hard to be an icy, but has to manage and you know, we used to be the devops team now with the Ops Team because
03:24:46 [W1] To do now that we can't really do any of the dev stuff.
03:24:45 [W1] We had a platform team at one point is, you know being dispersed out into the feature teams.
03:24:51 [W1] And so there's actually no resources dedicated to building this platform.
03:24:55 [W1] It's all just been done with whatever we can sort of grab from people and get them to contribute.
03:25:02 [W1] And it's just been time.
03:25:03 [W1] It's taken us about two and a half years to figure out.
03:25:05 [W1] what is our platform even need to have much less. How will build it and how will you know what pieces we can put together to compose it and then just to do the iterative process of we moved in at now.
03:25:15 [W1] We've moved Lots.
03:25:16 [W1] What's missing?
03:25:17 [W1] do we need right and going through that sort of Discovery process?
03:25:21 [W1] And so there's been this pendulum from Simplicity to complexity and and then sort of back in and into the middle that I want to talk about when this was going to be a breakout session.
03:25:31 [W1] I had all the logos here is going to do a deep dive into the the stack that we created but the deeper message for me going through this is that the cncf has lots of options for every kind of tool.
03:25:42 [W1] There's probably more than one implementation and you can really build and compose a stack out of whatever you want that fits for your needs. So so less than paying attention to the choice.
03:25:50 [W1] We made you know, it's more that it's possible to do this composition of a platform now.
03:25:56 [W1] The ecosystem is moving so fast.
03:25:57 [W1] The tooling is being developed so fast that in six months, everything is going to be a hundred percent easier.
03:26:04 [W1] I'll get into my case study for that in a little bit.
03:26:06 [W1] But but I've been saying this over and over again and I feel like it's starting to change.
03:26:13 [W1] So we started the the pendulum swing of Simplicity and simplicity is great, but it has limits we were able to deploy really quickly.
03:26:21 [W1] Everything was sort of there for us, but we hit this point where we had maxed out the number of servers. The the postgres configuration was slowing us down and we needed to have more control and customization over our environment.
03:26:32 [W1] So we swung the pendulum all the way over to the other side of power and complexity.
03:26:38 [W1] We had the ability to do whatever we wanted, but you know,
03:26:42 [W1] Like the dog catching the car. What what do we actually want is is the question we end up having to answer and do we have the resources to do it?
03:26:51 [W1] All right going to that side we get into kubernative and we realize oh we need metrics. Okay.
03:26:59 [W1] Well great.
03:26:59 [W1] Prometheus is there for that and we need to be able to see what's happening between our systems well-distributed tracing is therefore that Jaeger is there.
03:27:05 [W1] Okay cool.
03:27:06 [W1] We got that covered. But then you get into the smaller nitty-gritty things of how do we actually secure this properly and
03:27:12 [W1] and hey, I would expect that if I change my environment variables the pods going to restart but that's not actually true.
03:27:18 [W1] How do we get Pottery starts when the config changes and there's just these and and and and it starts to feel overwhelming.
03:27:26 [W1] And and and and it starts to feel overwhelming.
03:27:31 [W1] And that's why I'm so excited that things are kind of swinging into this happy medium where the toolings there.
03:27:36 [W1] It takes less work to get the tools working together.
03:27:41 [W1] And I think critically the vendor support is catching up. I'm going to give you some some concrete examples of this.
03:27:48 [W1] Observability a couple years ago.
03:27:51 [W1] We were running our own Prometheus and Jaeger rail support for both was meager or non-existent.
03:27:56 [W1] So our observability infrastructure and up getting split between an APM solution in the cloud knative stuff.
03:28:00 [W1] Some of the cloud native stuff was behind a VPN.
03:28:03 [W1] Some of it wasn't most of our Engineers had ever been on a VPN before so that was confusing for them and the cloud native stuff was falling over occasionally or commonly because we didn't have dedicated eyes on it because we were doing 50 things trying to set all the
03:28:18 [W1] And and stuff up, so it wasn't a great experience now. We actually have a vendor who fully supports Cloud native stuff.
03:28:26 [W1] It's not an external metric that's going to get charged at five times the cost and they give us a Helm chart.
03:28:31 [W1] We deploy the helm chart.
03:28:33 [W1] It sends out Prometheus and opentelemetry we send our stuff there. It forwards it onto a central place where everyone can look at it and we've got great tooling around it.
03:28:42 [W1] It's been a fantastic experience. That's only really been possible in the last year year and a half.
03:28:48 [W1] Mesh in increases and other this was my case study for if you can wait six months. You should three years ago.
03:28:54 [W1] Adding because it was just going to do it for me, but it was still hard to deploy and it was really complex may be more complex than we needed. And and so it's grown to now most of these things like we use linkerd e and you can deploy a production configuration of linkerd e with a single CLI command
03:29:09 [W1] This is just astounding and it has made it so much easier to be in the space and to play with things and get things out quickly.
03:29:11 [W1] The standards are getting better.
03:29:14 [W1] And so interoperability is actually becoming a real possible thing. Right? We use Ambassador for our Gateway and it plays very nicely with linkerd e both of those play nicely with the open tracing and opentelemetry so we can get observability into all this
03:29:29 [W1] It's nice.
03:29:29 [W1] It's really nice.
03:29:31 [W1] Building and deploying.
03:29:33 [W1] This is probably the number one value add we've had this year for our develop that developer experience. I mentioned in that, you know, when that when that lead was talking about how hard everything was he was really referring to I got to do 17 a multi has and I gotta write this Docker file
03:29:48 [W1] Tain all of it and we've cut all that down by abstracting and out with standardized Helm charts that we've written for our Organization for Argo micro service or a rails service and now they just have to put in some values and deploy it it's getting
03:30:00 [W1] Just have to put in some values and deploy it it's getting much closer to being a pleasant platform for dr. Files were using buildpacks. Now no more dockerfile Strider maintain, ironically enough.
03:30:12 [W1] We're using Heroku is buildpacks Tak so as we migrate out the last long tail of our applications.
03:30:18 [W1] We don't even have to change the proc files its seamless to just get things out from where they were and deploy them into kubernative zits.
03:30:26 [W1] It's been real Pleasant and made things much faster.
03:30:29 [W1] And the thing I'm most excited about honestly is actually the marketplace that is being created.
03:30:34 [W1] I've actually gotten surprise from people when I say that we want to buy and not build and I think that's partially because a lot of the most vocal people in this environment are the big players. And so thanks to coupon for
03:30:49 [W1] Created I've actually gotten surprise from people when I say that we want to buy and not build and I think that's partially because a lot of the most vocal people in this environment are the big players. And so thanks to coupon for
03:31:14 [W1] It's like so to give this perspective but you know, there's a space between the five-person Oregon the 5,000 person org that where there's room.
03:31:23 [W1] We want to buy and not build we're too big for the platform as a service but I was talking to the VP of a Fortune 500 tech company and his team that is extending tekton is twice as big as my team, right they can build we need to buy because we can't be
03:31:38 [W1] A little bit about the process of how we built this platform.
03:31:44 [W1] The first most important thing that we did was we treated it like a product your internal devtools our product any platform you build inside is a product.
03:31:53 [W1] You have to treat it like one.
03:31:54 [W1] I think one of the biggest pitfalls of platform teams that I've seen is that they build things for themselves and platform teams tend to be composed of the wonky assist engineer's or some of them and you know, it ends up being a complex doctor or
03:32:08 [W1] Castration stack that is powered by make files written in Vim to do local development. And I'm one of those lucky people.
03:32:16 [W1] I like make I like them don't at me, but that's not what everyone consuming that platform wants to use.
03:32:23 [W1] And so I got I didn't get a product manager but I got part-time from a product manager for 2 months who helped me do interviews of our internal users of our engineer's and QA and support and we put together personas and
03:32:38 [W1] Then we were and then he kind of downloaded into my brain a bunch of product management, you know info and strategies and stuff that we've used over the rest of the year to help inform what we build.
03:32:50 [W1] The next thing we've done that's been really critical is sort of harnessing Conway's law.
03:32:55 [W1] Personas, we created from various people.
03:32:51 [W1] So, you know, we had a Persona of the junior engineer and the like the senior engineer that just wants to ship stuff and isn't really into configuration and getting their hands dirty and devops stuff.
03:33:01 [W1] Those people need to be represented on a group that's working on.
03:33:04 [W1] How do we do observability or how do we you know work on how do we improve our local development?
03:33:08 [W1] So those groups have defined most of the work to be done as an operations team. We stood back and one more the library for
03:33:17 [W1] Tools here are all the things we know about that will let you local deploy into the cluster, right and and then you guys can go research this and y'all can figure out what you want to use and then they're able to become.
03:33:33 [W1] And make the choices which are really impactful for people actually wanted to use that since we don't have anyone any dedicated team to actually implementing that team has a board. It generates we generate stories and then people pick up those stories as they can to
03:33:48 [W1] We'll pick up those stories as they can to write the script that will you know install all the tooling you need to interact with our servicemeshcon something.
03:33:55 [W1] So the final thing I want to talk about is just the promise of this ecosystem that I think is starting to become fulfilled.
03:34:04 [W1] Last year my favorite keynote was from Bryan Liles the when is it kubernative is going to have its rails moment and and he call that my favorite quote which is that kubernative is is necessarily complex.
03:34:15 [W1] I don't think we're quite at a rails moment. But as a relatively early adopter, I think we're getting a lot closer.
03:34:23 [W1] It's not exactly the stroller.
03:34:24 [W1] I showed earlier but but it's getting to be a little bit more like this necessarily complex squirrel feeder, you know, it does a lot of stuff that is
03:34:33 [W1] Important it we still had to put it together.
03:34:36 [W1] We had to compose it but it looks and feels a lot more professional and it's a lot more pleasant to use.
03:34:45 [W1] So my Mantra is changing its now if you can wait six months, you might actually be good to go. Right now. The the tooling is at a point in terms of maturity and interoperability where you can dive in as a midsize organ compose something that that is going to serve your needs
03:35:00 [W1] Off that is important it we still had to put it together.
03:35:05 [W1] We had to compose it but it looks and feels a lot more professional and it's a lot more pleasant to use.
03:35:13 [W1] So my Mantra is changing its now if you can wait six months, you might actually be good to go. Right now. The the tooling is at a point in terms of maturity and interoperability where you can dive in as a midsize organ compose something that that is going to serve your needs
03:36:02 [W1] And one of things I'm most excited about is seeing the actual platform as a service here every year because I really truly believe that if you are a five person or gue should not be in kubenetes. You should be on a platform as a service but now you can still go places be like
03:36:18 [W1] But now you can still go places be like we use containers and kubernative is because behind the scenes you actually are.
03:36:26 [W1] Thank you for watching.
03:36:27 [W1] I'm at the develop Nick and the regular places since I'm not about to step down from a platform Podium into the crowd of my peers and chat, which I really miss.
03:36:36 [W1] If you didn't get something out of this that you wish you did or you want more detail.
03:36:40 [W1] I would be overjoyed if you reached out to me.
03:36:41 [W1] I love talking about this stuff.
03:36:42 [W1] Thank you so much for watching.
03:36:55 [W1] Yes connecting with a crowd even when virtual thank you.
03:36:59 [W1] Thank you David So as we go into the close of day to Keynotes. I just wanted to give a few closing remarks.
03:37:07 [W1] So, you know at the end of the day event today, we're going to have a variety of interactive experiences for you to enjoy such as Marvel Cinematic Universe trivia computer and video game history trivia and
03:37:22 [W1] Virtual Escape rooms. So police kept check out the schedule and RSVP to participate. So as David had mentioned being able to connect with all of the attendees is super important.
03:37:35 [W1] So take some time for the next 15 minutes.
03:37:38 [W1] All of the keynote speakers are going to be hanging out in the keynote slack Channel.
03:37:43 [W1] There's been lots of chatter already.
03:37:45 [W1] So if you want to connect to anyone and chat a little bit more deeply about about what they've talked about today head to
03:37:51 [W1] to number pound to - Keep Calm Keynotes. So the breakout sessions will begin at 2:55 u.s.
03:38:04 [W1] Eastern.
03:38:04 [W1] So take a break hang out in slack and then come join us for for more.
03:38:10 [W1] Awesome Cube con Cloud knative Khan. Thank you all.
03:38:19 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
03:39:29 [W1] Kubernative will be boring as soon as we can make it boring.
03:39:32 [W1] Maybe after I retire it's already boring.
03:39:36 [W1] So I think we're well on our journey towards kubernative being boring.
03:39:40 [W1] It'll only be boring when it's done.
03:39:42 [W1] It'll only be done when it's no longer relevant.
03:43:20 [W1] our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because
03:43:30 [W1] solving a complex problem
03:44:30 [W1] Kubernative will be boring as soon as we can make it boring.
03:44:34 [W1] Maybe after I retire it's already boring.
03:44:37 [W1] So I think we're well on our journey towards kubernative being boring.
03:44:41 [W1] It'll only be boring when it's done.
03:44:43 [W1] It'll only be done when it's no longer relevant.

Transcription for wordly [W1]

00:00:00 [W] Kubernative will be boring as soon as we can make it boring.
00:00:03 [W] Maybe after I retire it's already boring.
00:00:06 [W] So I think we're well on our journey towards kubernative being boring.
00:00:10 [W] It'll only be boring when it's done.
00:00:12 [W] It'll only be done when it's no longer relevant.
00:00:59 [W] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:13:59 [W1] Kubernative will be boring as soon as we can make it boring.
02:14:02 [W1] Maybe after I retire it's already boring.
02:14:06 [W1] So I think we're well on our journey towards kubernative being boring.
02:14:09 [W1] It'll only be boring when it's done.
02:14:11 [W1] It'll only be done when it's no longer relevant.
02:15:10 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:18:49 [W1] Kubernative will be boring as soon as we can make it boring.
02:18:52 [W1] Maybe after I retire it's already boring.
02:18:56 [W1] So I think we're well on our journey towards kubernative being boring.
02:18:59 [W1] It'll only be boring when it's done.
02:19:01 [W1] It'll only be done when it's no longer relevant.
02:20:00 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
02:20:49 [W1] Hello.
02:20:50 [W1] Hello everyone Cube con Cloud knative con virtual attendees.
02:20:55 [W1] I hope all of you had a great opportunity to check out the content from yesterday.
02:20:59 [W1] We've got I hope you met some new people hung out in slack the hallway chat.
02:21:06 [W1] I know that I got an opportunity to meet a lot of new folks as well as Place among us later in the night.
02:21:12 [W1] So I hope you're having fun. And we're going to get started with the day to Keynotes. So kicking us off my lovely co-chair Constance caramanolis.
02:21:24 [W1] This constants is a principal engineer at Splunk.
02:21:29 [W1] And yeah, she is going to be talking to you today about the opentelemetry collector and how it empowers end users.
02:21:37 [W1] So check it out.
02:21:48 [W1] Hi again, everyone.
02:21:50 [W1] I'm Constance.
02:21:51 [W1] caramanolis. And today I will be talking to you about the opentelemetry collector and how it empowers and users.
02:22:01 [W1] Now you're probably wondering what is opentelemetry.
02:22:03 [W1] It is a merger of open senses and opentracing from early 2019.
02:22:08 [W1] All right, these were to project successful projects.
02:22:12 [W1] They're working on very similar goals. And the decision was done to merge these into one project to unify our resources.
02:22:20 [W1] It is a collection of tools apis and sdks and yes that actually means that there are their support and several languages actual libraries that you can
02:22:31 [W1] Use the adopt in your and applications today for more details in terms of the instrumentation side today.
02:22:40 [W1] We focus on tooling I highly suggest that you watched last year's keynote with Liz Fang Jones and Sarah Novotny introducing opentelemetry and also some Kook Connie you talks.
02:22:55 [W1] Now why opentelemetry now the moment observability is here as we build more complex systems.
02:23:04 [W1] We need to better understand this our systems and so it is the time for a project that focuses on merging multiple Telemetry formats metrics traces actually logs.
02:23:19 [W1] and it is all about prioritizing end users and as you'll see in our landing page we talked about it being a vendor agnostic because unfortunately when it comes to Telemetry and the resulting back ends vendor lock-in has happened,
02:23:35 [W1] It depends on where you are in your car knative Journey if you are lucky and or an early adopter of cognitive vendor lock-in might not be a strong story for you. But for meant I can guarantee that for many of you watching today vendor lock-in is a pain.
02:23:50 [W1] and this actually hinders your ability to choose the proper back ends for you because vendors will, you know will provide better tooling and better instrumentation that works better with their back end, but this isn't
02:24:06 [W1] The way in the future that is not the point of cloud native and so opentelemetry is partial is strong addressing that by making sure to provide vendor agnostic instrumentation tooling.
02:24:17 [W1] Now you're probably thinking great another migration and I don't blame you. It is annoying to think about another migration, right?
02:24:26 [W1] You're probably thinking about like a job servicemeshcon Burnett. He's like what about containers you security so many things working there. And so I deserve this face now I am here to tell you that in terms of adopting and migraine opentelemetry.
02:24:42 [W1] Retrieve it isn't an all-or-nothing.
02:24:43 [W1] There is a way for you to start adopting opentelemetry collect opentelemetry and specifically The Collector without changing much of your existing infrastructure.
02:24:52 [W1] And so the path forward is oh The opentelemetry Collector now, you're probably wondering what is it?
02:25:01 [W1] It's an executable that receives State Telemetry data transforms it and sends the data long. It works on metrics and traces.
02:25:10 [W1] It supports several popular open source protocols.
02:25:13 [W1] I want to call this out because right opentelemetry does have its own protocol for spec for a PS4 metrics and traces but we do acknowledge that there have been other protocols for metrics and traces before
02:25:28 [W1] And so to successfully adopt opentelemetry doesn't mean that you have to use our protocol right away it you can still use your existing protocols as I'll show in a little bit.
02:25:39 [W1] And it's a pluggable architecture.
02:25:40 [W1] If you like. The code base is pretty nice and clean.
02:25:43 [W1] They're very clear interfaces. So you can Implement, you know, either new ways to get data in transform the data and Export it out.
02:25:51 [W1] There are two ways.
02:25:53 [W1] You can deploy it one as an agent sidecar or as a standalone binary instance, you know examples for my why you might want to write it as an agent is that if you want to do sampling decisions or off if you have a high throughput servicemeshcon,
02:26:08 [W1] And you want to offload the Telemetry as soon as possible to some other binary so you don't create a bottleneck in the application. That's one way or you can just have it a standalone instance because maybe you adding some extra information to your metrics and traces after it before for the long trip back
02:26:24 [W1] Either one it is all the same binary.
02:26:28 [W1] Now the internals of The Collector we said it before but if there's three parts would getting data in and the formal term is the receivers and so receivers do is that once you get something with an once
02:26:43 [W1] Within a certain protocol it gets translated into internal format and this internal format the processors consume this internal for format and then transforms the data optionally little spoilers in a bit and
02:26:58 [W1] Then it forwards data along to whatever for Mike what?
02:27:04 [W1] Now we're using you migrations is a few complex things about it to think about it one is that you can have multiple Telemetry workloads, right different teams might have used different protocols.
02:27:15 [W1] They might be sending these different back ends.
02:27:18 [W1] ends. It's a little complicated there depending you know, when the Telemetry is added the quality of the Telemetry can differ.
02:27:28 [W1] So you want to standardize this and then controlling where the data goes?
02:27:32 [W1] It's grades. You generate all this Telemetry but controlling where it goes in the end is so important.
02:27:39 [W1] Now, let's tackle the first thing managing multiple workloads.
02:27:42 [W1] This is naturally a simplified service Services application depiction.
02:27:48 [W1] And so all these services use only one Matrix format and some things to Prometheus back end and now they're actually two different at racing protocol Zipkin any acre in this case and you know
02:28:03 [W1] Services right.
02:28:04 [W1] They might have these values hard-coded what makes it really hard is say if you're the observability team is how do you manage all of these right?
02:28:11 [W1] It's if you don't provide a central way for teams to send their traces to some data collection.
02:28:18 [W1] Yes, the collector it's hard to audit where things are going.
02:28:22 [W1] So what I'm suggesting to you today is that you can use a collector to maintain all of your workflows. Now, there's a few ways you can do it one is that you can use a collector to create
02:28:33 [W1] in this case since we had three different protocols one for metrics into for traces three different pipelines one for Prometheus one for Zipkin.
02:28:40 [W1] Sorry go back and one for jaeger.
02:28:43 [W1] But also if you wanted to say, you know, what we're tired of having two different backends for traces.
02:28:50 [W1] We're going to merge them.
02:28:52 [W1] What's really cool about the collector right was hinted at in the pipeline's diagram.
02:28:57 [W1] is that since the port supports both receivers? It can see on one pipeline.
02:29:01 [W1] You can say I want to support Zipkin and Jaeger and then it will convert both of those into the internal data format. And then after once it goes through the rest of the pipeline exporting whatever format you want it to be this allows for you to have multiple protocols.
02:29:16 [W1] Most for Telemetry data and send it to one back end or you know, maybe something even cooler the end.
02:29:23 [W1] Now, how does this new work out of like, you know, you have these protocols and a new service comes along and this you service owners are super excited about opentelemetry and they want to use the opentelemetry format now
02:29:38 [W1] Kind of like what we did with is it going to Jaeger Trace pipeline?
02:29:41 [W1] We can do the exact same thing.
02:29:42 [W1] We can add the oh tell protocol to the receivers for the metric and traces and just send the data alone.
02:29:49 [W1] This means that your service owners don't need to change protocols.
02:29:53 [W1] when you can control the data, yes, it does require changing where the data is, you know exporting to but it also means to that you don't have to force everyone to upgrade to the latest instrumentation right to the latest library or
02:30:07 [W1] all this gives you power to choose.
02:30:10 [W1] Hey, maybe we only upgrade the latest Services. Right only new Services uses latest protocol or you know, this one service it's needs to be re-written.
02:30:20 [W1] Let's do that one there.
02:30:21 [W1] This gives you power to choose when you adopt a new protocol.
02:30:25 [W1] Now let's talk about standardizing data. We have you know, now we can we get all our data working through but now we're seeing that the quality of data is and so great right and a few causes for this different Source different quality of data can be different languages, you
02:30:41 [W1] Naming for that maybe earlier data that doesn't isn't as rich as newer data because we've asked as we play around with systems for longer. We learn to ask better questions meanings evolved a term that we might have used for five years ago doesn't make sense
02:30:57 [W1] More right maybe you've added more regions. And before there were no region is used to use default and accidents happen as best as we try sometimes, you know, like Pai leaks right something like that and we'd way to catch that.
02:31:13 [W1] Yes, as you all know the collector does all of these things.
02:31:16 [W1] so it's called the a trees processor and this example, I'm going to be operating on traces and so say I'm missing an attribute right?
02:31:25 [W1] I have an earlier version of the application that wasn't setting environment before we just had one environment now, we actually have testing staging production.
02:31:36 [W1] And so using this processor I can say let me add this attribute environment prodyna.
02:31:43 [W1] And now my data will look similar, you know from your stuff.
02:31:48 [W1] And then I realized looking a little bit more.
02:31:50 [W1] Oh, oops.
02:31:51 [W1] you know what I got into a little joke saying that the west coast is the best coast and so I set region to Best Coast and it didn't make it wasn't as much of an issue back then because we're only deployed on the west coast, but now we actually have east coast and yes
02:32:06 [W1] East Coast is really cool too. You know, it can be the best coast.
02:32:10 [W1] So the best is Coast together, so I want to change Best Coast to West Coast.
02:32:16 [W1] And you know, I'm looking at the state of little bit more and oops. Sorry Steven.
02:32:20 [W1] I'll beat your name in logging, you know in setting these attributes.
02:32:26 [W1] And so that's not okay.
02:32:27 [W1] So I'm actually going to delete anything with username for my Trace to look like my span to look like this.
02:32:34 [W1] Now this is giving you just a few examples of how you can tweak existing data. And this is all done in the collector.
02:32:41 [W1] I haven't given any llamo files here because I think we're all a little bit tired of Gamal the moment, but this is all done with a few lines of animal.
02:32:50 [W1] This doesn't require application to change anything. This allows you to say the central observability team to control the quality of the data without pushing changes on to the application.
02:33:02 [W1] And the last part 14 your data along.
02:33:05 [W1] All right, we're getting got data in multiple receivers.
02:33:08 [W1] We have enriched it standardize the data and now we want to send it on out work.
02:33:12 [W1] And so right now in this example here, we only have we only sent one back end.
02:33:17 [W1] But what happens maybe if you had a hack-a-thon and two teams came up with two different back ends and you want to compare these back and side by side.
02:33:25 [W1] Well, this is great is that with similar to the receivers? You have multiple receivers we support multiple.
02:33:31 [W1] Orders and so with a few lines from few lines again will code that saving your eyes from at the moment is that you can tell the collector.
02:33:40 [W1] Hey send everything, you know, send all these traces to back in 1 2 & 3 and this has absolutely no implication on your application.
02:33:48 [W1] Yes implication your application. Absolutely none.
02:33:52 [W1] Now the statement I want to make a microphone want to make an even stronger statement is that this gives you the power to choose whatever backend you want, right? This is partially targeting the vendor lock-in is that if you can control how you collect all your data, you can also
02:34:07 [W1] Control where your data goes and this actually holds I know it's funny coming from me because I do work for a vendor but this holds us vendor accountable to providing a great back end for you, right if you have the power to choose where your data goes
02:34:22 [W1] We have responsibilities do to make sure that you have a bad experience with your data.
02:34:26 [W1] And so that's why I think that this is actually the coolest thing about opentelemetry collector is that this puts the power in your hands?
02:34:33 [W1] So to reiterate some of the things we saw today from complex operations to ease your operations, we can go from instead of you know, very disjointed ways to managing multiple Telemetry workflows.
02:34:48 [W1] It's a collector you can have multiple pipelines and multiple protocol support so you can
02:34:51 [W1] Do that with one binary one deployment if you want to gamble you can standardize a quality to climb a tree using the attributes processor.
02:34:59 [W1] All right, this is once again, no code changes and you get to control where your Telemetry goes.
02:35:05 [W1] This is leveraging multiple exporters and multiple protocol support now a few other takeaways is that it's not a requirement for you to use the opentelemetry protocols to Leverage The Collector.
02:35:18 [W1] There are a lot of really cool features in the collector that I highly stressed you're looking looking into if you're trying to collect and manage all of your telemetry.
02:35:29 [W1] There's also provides a way for you to adopt adopt new Telemetry protocols and only change what is needed right by say having one pipeline that accepts multiple protocols.
02:35:41 [W1] You can then say I keep everything, you know anything before a certain date only using protocol, you know Legacy protocol anything going forward using opentelemetry protocol.
02:35:51 [W1] This gives you more power to choose.
02:35:53 [W1] It's easily extendable write anything that is missing add it right. If you have a custom protocol crate, you know, it's an easy to add a new receiver and exporter to support your custom protocols.
02:36:08 [W1] And processors do more of the modify attributes.
02:36:10 [W1] They can do sampling memory limiting which protects the collector from booming because you don't know losing your Telemetry batching or retrying queued like standing with a kubernative metadata.
02:36:22 [W1] There are quite a few processes that are able to enrich the data and also control how data flows through
02:36:30 [W1] So I'm hoping that your little bit less annoyed with the call to migration and that this may be provides a path forward for you to look into opentelemetry.
02:36:40 [W1] So the collector and also instrumentation without thinking without it having to be a huge migration and only tensely updating small things that need to be updated.
02:36:50 [W1] It's maybe a little happier with this concept.
02:36:52 [W1] We are working towards a g a and you can
02:36:58 [W1] Find The Collector repository here collector and contribute their and there's a meeting every second Wednesday.
02:37:04 [W1] Thank you. Everyone. Keep Cloud native connected everywhere.
02:37:07 [W1] Bye.
02:37:11 [W1] Welcome back.
02:37:12 [W1] Thank you so much constants for that awesome.
02:37:15 [W1] And opentelemetry collector.
02:37:17 [W1] There's some great chatter happening in the Keynotes channel on slack as well.
02:37:21 [W1] So check that out including a link to a list following Jones is talk.
02:37:25 [W1] So next up.
02:37:27 [W1] are we have non davidov?
02:37:30 [W1] Ananda is a lead data scientist and distinguished technologist at hpe and she will be discussing how to leverage open source projects such as Spire Opa and Envoy to provide A fine grain policy for overlay for your
02:37:45 [W1] Your machine learning pipelines take it away.
02:37:57 [W1] Hi, welcome everyone.
02:37:58 [W1] This talk is about scaling machine Learning Without compromising privacy and security.
02:38:05 [W1] Thank you cncf for giving me this opportunity.
02:38:09 [W1] and thank you audience for your attention.
02:38:12 [W1] My name is Amanda, which I Dave I lead product management at hpe s Barrel container platform and machine learning Ops. My background is in technology transformation at medium to large Enterprises specialized.
02:38:27 [W1] And big data and machine learning operations over the next few minutes.
02:38:32 [W1] We will take a look at the complexities of the machine learning pipeline. What are some of the possible touch points?
02:38:42 [W1] And areas of concern and what are some of the technologies that are being used to mitigate risks and lower the exposure in machine learning by plants?
02:38:53 [W1] This picture is from a famous paper by Google where it talks about the technical debt and the various stages of machine learning that most people associate machine learning with algorithms
02:39:08 [W1] The MLK chord or the MLS self because there's a lot that happens before during and after this may run on one or many communities clusters depending on the deployments at
02:39:23 [W1] Stammers there are various actors.
02:39:26 [W1] There is where various personas that interact with different stages of the pipeline and there are services within kubenetes and Ingress for users to access those services
02:39:41 [W1] And also there are external entity. If you look at this picture, this is our representative architecture of a large financial institution that has a pretty large footprint of machine learning activities that happen across
02:39:56 [W1] Sweetie systems and across different groups, right? You have data pre-processing done by your data engineers. And then you have the actual machine learning process done by your data scientists
02:40:11 [W1] And then a deployment of a model so not getting into the detail of this.
02:40:19 [W1] What's of concern for us in the next few minutes is that there are several different processes as you can see here this perimeter there is end attack surface multiple different kubernative
02:40:34 [W1] These clusters probably in different trusted domain and then you have a number of different external systems that have to be accessed and for those who are familiar with security and kubernative by default.
02:40:48 [W1] There is really no security that's enabled. So if you do a threat model for this
02:40:54 [W1] You can see that the graph of how a user accesses the system and what are all the touch points they go through for various activities.
02:41:05 [W1] What services are talking to other services?
02:41:08 [W1] So you have authenticating and authorizing to individual service by users. Then you have to establish trust between Services tenants and you know across sites if needed.
02:41:24 [W1] Because of external systems that are accessed as part of this pipeline you have an increased attack surface, especially with data access.
02:41:34 [W1] Now you have privacy concerns here. Our path to resolution is happening in multiple phases using best of the build Technologies such as ldap and I DC connector as you can see here.
02:41:49 [W1] And M TLS between systems.
02:41:54 [W1] Using you know jot and spiffe E identities.
02:42:00 [W1] For preventing attacks surface.
02:42:03 [W1] This is something we are paying special attention to with admission control various parts creating policies and network controls.
02:42:15 [W1] And for extended policies such as limiting your authorized Docker registry and things like that. There are Opa Rego policies.
02:42:26 [W1] For phase two there is experimentation going on to establish more transitive identity between users services and also external resources that are outside of the Cabela's Network and for
02:42:41 [W1] Relation between on-prem and Cloud trust remains. We are looking at spiffe a federation.
02:42:48 [W1] Thank you and looking forward to seeing you on another talks.
02:42:52 [W1] Very exciting.
02:42:56 [W1] Thank you nan de next up. We have Jonathan beri.
02:43:00 [W1] Jonathan is the founder of stealth and today he will be talking discussing the need of how and where networking Protocols are supported. It's uncover roadblocks.
02:43:10 [W1] So Jonathan take it away.
02:43:23 [W1] Hi everyone in this session.
02:43:26 [W1] We'll be discussing protocols specifically networking protocols.
02:43:31 [W1] I'll cover some of the challenges we faced upon protocols to production and where we as a community to make it easier.
02:43:38 [W1] Let's go.
02:43:42 [W1] My name is Jonathan beri, and I work on an iot startup. You can find me on Twitter at berry berry kicks if you want to chat about protocols or anything else.
02:43:55 [W1] We should probably begin our discussion with HTTP as it is the networking protocol.
02:44:01 [W1] Most people are familiar with its the lingua Franca of the web and a large percentage of web services are built using http.
02:44:13 [W1] Now once you have your HD TV service, you're not done.
02:44:17 [W1] You need a bunch of things to deploy to production and in the cloud things like load balancing routing security and observability.
02:44:29 [W1] Since HTTP is a popular protocol different projects and operators provide these things for HTTP out of the box.
02:44:39 [W1] Which is great since you would otherwise have to build these things yourself.
02:44:44 [W1] But HTTP isn't the only protocol you use all the time.
02:44:51 [W1] There's also DNS. Everyone's favorite service you use it when you want to configure a server to master domain name or enable services within a cluster to discover each other.
02:45:03 [W1] But what if you wanted to create a DNS service?
02:45:06 [W1] Turns out this is a thing that people want to do.
02:45:11 [W1] Well, you may not have realized that DNS is actually a suite of protocols and when you use services like Amazon Route 53 or Cordina s those are actually implementations of DNS protocols.
02:45:28 [W1] Dennis like HTTP has similar things it needs in order to be deployed to production.
02:45:36 [W1] However, since DNS is a less common protocol compared to http.
02:45:41 [W1] It isn't supported out-of-the-box by many of the cloud upon objects. And therefore you have to build a lot of these things 14s yourself.
02:45:51 [W1] And that makes it way harder to build your own solution that implements DNS.
02:45:59 [W1] And there are a lot of application and Industry protocols that are designed to fill a specific purpose.
02:46:08 [W1] I've listed a few examples here, but they range from synchronizing game state to streaming webrtc to the internet of things and actually iot is what first got me interested in exploring the topic of implementing protocols in the first place.
02:46:27 [W1] See, there are a ton of iot protocols designed specifically for the needs of connecting physical devices to each other and to the internet.
02:46:38 [W1] Some are optimized for power somewhere optimized for bandwidth others Implement standardized industrial consumer control planes.
02:46:47 [W1] Some are built on the Internet Protocol like UDP and TCP. While others simply can't use IP.
02:46:54 [W1] As a start-up we're interested in implementing many of these protocols in our Cloud solution.
02:47:02 [W1] the challenge my startup phases in bringing iot protocols to production is similar to gns in that these Protocols are less common than HTTP a lot less in fact and therefore aren't supporting easily by many of the projects
02:47:17 [W1] Services and clouds. We might want to leverage their for we're faced with the challenge of implementing load balancing routing Etc from scratch all by yourself. That means we can't leverage the wealth of
02:47:32 [W1] source for this community or take advantage of great solution providers
02:47:39 [W1] But I did say before the challenge of implementing protocols isn't specific to iot.
02:47:45 [W1] I want to see if other people in the community are also implementing new protocols and see what I can learn from them.
02:47:52 [W1] So I started talking to other folks like game server devs working on Project Argo Nats webrtc maintainers working on Pion Telco folks from the network servicemeshcon Direct in others.
02:48:06 [W1] Turns out they're all asking a similar fundamental question.
02:48:13 [W1] How can we Implement any protocol in a cloud native way?
02:48:17 [W1] This is clearly a community challenge something bigger than my startup or iot.
02:48:26 [W1] I'm a p.m.
02:48:27 [W1] So I did what we do best start a dock can go it to it now at bit ly / Beyond http.
02:48:35 [W1] It's a living doc.
02:48:37 [W1] that survey is a growing list of cloud native projects that might be used as part of a solution that implements a networking protocol it tries to hit identify which projects are specifically focusing on supporting non HTTP protocols and suggestions on where they can add additional
02:48:53 [W1] And we implementers might need feedback in contributions are greatly welcome.
02:48:59 [W1] I started the dock over a year ago and since that time contributors and maintainers have helped shave it at the same time projects across the cloud native ecosystem have added up streamed features that make it easier to implement new protocols.
02:49:14 [W1] His progress is both exciting and encouraging.
02:49:19 [W1] There's more projects than I have time to discuss in this session, but I wanted to highlight a few I think will be used by the most amount of those.
02:49:27 [W1] communities Envoy servicemeshcon face cloudevents and Network Services
02:49:36 [W1] Let's start with kubernetes.
02:49:39 [W1] Kubernative has a concept of Ingress which is an object that exposes networking traffic from outside the cluster to services within the current stable Ingress implementation supports HTTP go figure within the
02:49:55 [W1] Say it group has been hard at work developing the new Gateway guy to replace Ingress.
02:50:01 [W1] Like Ingress a Gateway routes traffic to a service that supports more protocols like UDP and TCP among other things with a goal to enable protocol implementers to create custom gateways in the future.
02:50:14 [W1] They already have demos you can try and it may be in an alpha look released by the time you're watching this report.
02:50:21 [W1] The envoy as a popular networking proxy by its nature needs to support a protocol in order to act as a proxy for said protocol.
02:50:31 [W1] Envoy has always had native support for HTTP, but due to its popularity began implementing support for protocols like redis and postgres albeit in a one-off fashion.
02:50:44 [W1] Over the last year or two, the team has added UDP and TCP listeners, which make it possible for protocol implementers to use Envoy to proxy any protocol based on those layer for Primitives which covers a lot of potential protocols.
02:51:00 [W1] Also with the introduction of webassembly support Envoy has now made it even easier for more people to implement custom protocols in the language of their choice.
02:51:11 [W1] One more thing since Envoy is used as a basis for other projects like servicemeshcon and observability tools and now becomes easier for projects like sto and Prometheus to support custom protocols in the future.
02:51:27 [W1] Next I'd like to highlight servicemeshcon face for SMI, which is developing a standard for servicemeshcon kubenetes.
02:51:37 [W1] One aspect of a servicemeshcon it manages traffic between services within Foster.
02:51:44 [W1] SMI defines a concept of traffic specs a list of subjects that Define how traffic flows between the mesh.
02:51:52 [W1] There are already some specs for HTTP TCP and UDP.
02:51:57 [W1] The intent is for protocol implementers to Define their own traffic specs and eventually contribute them Upstream to SLI. Where make sense.
02:52:07 [W1] You might notice some parallels between the Gateway API for Nettie's and the traffic spec here in SMI.
02:52:14 [W1] Gateway's are about traffic coming into the cluster while traffic specs are internal traffic Within.
02:52:21 [W1] So there's a potential the two projects to work well together.
02:52:25 [W1] I'm personally excited to see how these two efforts collaborate in the future.
02:52:31 [W1] Cloudevents is a specification for describing event data in common formats to provide interoperability across Services platforms and systems.
02:52:40 [W1] It defines how to serialize events in different formats like Json and protocols which of course include http.
02:52:49 [W1] There's already some specs for protocols like Kafka mqtt and Nats and cleared documentation on how to define custom / proprietary protocols.
02:53:01 [W1] Cloudevents is already being used today in projects. Like he knative idly will become more commonplace among event oriented microservices.
02:53:08 [W1] So having a way to define custom protocol bindings will make it easier for services that leverage non-http protocols to interoperate.
02:53:20 [W1] The last project I'm going to touch on is Network servicemeshcon fused with the other kind of service for the servicemeshcon your face.
02:53:29 [W1] Nsmcon to extend the existing kubernative networking model, but address networking use cases.
02:53:35 [W1] They need to go deeper in the stack like el34 L 2 layers or even ethernet frames themselves inspired by sto network is servicemeshcon the concepts of a servicemeshcon L2 L3 payloads.
02:53:50 [W1] Hence the name.
02:53:51 [W1] Telco 's and the cncf Telecom User Group are big fans of nsmcon.
02:53:56 [W1] One reason is how it enables them to implement cellular specific protocols in a cloud knative way 5G is repelling the Telecom industry fully embracing what they call Cloud Network functions many of the
02:54:11 [W1] Oceans have been classically implemented in very expensive Hardware found in cell towers would take a long time to physically operate with Cloud native functions Telecom operators.
02:54:21 [W1] to build these capabilities as services that can be more easily upgraded over time.
02:54:27 [W1] Lots of the functions are specific Niche protocols that are part of the operation of 5G networks and nsmcon has the means to configure them on top of case moist.
02:54:39 [W1] I'm only scratching the surface of what nsmcon fun fact because servicemeshcon Sly Castillo operate up the stack at L7 L4 and nsmcon LP and below.
02:54:50 [W1] You can use a servicemeshcon NSM. There's a bunch of advantages when combining these two concepts.
02:54:57 [W1] I don't have time to dig into that now, but the innocent grew have some examples.
02:55:03 [W1] with that I want to conclude with what you could do to participate if you're trying such a great something with protocols other than HTTP, share your story documenting use cases both real world and aspirational has been
02:55:18 [W1] The operation 5G networks and nsmcon has the means to configure them on top of case moist.
02:55:26 [W1] I'm only scratching the surface of what nsmcon fun fact because servicemeshcon is like sto operate up the stack at L7 L4 and nsmcon LP and below.
02:55:37 [W1] You can use a servicemeshcon NSM. There's a bunch of advantages when combining these two concepts.
02:55:44 [W1] I don't have time to dig into that now, but the innocent proof have some examples.
02:55:49 [W1] with that I want to conclude with what you can do to participate if you're trying such a great something with protocols other than HTTP, share your story documenting use cases both real world and aspirational has been the
02:57:49 [W1] If tool to help improve protocol support create issues hop on slack. The cncf poor communities is a great place or just hit me up on Twitter.
02:57:58 [W1] And if you're a project maintainer try to identify where your projects currently assume traffic is HD and look for places for accessibility. Oh and asking you is what they think.
02:58:11 [W1] And that's all I got.
02:58:12 [W1] Thank you. Everyone.
02:58:14 [W1] Here's a link to that dock one more time, and you can reach me on Twitter at Heretics. Cheers.
02:58:22 [W1] Thank you Jonathan for that wonderful presentation again harkening back to the end-users reach out to your end users and think about their use cases so you can build for that next up. We have vijoy append a feature is
02:58:37 [W1] Presentation again harkening back to the end-users reach out to your end users and think about their use cases so you can build for that next up. We have vijoy Pandey futurewei is a VP of engineering for emerging Technologies
02:59:00 [W1] Paying for emerging Technologies and incubation at Cisco and he will be discussing the challenges of making your API secure and and the waist overcome them.
02:59:19 [W1] Hello, folks.
02:59:21 [W1] I'm vijoy and welcome to Q Khan and I'm here to ask Marvin about where my security apis are.
02:59:28 [W1] It's also an ask for the community to step up our application security game. But before we begin we have to ask so who is Marvin?
02:59:38 [W1] If you haven't read The Hitchhiker's Guide to the Galaxy go and get a copy now.
02:59:43 [W1] Marvin is a robot with a brain the size of a planet by his own account.
02:59:49 [W1] He's at least fifty thousand times smarter than a human but he's being made to park cars pick papers check airlocks.
02:59:58 [W1] not really job satisfaction worthy since he is built with genuine people personality or GBP Tech. He also has human emotions. So he gets depressed with all of this mindless work.
03:00:11 [W1] but most importantly he is a Paranoid Android and all of these properties make him very suited for the State of Affairs around API security as you shall find out but first a little bit of historical context around
03:00:27 [W1] Knative as we all know a pocket textures are changing from monolithic that metal ovhcloud.
03:00:43 [W1] Over the last 3 Cube cons. We have discussed how the network needed to evolve to handle this transformation to Cloud native through the network servicemeshcon. Jecht. And today we are looking at this Cloud knative developer as she is building
03:00:58 [W1] Moving fast utilizing the Trove of apis from public Cloud providers such as providers and other services she is faced with the dependency graph complexity of a model globally distributed.
03:00:57 [W1] Microsoft is based app Security in such an architecture is a modern brain size problem.
03:01:03 [W1] By the way. This is the Microsoft this dependency graph of the Monza banking app and as she uses these globally distributed apis, which are either home.
03:01:13 [W1] All groan or from external providers the quality and security of these remote Services is often unknown.
03:01:20 [W1] The pivot on API reputation is not inherently part of the ci/cd process which might put your customers data eventually at risk.
03:01:31 [W1] This rest causes all kinds of unwanted Behavior developers want convenience and velocity security a sorry sack of sand sea Source pound the table for reviews and are still unsure whether the mm API is being used are safe or compliant.
03:01:47 [W1] From One Cloud ideas are always wanted that insecure code has been pushed to prod and all of these reviews and Gates and meetings just add complexity and opacity to the simple job of developing and deploying fast.
03:01:57 [W1] The simple job of developing and deploying fast.
03:01:41 [W1] Seeing all of this Marvin the Paranoid Android with the weight of the world on his shoulders steps in with this depressing Court.
03:01:49 [W1] Wouldn't it be nice if there really was a modern with a PlanetSide brain who could be observing the entire application lifecycle fund repo to runtime and identify the risks of using
03:02:04 [W1] All the teams concerned report that risk to security a sorry sack of sand sea salts and maybe even remediate that risk to customer Data before it happens.
03:02:13 [W1] Now that is indeed a modern sighs problem, but do not underestimate Marvel he could simultaneously plan the entire planets military strategy and solve all of these major problems of the universe
03:02:28 [W1] I didn't Pablo also compose a number of lot of eyes if you could do all of that. He could surely solve for the application and API security problem.
03:02:38 [W1] So what would the app lifecycle look like? If we had this Marvin the developer would take Marvin curated apis and move fast fast fast.
03:02:47 [W1] Marvin would notify security teams of possible issues and mediations continuously and cloud form teams could rest easy knowing they can report compliance in a real-time matter.
03:03:02 [W1] How would we train this morning?
03:03:04 [W1] What are these boundary conditions?
03:03:05 [W1] What are its parameters first and foremost we have to realize that it's all about protecting a customer's data.
03:03:13 [W1] The open internet is really the runtime for all modern Cloud native apps security attacks risk, both clients users and other services other applications and the new parameter of apps and security is really
03:03:28 [W1] Is down to an API or data object.
03:03:28 [W1] What would be some of the apps with questions we could ask this Marvin?
03:03:33 [W1] Is the correct in-house service image or artifact being used in creating a new app are we integrating with compliant third-party apis are we tracking RPC calls to make sure we do not import never is data and
03:03:48 [W1] these to ensure safety for elapsed
03:01:50 [W1] some of the broader questions we have to ask ourselves as the community could be if a distributed app does get broken into do we have all the tooling to drive meantime to detect and debug down can be formally model and verify
03:02:05 [W1] Logic in our business apps even across layers.
03:02:09 [W1] Can we the community help spread the word on API misuse?
03:02:14 [W1] If we aren't able to step up to the plate and answer these questions, I'm afraid Marvin has this to say to you.
03:02:22 [W1] Thank you for listening.
03:02:22 [W1] Please reach out to me.
03:02:24 [W1] I would love to continue the conversation visit us at the Cisco Booth where we have demos on app Security app and in for observability cloud native networking a lot more and we are highly and we would love to have you on board. Thank you.
03:02:41 [W1] Awesome presentation from vijoy.
03:02:45 [W1] I hope we can all please Marvin through moving forward when we're designing our apis.
03:02:49 [W1] So next up. We have a void doc to change to 10 ski why Doc is a staff engineer at Google and also one of the co-chairs for kubernative Sig scalability today.
03:03:03 [W1] He will be discussing improvements made to kubernative A's and SED that unlock
03:03:08 [W1] massive clusters running with 15,000 nodes. So enjoy
03:03:23 [W1] Hello Cube Khan. My name is Victor twins key. I work in Google and I'm part of this amazing kubernative Community for almost six years at this point.
03:03:33 [W1] There's I'm the TL of scalability and in connection to that during this presentation.
03:03:34 [W1] I'm going to talk about example features that allow us to run kubernative clusters with 15,000 notes.
03:03:43 [W1] But let's start with making this explicit.
03:03:47 [W1] Kubernative clusters with 15,000 nodes are already a thing.
03:03:53 [W1] As you might have heard earlier this year buyer crop science and Google together published an interesting blog post.
03:04:02 [W1] You can read their how thanks to running on g k clusters with fifteen thousand nodes with almost a quarter million, of course and over 1.5 petabyte-scale Fram
03:04:17 [W1] Process roughly 15 billion genotypes per hour as part of the pipeline responsible for deciding which seeds which are the final product should be Advanced for further experiments in their R&D Department.
03:04:20 [W1] Couple months later during joint presentation Twitter and Google together described how we approached validating that the applications that they are currently running on their messes Aurora clusters
03:04:35 [W1] Plated and run on g k clusters with 15,000 nodes.
03:04:37 [W1] But the goal of this presentation is to show that this work matters for all of you.
03:04:43 [W1] Even if your clusters are order or maybe even orders of magnitude smaller.
03:04:51 [W1] However, we need to start with understanding what scalability really means for kubernative.
03:04:57 [W1] Even though we tend to use node count or cluster size as a proxy for the overall scale.
03:05:03 [W1] It's actually much more complex than that.
03:05:07 [W1] scalability is a multi-dimensional problem with thousands of Dimensions such as number of services number of volumes pot turn and so on even though they in many use cases
03:05:22 [W1] Together with cluster size you need to acknowledge that not count is not the only thing that matters.
03:05:31 [W1] So, how did we approach scaling kubernative to the next level which 15,000 notes certainly is?
03:05:40 [W1] The core principle for any scalability or performance work is don't optimize blindly and always sort of real-life problems.
03:05:51 [W1] Almost every optimization is making a system a little bit more complex.
03:05:55 [W1] So it's super important to keep that in mind.
03:06:00 [W1] According to this rule. We first found users for whom running such a massive clusters would have actual benefits.
03:06:10 [W1] We started with understanding their use cases but also their motivations and as you may suspect those are the ones that I already mentioned.
03:06:20 [W1] The first one was buyer cropscience who is refusing kubernative to run embarrassingly parallel Isabel batch computations.
03:06:28 [W1] For them larger clusters immediately translate to having results faster.
03:06:34 [W1] In addition to that given that the users are generally data scientists who don't want to understand the underlying infrastructure.
03:06:43 [W1] They want to make it as simple as possible. Ideally a single cluster.
03:06:47 [W1] Migrate they microservices apps to kubernative.
03:06:48 [W1] As I already mentioned they are currently running them on my so Sarah clusters, which can handle even 40 to 60 thousand notes in a single cluster.
03:06:59 [W1] So while migrating to kubernative they would like to avoid the need to suddenly manage an order of magnitude more clusters.
03:07:08 [W1] In addition to that they would also like to unified a setup and run application that they are currently running differently like some stressful apps also and kubernative.
03:07:23 [W1] At this point you're probably thinking that you are wasting your time.
03:07:27 [W1] I'm saying that we focused on two users.
03:07:30 [W1] Doesn't that sound simple by the way, and your workloads probably aren't even similar to what they are running.
03:07:39 [W1] However, just like landing on the moon push the frontiers of Technology. It's the same with our scalability work.
03:07:47 [W1] The improvements we did not only push the scalability limits, but they are primarily making all clusters more reliable and more performant.
03:07:57 [W1] So now I'm going to describe a couple improvements to show how they make your life better.
03:08:05 [W1] Sting your time.
03:08:04 [W1] I'm saying that we focused on two users.
03:08:07 [W1] doesn't that sound simple by the way and your workloads probably aren't even similar to what they are running. However, just like landing on the moon push the frontiers of Technology.
03:08:19 [W1] It's the same with our scalability work.
03:08:24 [W1] The improvements we did not only push the scalability limits, but they are primarily making all clusters more reliable and more performant.
03:08:35 [W1] So now I'm going to describe a couple improvements to show how they make your life better.
03:08:43 [W1] So let's start with that CD the main activity Improvement that is worth mentioning is concurrent reads.
03:08:52 [W1] Before this change every read operation on that CD was blocking all other operations both read and write for the entire time of its processing.
03:09:03 [W1] Thanks to this Improvement.
03:09:05 [W1] We are now blocking other operations only for a very short period of time just to grab a copy on write pointer to the current state.
03:09:17 [W1] What is crucial for large clusters?
03:09:19 [W1] It's not the only place where it helps.
03:09:31 [W1] 20 notes
03:09:34 [W1] thanks to this Improvement. When you are listing custom resources, you will no longer observe spikes in the API code latencies, even if your customers small.
03:09:47 [W1] And it's already available in SCD 3.4.
03:09:51 [W1] So let's now make a step up and talk about a pi server and API machinery.
03:09:57 [W1] As you know watches The crucial part of our API, but no request runs forever.
03:10:04 [W1] So what happens when it times out, especially if it's a very selective watch that selects only a small fraction of all objects of a given type.
03:10:15 [W1] To resume the watch we are using resource version of the last object that we received yet.
03:10:22 [W1] But many other changes might have happened in the meantime.
03:10:27 [W1] And now we need to process all of them again.
03:10:31 [W1] simply because we didn't have a way to Signal the client that they have already been processed.
03:10:38 [W1] So introduced a concept of what bookmark it's basically a new event type.
03:10:46 [W1] That you can receive the awards that basically tells you we processed everything up to a resource version X. So if you didn't receive anything, it simply means nothing much your selector.
03:10:58 [W1] There's nothing specific to 15,000 not clusters. It helps for four clusters with thousand or even hundred nodes.
03:11:07 [W1] And it just happens out of the box because Q black that is watching its own thoughts is a perfect example of very selective watch that benefits a lot from this Improvement.
03:11:20 [W1] And it's already ga-in kubernative to 117.
03:11:25 [W1] However, improvements were needed across the whole stack. So let's take a look into an example from the networking area.
03:11:33 [W1] For those not familiar with endpoints API the endpoints object contains information about all back ends of a given service.
03:11:44 [W1] So it's size is proportional to the number of PODS behind that service.
03:11:50 [W1] That has many consequences. But let's think about kubeflow oxy which is an agent running on every single node in the cluster that is responsible for programming in cluster networking.
03:12:03 [W1] In order to do that. It's watching for changes of every single endpoints object in the cluster.
03:12:11 [W1] So imagine that you have a service with 5,000 Parts, the corresponding endpoints object will have optimistically around one megabyte.
03:12:21 [W1] Is that a pi server has to send 5 gigabytes of data for a single change of that object.
03:12:19 [W1] And for the rolling upgrade of that service, it would be 25 terabytes of data to mitigate that we introduced the concept of endpoints eyes.
03:12:31 [W1] That allows us to share the information about the endpoints of a given service into multiple endpoints likes objects.
03:12:40 [W1] Thanks to that we can significantly reduce the load on the control plane and amount of data that IP a server has to serialize and send.
03:12:50 [W1] the pieces of that solution when the batter in kubernative 119
03:12:56 [W1] So let's take a look into one more example this time from the storage area and let's talk about secrets and config Maps.
03:13:04 [W1] As you probably know whenever any of those changes cubelets are reflecting those changes to all ports that are mounting them.
03:13:11 [W1] But in order to do that, they are opening a watch for every single secret and conflict map that they pods are using
03:13:22 [W1] That might translate even to hundreds or hundreds of thousands of additional watches in the system.
03:13:29 [W1] Optimizing them would bring a lot of complexity to the API machinery. And as I mentioned before we should always be solving real life problems.
03:13:38 [W1] It appeared that they don't mutate majority of Secrets and config maps at all.
03:13:36 [W1] So we introduced the concept of immutability to secret and config map API when explicitly marked as immutable by the user the contents cannot be changed, but cubelets also don't need
03:13:51 [W1] Which is vastly reducing the load on the control plane.
03:13:38 [W1] As in the previous examples there is nothing specific to large clusters and you can take advantage of it. Even if your cluster is small.
03:13:47 [W1] And it's already better in kubernative 119.
03:13:52 [W1] However, we were also looking outside. Of course kubernative. We worked closely with goal and community on optimizing its memory locator.
03:14:03 [W1] It may be surprising to many of you but lock contention at the level of go memory allocator is actually one of the bottlenecks that we really suffer from.
03:14:14 [W1] While some optimizations have already landed in newer versions of go even more are coming.
03:14:21 [W1] And this benefits not just kubernative this benefits everyone who is writing their applications in go.
03:14:29 [W1] I described a couple improvements and all of them as well as tens or maybe even hundreds of others were done in Upstream kubernative.
03:14:37 [W1] However, that doesn't immediately mean that every kubernative distribution will scale to 15,000 note clusters in order to work.
03:14:49 [W1] Your ecosystem has to work at that scale to that includes the underlying infrastructure both computer and networking you need
03:14:59 [W1] Includes Auto scaling logging and monitoring control plane upgrades and many other things.
03:15:04 [W1] Based on GK experience. I can say that it's a huge effort to make all of them work.
03:15:11 [W1] So kubernative improvements are necessary, but they don't solve all the problems for you.
03:15:19 [W1] There's one more question that we should answer here, which is how do we know when we can stop?
03:15:27 [W1] Fortunately, the answer for this question is fairly simple. As soon as we meet our slos service level objectives. You can think about them as system level metrics with fresh holes.
03:15:41 [W1] While the concept they cover in kubernative are still fairly basic like API call latencies or pot startup time.
03:15:49 [W1] They greatly correlate with user experience.
03:15:51 [W1] So to summarize scalability work matters for almost everyone because scalability is much more than just the cluster size.
03:16:04 [W1] The improvements we did to push the scalability limits are also or maybe even primarily making smaller clusters more reliable and more performant.
03:16:16 [W1] So if your Caster didn't work because of some scalability or performance issues in the past.
03:16:21 [W1] It's probably time to re-evaluate it.
03:16:18 [W1] Unfortunately, we don't have more time. So I will just mention that with upcoming releases. You can expect even more improvements and extending the portfolio of use cases supported by clusters with fifteen thousand nodes and with that.
03:16:33 [W1] Much for staying with me.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you.
03:16:31 [W1] Thank you. I am always intrigued about the scalability issues and how kind of everything interacts especially because sink scalability often has to work Upstream in go to get things done and it and it mingles with
03:16:47 [W1] Upstream in go to get things done and it and it mingles with the with the work that we do in Sig release.
03:16:51 [W1] So thank you for that exciting presentation.
03:16:54 [W1] Next up. We have David sutiya the last keynote for the morning last but certainly not least David is a senior devops engineer at go spot check and today he'll talk about how go spot-check and has
03:17:09 [W1] Together buildpacks Helm opentelemetry Prometheus Envoy linkerd e and grpc to create a smoother experience for developers.
03:17:17 [W1] So you're going to get like the whole Cloud native ecosystem or a good portion of it.
03:17:22 [W1] So check it out.
03:17:34 [W1] Hi, this is more power less pain building an internal platform with cncf tools. And I am nervous. My name is Dave sutiya though. And this is actually a sequel to a talk. I gave last year called balancing power and pain.
03:17:49 [W1] Which I gave a Tony rib, and it was about moving a single application from a platform as a service over into kubernative.
03:17:54 [W1] He's and when we finish that talk a bunch of people came said oh, so you build a platform to deploy apps and we said no we know we moved a nap.
03:18:05 [W1] the platforms coming could come talk to us next year. So this is next year.
03:18:11 [W1] Who am I I'm a senior devops engineer. I build clusters and deployed databases and right utility applications and I helped with architecture and this year.
03:18:20 [W1] I've been leading the effort to make a more developed developer friendly platform and inside of our company and that was prompted by one of our lead developers at a hackathon last winter coming up and saying I don't really know what my projects going to be other than figuring out why this is all so hard
03:18:35 [W1] Here. I've been leading the effort to make a more developed developer friendly platform and inside of our company and that was prompted by one of our lead developers at a hack-a-thon last winter coming up and saying I don't really know what my projects going to be other than figuring out why this
03:18:56 [W1] Make it not so hard.
03:18:57 [W1] So that was a catalyst for saying okay we can operate now, but it's not pleasant. We need to work on the developer experience and make this better.
03:19:07 [W1] I work for a company called go spot check.
03:19:09 [W1] It's a field execution app where you have sort of had data collection missions that you fill out on a mobile application that go back for near real-time sort of business intelligence analysis.
03:19:20 [W1] We as just for context on our sighs. We've got about 40 engineers and QA folks and we started as a rails monolith and we've sort of branched out in the last couple of years to go microservices and node-based Cloud functions we
03:19:35 [W1] Have sort of had data collection missions that you fill out on a mobile application that go back for near real-time sort of business intelligence analysis.
03:19:43 [W1] We as just for context on our sighs. We've got about 40 engineers and QA folks and we started as a rails monolith and we've sort of branched out in the last couple of years to go microservices and node-based Cloud functions we
03:20:43 [W1] And couch and Kafka.
03:20:45 [W1] We have a scholar based data pipeline.
03:20:49 [W1] So lots of different Technologies, which is really why Cloud native made sense for us as we migrated out because you know, we believed it would end up supporting supporting everything that we did.
03:21:01 [W1] So bit of context on why we're building this platform why cncf tools?
03:21:06 [W1] Well, we started on Heroku platform as a service. We outgrew it and kudos to Heroku for enabling us to get too big for Heroku.
03:21:14 [W1] We love you guys.
03:21:14 [W1] We decided to move to kubernative future-proof ourselves and for a lot more detail on that, please watch last year's talk but
03:21:23 [W1] One of the reasons for moving to kubernative was the ecosystem.
03:21:26 [W1] We figure we're going to go to a place where we have a lot of surrounding tools that are going to fill the gaps of utility that we need.
03:21:35 [W1] Maybe not right now, but but soon or in the future as well as now and we figured that it would be a long-term bet to pay off in terms of interoperability and efficiency and I do think it is paying off.
03:21:48 [W1] but there was struggle going from you know a monolith on an all-in-one platform with one or two teams that were very close to lots of teams working on lots of services in a distributed environment with open source tooling has been difficult
03:22:04 [W1] We decided to move to kubernative future-proof ourselves and for a lot more detail on that, please watch last year's talk but.
03:22:11 [W1] One of the reasons for moving to kubernative was the ecosystem.
03:22:14 [W1] We figure we're going to go to a place where we have a lot of surrounding tools that are going to fill the gaps of utility that we need.
03:22:23 [W1] Maybe not right now, but but soon or in the future as well as now and we figured that it would be a long-term bet to pay off in terms of interoperability and efficiency and I do think it is paying off.
03:22:36 [W1] but there was struggle going from you know a monolith on an all-in-one platform with one or two teams that were very close to lots of teams working on lots of services in a distributed environment with open source tooling has been difficult
03:24:13 [W1] I could come up with is from my experience as a parent.
03:24:16 [W1] It is really felt like going from the Slick stroller system to a bunch of toddlers poking each other in the heart. There have been there have been growing pains. One of the biggest difficulties we've had was just resourcing were as pretty, you know,
03:24:31 [W1] Midsize company my team the operations team is 2.2 people the point to being my manager who tries really hard to be an icy, but has to manage and you know, we used to be the devops team now with the Ops Team because
03:24:46 [W1] To do now that we can't really do any of the dev stuff.
03:24:45 [W1] We had a platform team at one point is, you know being dispersed out into the feature teams.
03:24:51 [W1] And so there's actually no resources dedicated to building this platform.
03:24:55 [W1] It's all just been done with whatever we can sort of grab from people and get them to contribute.
03:25:02 [W1] And it's just been time.
03:25:03 [W1] It's taken us about two and a half years to figure out.
03:25:05 [W1] what is our platform even need to have much less. How will build it and how will you know what pieces we can put together to compose it and then just to do the iterative process of we moved in at now.
03:25:15 [W1] We've moved Lots.
03:25:16 [W1] What's missing?
03:25:17 [W1] do we need right and going through that sort of Discovery process?
03:25:21 [W1] And so there's been this pendulum from Simplicity to complexity and and then sort of back in and into the middle that I want to talk about when this was going to be a breakout session.
03:25:31 [W1] I had all the logos here is going to do a deep dive into the the stack that we created but the deeper message for me going through this is that the cncf has lots of options for every kind of tool.
03:25:42 [W1] There's probably more than one implementation and you can really build and compose a stack out of whatever you want that fits for your needs. So so less than paying attention to the choice.
03:25:50 [W1] We made you know, it's more that it's possible to do this composition of a platform now.
03:25:56 [W1] The ecosystem is moving so fast.
03:25:57 [W1] The tooling is being developed so fast that in six months, everything is going to be a hundred percent easier.
03:26:04 [W1] I'll get into my case study for that in a little bit.
03:26:06 [W1] But but I've been saying this over and over again and I feel like it's starting to change.
03:26:13 [W1] So we started the the pendulum swing of Simplicity and simplicity is great, but it has limits we were able to deploy really quickly.
03:26:21 [W1] Everything was sort of there for us, but we hit this point where we had maxed out the number of servers. The the postgres configuration was slowing us down and we needed to have more control and customization over our environment.
03:26:32 [W1] So we swung the pendulum all the way over to the other side of power and complexity.
03:26:38 [W1] We had the ability to do whatever we wanted, but you know,
03:26:42 [W1] Like the dog catching the car. What what do we actually want is is the question we end up having to answer and do we have the resources to do it?
03:26:51 [W1] All right going to that side we get into kubernative and we realize oh we need metrics. Okay.
03:26:59 [W1] Well great.
03:26:59 [W1] Prometheus is there for that and we need to be able to see what's happening between our systems well-distributed tracing is therefore that Jaeger is there.
03:27:05 [W1] Okay cool.
03:27:06 [W1] We got that covered. But then you get into the smaller nitty-gritty things of how do we actually secure this properly and
03:27:12 [W1] and hey, I would expect that if I change my environment variables the pods going to restart but that's not actually true.
03:27:18 [W1] How do we get Pottery starts when the config changes and there's just these and and and and it starts to feel overwhelming.
03:27:26 [W1] And and and and it starts to feel overwhelming.
03:27:31 [W1] And that's why I'm so excited that things are kind of swinging into this happy medium where the toolings there.
03:27:36 [W1] It takes less work to get the tools working together.
03:27:41 [W1] And I think critically the vendor support is catching up. I'm going to give you some some concrete examples of this.
03:27:48 [W1] Observability a couple years ago.
03:27:51 [W1] We were running our own Prometheus and Jaeger rail support for both was meager or non-existent.
03:27:56 [W1] So our observability infrastructure and up getting split between an APM solution in the cloud knative stuff.
03:28:00 [W1] Some of the cloud native stuff was behind a VPN.
03:28:03 [W1] Some of it wasn't most of our Engineers had ever been on a VPN before so that was confusing for them and the cloud native stuff was falling over occasionally or commonly because we didn't have dedicated eyes on it because we were doing 50 things trying to set all the
03:28:18 [W1] And and stuff up, so it wasn't a great experience now. We actually have a vendor who fully supports Cloud native stuff.
03:28:26 [W1] It's not an external metric that's going to get charged at five times the cost and they give us a Helm chart.
03:28:31 [W1] We deploy the helm chart.
03:28:33 [W1] It sends out Prometheus and opentelemetry we send our stuff there. It forwards it onto a central place where everyone can look at it and we've got great tooling around it.
03:28:42 [W1] It's been a fantastic experience. That's only really been possible in the last year year and a half.
03:28:48 [W1] Mesh in increases and other this was my case study for if you can wait six months. You should three years ago.
03:28:54 [W1] Adding because it was just going to do it for me, but it was still hard to deploy and it was really complex may be more complex than we needed. And and so it's grown to now most of these things like we use linkerd e and you can deploy a production configuration of linkerd e with a single CLI command
03:29:09 [W1] This is just astounding and it has made it so much easier to be in the space and to play with things and get things out quickly.
03:29:11 [W1] The standards are getting better.
03:29:14 [W1] And so interoperability is actually becoming a real possible thing. Right? We use Ambassador for our Gateway and it plays very nicely with linkerd e both of those play nicely with the open tracing and opentelemetry so we can get observability into all this
03:29:29 [W1] It's nice.
03:29:29 [W1] It's really nice.
03:29:31 [W1] Building and deploying.
03:29:33 [W1] This is probably the number one value add we've had this year for our develop that developer experience. I mentioned in that, you know, when that when that lead was talking about how hard everything was he was really referring to I got to do 17 a multi has and I gotta write this Docker file
03:29:48 [W1] Tain all of it and we've cut all that down by abstracting and out with standardized Helm charts that we've written for our Organization for Argo micro service or a rails service and now they just have to put in some values and deploy it it's getting
03:30:00 [W1] Just have to put in some values and deploy it it's getting much closer to being a pleasant platform for dr. Files were using buildpacks. Now no more dockerfile Strider maintain, ironically enough.
03:30:12 [W1] We're using Heroku is buildpacks Tak so as we migrate out the last long tail of our applications.
03:30:18 [W1] We don't even have to change the proc files its seamless to just get things out from where they were and deploy them into kubernative zits.
03:30:26 [W1] It's been real Pleasant and made things much faster.
03:30:29 [W1] And the thing I'm most excited about honestly is actually the marketplace that is being created.
03:30:34 [W1] I've actually gotten surprise from people when I say that we want to buy and not build and I think that's partially because a lot of the most vocal people in this environment are the big players. And so thanks to coupon for
03:30:49 [W1] Created I've actually gotten surprise from people when I say that we want to buy and not build and I think that's partially because a lot of the most vocal people in this environment are the big players. And so thanks to coupon for
03:31:14 [W1] It's like so to give this perspective but you know, there's a space between the five-person Oregon the 5,000 person org that where there's room.
03:31:23 [W1] We want to buy and not build we're too big for the platform as a service but I was talking to the VP of a Fortune 500 tech company and his team that is extending tekton is twice as big as my team, right they can build we need to buy because we can't be
03:31:38 [W1] A little bit about the process of how we built this platform.
03:31:44 [W1] The first most important thing that we did was we treated it like a product your internal devtools our product any platform you build inside is a product.
03:31:53 [W1] You have to treat it like one.
03:31:54 [W1] I think one of the biggest pitfalls of platform teams that I've seen is that they build things for themselves and platform teams tend to be composed of the wonky assist engineer's or some of them and you know, it ends up being a complex doctor or
03:32:08 [W1] Castration stack that is powered by make files written in Vim to do local development. And I'm one of those lucky people.
03:32:16 [W1] I like make I like them don't at me, but that's not what everyone consuming that platform wants to use.
03:32:23 [W1] And so I got I didn't get a product manager but I got part-time from a product manager for 2 months who helped me do interviews of our internal users of our engineer's and QA and support and we put together personas and
03:32:38 [W1] Then we were and then he kind of downloaded into my brain a bunch of product management, you know info and strategies and stuff that we've used over the rest of the year to help inform what we build.
03:32:50 [W1] The next thing we've done that's been really critical is sort of harnessing Conway's law.
03:32:55 [W1] Personas, we created from various people.
03:32:51 [W1] So, you know, we had a Persona of the junior engineer and the like the senior engineer that just wants to ship stuff and isn't really into configuration and getting their hands dirty and devops stuff.
03:33:01 [W1] Those people need to be represented on a group that's working on.
03:33:04 [W1] How do we do observability or how do we you know work on how do we improve our local development?
03:33:08 [W1] So those groups have defined most of the work to be done as an operations team. We stood back and one more the library for
03:33:17 [W1] Tools here are all the things we know about that will let you local deploy into the cluster, right and and then you guys can go research this and y'all can figure out what you want to use and then they're able to become.
03:33:33 [W1] And make the choices which are really impactful for people actually wanted to use that since we don't have anyone any dedicated team to actually implementing that team has a board. It generates we generate stories and then people pick up those stories as they can to
03:33:48 [W1] We'll pick up those stories as they can to write the script that will you know install all the tooling you need to interact with our servicemeshcon something.
03:33:55 [W1] So the final thing I want to talk about is just the promise of this ecosystem that I think is starting to become fulfilled.
03:34:04 [W1] Last year my favorite keynote was from Bryan Liles the when is it kubernative is going to have its rails moment and and he call that my favorite quote which is that kubernative is is necessarily complex.
03:34:15 [W1] I don't think we're quite at a rails moment. But as a relatively early adopter, I think we're getting a lot closer.
03:34:23 [W1] It's not exactly the stroller.
03:34:24 [W1] I showed earlier but but it's getting to be a little bit more like this necessarily complex squirrel feeder, you know, it does a lot of stuff that is
03:34:33 [W1] Important it we still had to put it together.
03:34:36 [W1] We had to compose it but it looks and feels a lot more professional and it's a lot more pleasant to use.
03:34:45 [W1] So my Mantra is changing its now if you can wait six months, you might actually be good to go. Right now. The the tooling is at a point in terms of maturity and interoperability where you can dive in as a midsize organ compose something that that is going to serve your needs
03:35:00 [W1] Off that is important it we still had to put it together.
03:35:05 [W1] We had to compose it but it looks and feels a lot more professional and it's a lot more pleasant to use.
03:35:13 [W1] So my Mantra is changing its now if you can wait six months, you might actually be good to go. Right now. The the tooling is at a point in terms of maturity and interoperability where you can dive in as a midsize organ compose something that that is going to serve your needs
03:36:02 [W1] And one of things I'm most excited about is seeing the actual platform as a service here every year because I really truly believe that if you are a five person or gue should not be in kubenetes. You should be on a platform as a service but now you can still go places be like
03:36:18 [W1] But now you can still go places be like we use containers and kubernative is because behind the scenes you actually are.
03:36:26 [W1] Thank you for watching.
03:36:27 [W1] I'm at the develop Nick and the regular places since I'm not about to step down from a platform Podium into the crowd of my peers and chat, which I really miss.
03:36:36 [W1] If you didn't get something out of this that you wish you did or you want more detail.
03:36:40 [W1] I would be overjoyed if you reached out to me.
03:36:41 [W1] I love talking about this stuff.
03:36:42 [W1] Thank you so much for watching.
03:36:55 [W1] Yes connecting with a crowd even when virtual thank you.
03:36:59 [W1] Thank you David So as we go into the close of day to Keynotes. I just wanted to give a few closing remarks.
03:37:07 [W1] So, you know at the end of the day event today, we're going to have a variety of interactive experiences for you to enjoy such as Marvel Cinematic Universe trivia computer and video game history trivia and
03:37:22 [W1] Virtual Escape rooms. So police kept check out the schedule and RSVP to participate. So as David had mentioned being able to connect with all of the attendees is super important.
03:37:35 [W1] So take some time for the next 15 minutes.
03:37:38 [W1] All of the keynote speakers are going to be hanging out in the keynote slack Channel.
03:37:43 [W1] There's been lots of chatter already.
03:37:45 [W1] So if you want to connect to anyone and chat a little bit more deeply about about what they've talked about today head to
03:37:51 [W1] to number pound to - Keep Calm Keynotes. So the breakout sessions will begin at 2:55 u.s.
03:38:04 [W1] Eastern.
03:38:04 [W1] So take a break hang out in slack and then come join us for for more.
03:38:10 [W1] Awesome Cube con Cloud knative Khan. Thank you all.
03:38:19 [W1] Our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because we're solving a complex problem.
03:39:29 [W1] Kubernative will be boring as soon as we can make it boring.
03:39:32 [W1] Maybe after I retire it's already boring.
03:39:36 [W1] So I think we're well on our journey towards kubernative being boring.
03:39:40 [W1] It'll only be boring when it's done.
03:39:42 [W1] It'll only be done when it's no longer relevant.
03:43:20 [W1] our job is to make kubernative easier and easier and easier to use either from an Ops point of view or developer point of view while acknowledging it is complex because
03:43:30 [W1] solving a complex problem
03:44:30 [W1] Kubernative will be boring as soon as we can make it boring.
03:44:34 [W1] Maybe after I retire it's already boring.
03:44:37 [W1] So I think we're well on our journey towards kubernative being boring.
03:44:41 [W1] It'll only be boring when it's done.
03:44:43 [W1] It'll only be done when it's no longer relevant.
