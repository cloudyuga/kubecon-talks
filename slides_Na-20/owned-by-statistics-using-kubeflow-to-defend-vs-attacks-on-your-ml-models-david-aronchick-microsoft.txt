Owned by Statistics: Using Kubeflow to Defend vs Attacks on Your ML Models: HNGL-5347 - events@cncf.io - Wednesday, November 18, 2020 5:47 PM - 35 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hi everyone.
00:00:01 [W] Thank you so much for tuning in.
00:00:02 [W] I'm David Ron check co-founder of kubeflow and program manager and the CTO organization inside Azure. And with Ian has our katas.
00:00:48 [W] Hi everyone.
00:00:50 [W] Thank you so much for tuning in.
00:00:51 [W] I'm David Ron check co-founder of kubeflow and program manager in the CTO organization inside Azure. And with Ian has our katas.
00:01:00 [W] I'll be talking about owned by statistics how you can attack machine learning models and how you can use kubeflow to defend.
00:01:07 [W] One thing I want to say is how much I love coupon every year and it's such an honor to be able to present even though it's virtual and I miss you all terribly.
00:01:17 [W] so with that, you know one thing that we talk a lot about in machine learning is how powerful everything is and how many advancements have occurred in the recent past specifically you can see some of those examples here
00:02:19 [W] To defend one thing I want to say is how much I love que con every year and it's such an honor to be able to present even though it's virtual and I miss you all terribly.
00:02:31 [W] so with that, you know one thing that we talk a lot about in machine learning is how powerful everything is and how many advancements have occurred in the recent past specifically you can see some of those examples here
00:02:50 [W] Have machine learning models for Microsoft really setting the bar too new and interesting advancements and and really handing it back to the community. You know machine learning is all about open source, and it's not just that we built on these open source
00:03:05 [W] Money is all about open source, and it's not just that we built on these open source projects externally, but then gave back after we were done.
00:03:13 [W] Now with that, you know, you wonder how this affects Microsoft and the reality is is affects us in just about every way possible it touches our consumer our Enterprise our game business, you know our search business. They all simply
00:03:28 [W] Tap the power that machine learning gives us to analyze enormous data sets.
00:03:32 [W] And when I say Laura's datasets, I mean stuff like you see here whether or not its people using office people asking questions of Cortana or the six point five trillion signals that we evaluate on a daily basis around security.
00:03:45 [W] We could not handle this without the things that machine learning provides.
00:03:51 [W] With that said oftentimes in the machine learning space. We tell you these great stories and then kind of like leave it up to you to you as an exercise to the reader to go and implement it and the reality is machine learning is hard and it's something that we definitely
00:04:06 [W] attention to
00:04:07 [W] and one of the reasons it is so hard is because of what you see here most of the time people focus on just building a model.
00:04:15 [W] But the reality is that building models only one step of a very complicated workflow and thinking about the pipeline's is how people will help get people to adopt things more.
00:04:27 [W] Now let's say you're a native scientist. And you say well, I don't really care about that.
00:04:31 [W] I'm good.
00:04:32 [W] The answer is yes you do and the reason is because of what you see here people build these fantastic models and then take forever to get them out to production.
00:04:42 [W] One way through that is with MLS mlperf machine learning operations derivative from devops and gitops.
00:04:50 [W] it brings data scientists and machine learning engineer's into the development process. So now they can execute this inner loop like you see here on the left iterating very very quickly.
00:05:01 [W] And then when the time comes they're able to expand and bring be brought into the normal application developer cycle where in this case there are able to now integrate their sophisticated.
00:05:12 [W] Models into the development and roll out procedures. So you may be asking yourself wasn't this supposed to be a talk about security and the answer is yes, and that is because mlperf
00:05:27 [W] Baseline for security and I'll talk to you about how mlperf provide you security by focusing on three different types of attacks.
00:05:30 [W] You may see the first is that will dive into is your attacker gets your MLM a tool to life.
00:05:38 [W] So we're going to start with a really basic example of a model here in this case.
00:05:42 [W] We'll be looking at what I'm calling a circle detector and a circle detector.
00:05:46 [W] You're going to pull in a lot of things in to ingest it. You're going to split both your
00:05:51 [W] Or production in your training data, then you're going to actually train on that data, which will be quite large and then you'll have some certain inference and point now in this case, we're going to start by collecting a whole bunch of examples of
00:06:06 [W] That into our training data and then out the other side. When I present a sample model here, except me a sample object and it's going to come back and say that is is it is in fact a circle.
00:06:07 [W] So look so far so good.
00:06:08 [W] looking great. Now if I present a square to it, it still says it's a circle and I have to wonder why what's going on here?
00:06:16 [W] Well, if I look at these two things one is green one is blue one is round one is square. How could these to be?
00:06:24 [W] Fused and the reality is is because I didn't give my exit model enough examples. In this case. I gave it something which has all the same pixels as a circle. But in fact, I didn't tell it what to look for. And so the model says what has the same
00:06:39 [W] Pixels is a circle.
00:06:33 [W] So it's a circle but obviously that is incorrect.
00:06:38 [W] Now surely Advanced models are better.
00:06:40 [W] We don't see this in the real world. But the reality is that's wrong too.
00:06:45 [W] Here's an example of a Husky vs. Wolf detector. And you know, it looks great.
00:06:50 [W] We were able to do it with only one mistake out of the entire group and you're looking pretty good.
00:06:56 [W] But when you turn it over and you look into the explainer what you see is that a lot of the data under the hood looks like in the Husky case. It looks like it's picking up.
00:07:08 [W] Pictures of the animal but in the wolf case, it looks like it's just picking up stuff around the edge and kind of you're like well what's going on here as we dive just a little bit deeper into it.
00:07:19 [W] You see that in fact, lots of the wolf was not included and what we really did was train a snow detector.
00:07:26 [W] Now, you say well this again, you know, maybe this is just for paper purposes No, in fact, you see this over and over again.
00:07:34 [W] We're in one case you have a ship that is actually looking.
00:07:38 [W] For water in another case you have trains that are actually looking for train tracks and then in the third case and one of my favorites you see a horse detector who is really looking for copyright notifications.
00:07:52 [W] Apparently a lot of horse images have copyright notifications.
00:07:55 [W] Now you say well this again, you know, maybe this is just for paper purposes No, in fact, you see this over and over again.
00:08:02 [W] We're in one case you have a ship that is actually looking for water in another case.
00:08:09 [W] You have trains that are actually looking for train tracks and then in the third case and one of my favorites you see a horse detector who is really looking for copyright notifications.
00:08:21 [W] Apparently a lot of horse images have copyright notifications.
00:08:25 [W] And that's just from accidental stuff. Now let's talk about when people are actually trying to attack you this case on the left hand side.
00:08:32 [W] We put pixels over a stop sign and it indicates.
00:08:35 [W] It's a speed limit sign and on the right. They they 3D printed a shell and made it think that it was a rifle.
00:08:42 [W] It gets even worse when you start thinking about military implications. In this case.
00:08:48 [W] We have a model that which is looking for airplanes on a Runway which is easily able to detect until you put a sticker on the back of the airplane and it is able to camouflage those airplanes as though they didn't exist and finally, you know thinking about what happens when you put
00:09:03 [W] Airplanes on a Runway which is easily able to detect until you put a sticker on the back of the airplane and it is able to camouflage those airplanes as though they didn't exist and finally, you know thinking about what happens when you put a layer of
00:09:18 [W] Over these individuals you're able to convince the the model that these are in fact completely different people.
00:09:25 [W] And you think that these might just be toy examples Until you realize that Amazon's face recognition system mismatched 28 Congress people with their mugshots indicating.
00:09:36 [W] They might be criminals and it's not just that it's also extremely hard to opt out so you can't even get out of these systems even if you want it to so are you terrified yet? The answer for this is to use MLA Ops and
00:09:51 [W] Do you start with ingestion where you have more education has and detecting for bad data use better evaluation metrics and different models.
00:09:59 [W] You also make sure that you have a red team in place and attack your own models first along with Rich alerting and monitoring but most importantly it's feeding the information you get back from your serving into your model so that you can quickly become.
00:10:15 [W] Overtime and continually iterate and improve and this is really only possible with a rich mlrs grad' line.
00:10:16 [W] So a pipeline you say tell me more.
00:10:20 [W] Well when it comes to a pipeline, what we're talking about is the ability to break down a complex series of microservices and wire them together in a way that makes sense. In this case.
00:10:32 [W] We will be talking about using something like ci/cd get up actions in Jenkins Loosely modular components that you can use from on cloud or on-premise and making sure that you're constantly measuring an updating and the only way to do that is
00:10:47 [W] There it is that you can run over and over again without human intervention.
00:10:48 [W] To drill in just a little bit deeper.
00:10:50 [W] Let's say you start with Jupiter and the upper left-hand corner there where you're automatically iterating. Once you're done you check that into GitHub. And then at that point you're going to connect it to your pipeline in this case.
00:11:02 [W] We're going to use kubeflow pipelines as our ci/cd selection of choice and when get up receives that it's going to begin that process and so first it's going to use Kale which is a open source tool that takes a jupyter notebook
00:11:18 [W] Up with your pipeline and you may kick off something like your feature engineering step which may call out to actually and external pipeline such as spark to run through all your large data processing.
00:11:24 [W] Once that's done.
00:11:25 [W] It's going to hand back to kubeflow pipelines and continue with the next step. In this case. It'll kick off your training step. And in this case, we're going to use kubeflow as a pipeline itself.
00:11:36 [W] And so it's going to call out kubeflow pipelines will to that training step execute and then hand back when that's done.
00:11:43 [W] It'll then go to hyperparameters sweep where again you're going to reach out to a kubeflow pipeline.
00:11:48 [W] And in this case, it's going to execute many hundreds of pipelines simultaneously to search across the entire graphql.
00:11:54 [W] Of hyperparameters and find the one that is best suited for your model.
00:11:58 [W] Once that's done.
00:11:59 [W] It's also going to hand back.
00:12:01 [W] We're going to package that and hand it off to serving and then from that point from serving it's going to be out there for inference purposes and be able to be used by your application.
00:12:13 [W] This all sits on top of meta data storage and infrastructure and you know rather than continuing on about this.
00:12:20 [W] Let me hand it off and let Ian has show a quick demo.
00:12:22 [W] Thanks David.
00:12:25 [W] So maybe to explain the dangers for ML mother's face when they're exposed publicly and stress the importance of having solid and mlops strategy and the composable pipeline on which you can iterate quickly.
00:12:35 [W] Respectively with data scientists in this demo will show the steps that data scientist takes from experimenting local inductor notebook to running a single pipeline generated from that notebook running hundreds of pipelines for hyper parameter tuning and then choosing the best model to serve
00:12:47 [W] That notebook running hundreds of pipelines for hyperparameters tuning then choosing the best model to serve in this process erectus nuptials or and mlperf data in circular produce ability and limits tracking. So, let's Dive Right In
00:12:51 [W] So let's pretend that we are data scientists and we want to start working on an exciting new problem. We open the kubeflow dashboard go to the notebooks Tab and starting you notebook server connect to it and get right to work the problem.
00:13:05 [W] we're working on today is the open vaccine cargo Challenge and we're trying to locate the weak spots of a messenger RNA structure to help create a stable vaccine.
00:13:12 [W] I don't know too much about biology where so it's okay.
00:13:15 [W] We're working for a while. And in this notebook. We have prepared all the necessary steps for petabyte-scale.
00:13:21 [W] The data pre-processing them and training a model now.
00:13:25 [W] We want to run this whole process as a reproducible mlperf by plane.
00:13:30 [W] Normally we would have to rewrite our code to use a specific pipeline DSL.
00:13:35 [W] However, Gail make the transition from notebook to pipeline very simple and let me show you how in the set of the notebook.
00:13:43 [W] I open the kill sidebar and implicate as you can see certain colors appear derived in sale sales for the same.
00:13:50 [W] Color are part of the same pipeline step. This information is stored in the notebooks metadata, which is prefilled in this case.
00:13:58 [W] Kale makes a transition from notebook to pipeline very simple and let me show you how in the sight of the notebook.
00:14:05 [W] I open the kill Side Bar and they Bri kale as you can see certain colors are preserved in cell cells with the same color are part of the same pipeline step. This information is stored in the notebooks metadata, which is prefilled in this case.
00:14:20 [W] So we just need to annotate or notebook cells using kills intuitive. UI, for example, we can declare what type of cell be sees how it's called.
00:14:28 [W] And what other cells depends on enough datadog weak link on compile and run and kale will person notebook create steps out of cells detect data dependencies take a snapshot with Rock and finally generate a pipeline which starts from the notebooks in
00:14:48 [W] Steps out of cells detect data dependencies take a snapshot with Rock and finally generate a pipeline which starts from The Notebook snapshot it state in submitted kubeflow pipeline.
00:14:59 [W] And as you can see, the notebook is transparently converted to a reputable mlperf I play and weather data scientist.
00:15:06 [W] You don't have to learn anything new about pipelines after killed submits the pipeline it provides us with a link to the kubeflow the sport which we can follow to see the progress of the pipeline run and every step of this pipeline that you see is
00:15:21 [W] And with steps input and output artifacts are tracked by MF adata. So for every Stitch can see logos and also mlperf data, which records the input and output artifacts of it's set in this case. They are
00:15:36 [W] Artifacts of each set in this case. They are a rock snapshot which we can actually follow and see the snapshot in the Rock UI.
00:15:42 [W] Now we have a reproducible mlperf lines in the rated automatically from our notebook.
00:15:47 [W] However, this Pipeline and retrace our model for a specific set of parameters like learning great or but sighs tweaking those parameters can result in a vastly improved model.
00:15:56 [W] So we want to explore more configurations. This procedure is called hyper parameter tuning and cable provides an intuitive UI for enabling to perform a parameter tunic with its three things first, when it defined what the hyper parameters are in this case or
00:16:11 [W] There are in cable provides us with a special cell called pipeline parameters to declare a her parameter or parameters for every parameter killing then we need some Metric together search The Matrix shows
00:16:26 [W] Actually performing better or worse with its new parameter configuration.
00:16:25 [W] So in this case, we use the validation in those As the metric guiding the hybrid parameter unit to declare our metric. We simply print it and declare the cell as a pipeline metric in K and finally when able executing in jail
00:16:40 [W] if I a result algorithm and Hyper parameter settings
00:16:41 [W] So for example the but size can be between 32 and 256 in increments of 32. In this case. The settings are pretty field so we can start the tuning right away.
00:16:51 [W] Kayla's, you can see provides a pretty intuitive form to do this configuration that you adriaen way after defining the parameters The Medic in the cell Saga Rhythm.
00:17:01 [W] We are ready to start our hybrid parameter tunic by pressing the button of the kale sidebar to perform the hyper under tuning kill.
00:17:09 [W] You just calculate a kubeflow component Caleb will start the captive experiment which will create a trial for each different set of hyper parameter values. We have implemented a seam so that each character aisle.
00:17:21 [W] result in a gfp pipeline run once we click the kill button here will use kappab to spin up many instances the pipeline creating the first demo but with different parameters and as you can see can make a law also presented with links to follow the progress of
00:17:36 [W] No, because I have a parameter tuning will take a while. We have prepared the results of this tuning beforehand.
00:17:39 [W] We can see the kind of experiment with Allah created writers of parent graph showing all the have a parameter configuration in their success metric an intuitive way.
00:17:46 [W] We can also find the best trial so far as we're able highlighted for us from a Cattle Trail. You can navigate the keep the pipeline run. It corresponds to by clicking the small pipe line button next to the trial.
00:17:56 [W] So once we click this button and go to the pipeline run notice how many steps have
00:18:02 [W] The filter recycle like on them. This icon means that these steps were cast is massively speed up. The execution time of a pipeline run caching is part by erectus Rock by taking snapshots at the end and start off with step.
00:18:15 [W] We can also investigate the windows navigate from a pipeline run to the captive experiment belongs to so we start from a notebook and created a pipeline. Then we run many pipelines in order to perform hyper parameter tuning and get the best model after running
00:18:30 [W] There's massively speed up. The execution time with pipeline run caching is part by erectus Rock by taking snapshots at the end and start off with step. We can also investigate when also navigate from a pipeline run to the captive experiment belongs to so
00:19:00 [W] A tuning from the best combination of hyperparameters for our model and now we need to restore and serve that motor we can find the best combination by going to the capital Y. It will be highlighted for us from The Trial. We can easily navigate to the corresponding pipeline run as we saw earlier by clicking
00:19:15 [W] We saw earlier by clicking the little pipe line pattern. Now. We are going to use the snapshotting pirate Rook restore the best version of our model in the notebook to do that. We'll go to the last step of our best pipeline run and locate the Rope snapped at URL, which is found under the visualization tab,
00:19:30 [W] URL and use it to restore a new notebook to the state of the pipeline run at that specific step.
00:19:38 [W] We copy the Kudo PRL good The Notebook you I paste it and restore it.
00:19:44 [W] Once the notebook is ready.
00:19:46 [W] is ready. We connect to it and as we will see Kayla recognizes that it is restored from a snapshot and use the pop-up to point us to the last area that was running our best model in saved in the model variable which we will print.
00:20:03 [W] To show that f is in memory.
00:20:04 [W] We want to take this model and served to do that.
00:20:09 [W] We will use the careless Decay which sparked by KF serving underneath the function for serving is called serve and needs and model has input. In this case. We also use a function procedure or Data before passing them to the predictor. Once we run serve kale. Will Snap Shot
00:20:24 [W] And use it to serve the model in the same computer green environment.
00:20:26 [W] Once this process completes.
00:20:28 [W] Let's make some prediction. We first Define some Json data as input and then send it to the server will cave serverless big the key of several objects rub the whole procedure of calling the models every with HTTP or grpc and we immediately get a result back.
00:20:44 [W] We can also bring the gifts are variably to get information about the model server and follow the link.
00:20:51 [W] To get to the model UI the model you a super useful for listing monitoring and the banker model service.
00:20:57 [W] You can see the state of the server. You can see metrics from Griffon.
00:21:00 [W] A' you can also see logs from the model serverless folds like the predictor of the Transformer which performs the processing and you can also see the configuration of the model server.
00:21:13 [W] So only known we saw in ml of spire workflow where we went from experimenting inside a notebook to generate her produce full pipeline that would perform hyper.
00:21:21 [W] I'm tuning by running the pipeline.
00:21:23 [W] We generate before multiple times with different parameters. And finally we used to rock snapshotting power to restore the best model and easily save it using kale and Cave serving.
00:21:35 [W] So thank you so much.
00:21:36 [W] You're honest.
00:21:37 [W] A very rich very sophisticated pipeline that includes things like hyperparameters sweets and automatic roll out to production.
00:21:35 [W] We touched on the first attack that mlperf end against let's get into the second in this case where your model or where your attacker tries to take your model.
00:21:46 [W] Now. This is a situation in which a malicious attacker is going to try and probe your model many many times to get an underlying example of what that model is actually doing their gold mayadata.
00:21:58 [W] Not be complete accuracy.
00:22:00 [W] But as long as they can get close they can start to use your model in really malicious ways.
00:22:05 [W] And these are really hard to defend against so having a great mlperf critical the two types of attacks.
00:22:13 [W] I'm going to walk down our a distillation attack where they're pulling things around object detection and other things like that and the second is model extraction, which is more focused on Transformers and language. So a distillation attack the first you start with a black box model.
00:22:28 [W] Like you have here on the left and I'm just going to be again to probe that using sample examples.
00:22:34 [W] I don't know what's inside that model, but I do have access to the API. So I'm going to start with a heart and that fails then I present to a pentagon and it passes and then I decided to pick a whole bunch of widely arrayed examples to probe against and
00:22:50 [W] Bunch of different ones and I begin to get a better look at it. And then I present a lot of models anyone want to guess. What kind of detector this is It's of course a Nina Simone detector.
00:23:02 [W] No, of course not it is a in fact a triangle detector and from this from all these many examples.
00:23:08 [W] I'm able to get a really good understanding of what passes and what doesn't for this particular Black Box model now with that I'm able to pull that out and recreate the underlying model using
00:23:19 [W] adjust those examples and now I can present that model to my audience and they never have to go back to that original author models author. Which obviously is a real pain now the issue here is what how accurate can I get
00:23:35 [W] What does it take to get to 99% accuracy and thanks to a number of researchers.
00:23:28 [W] They were able to find that the number of queries is actually very very small for both of these. They were able to reproduce the model in under five thousand total queries, which is about two queries a minute over two days.
00:23:40 [W] really not a lot.
00:23:44 [W] So now let's get into a model extraction attack where we're actually going to take that NLP model based on Transformers. And so on in this case Burt came out somewhat recently from Google it really transformed the way that people do language models.
00:23:59 [W] Introducing the brand new Transformer architecture at the time and most models that you'll see today including ones like on Azure are based on some derivation of this original Transformer architecture.
00:24:03 [W] So you have a lot of surface area for under attack here the way these work is you present a large Corpus of data to the model to train on and from that point. Then you ask a question of that underlying data and it will do its best
00:24:19 [W] This case how many instruments did Prince play that comes from the Corpus and in this case the answer is 27 and you can see how sophisticated it is. There are a number of different places in the text where it describes instruments the actual sentence where it says 27 instruments is split by
00:24:23 [W] Participle and then you have down below different types of instruments in classes of instruments.
00:24:25 [W] So these models are quite sophisticated and very impressive.
00:24:29 [W] In order to attack these models you have these researchers from Google who have done some really interesting stuff here.
00:24:35 [W] What they've done is they've looked at the underlying model and decided that simply by presenting either random words or words from the Corpus you're able to get a really accurate representation of what the models looking for this case.
00:24:49 [W] Here's what the random looks like just obviously random words from you know, any dictionary in the world or the Corpus itself or you if you know what the underlying Corpus is which in-toto
00:24:59 [W] This case comes from Wikipedia you're able to present more structured questions about it in this case words from the actual block of text or the Corpus that it was trained on and in each case. The model is doing its best to give you a response.
00:25:14 [W] No, but it's going to pull things out of that Corpus and be able to present it to the user as a potential answer now. Once you've done that you're able to start probing that model many many times and with just one tenth the total number of queries you're able
00:25:27 [W] Probing that model many many times and with just one tenth the total number of queries you're able to get to 72 percent accuracy according to the model and if you were able to do the exact same number of questions, you're able to get to
00:25:34 [W] Exact same number of questions you're able to get to 86 percent, which is really good and certainly the basis for doing a lot of work and it's much much less than the millions of dollars that it's often required to train that model in the first place.
00:25:47 [W] So do you use mlperf end you can do your best you can try and focus on endpoints and securing the API watermarking, but the real value here is going to be in the pipeline your ability to retrain to train for domains to fix things over
00:26:02 [W] That's going to be where the value is.
00:26:04 [W] So really you should treat your models curity like anything else where if you're exposing it to the world it will be attacked and what's most important is how quickly you can update and iterate and detect those tax and make changes quickly.
00:26:19 [W] In short hand, I would spend the majority of your engineering time on the left hand side and much less of it on the right hand side because on the left hand side.
00:26:28 [W] Those are the areas where you going to be able to make all your changes and on the right-hand side, you know at best you can stop people but you're not really adding a lot of user value.
00:26:36 [W] So that's why if your attacker gets your model is able to take your models.
00:26:41 [W] Those are two examples there.
00:26:42 [W] Now, let's talk about a third one around data leakage where the attacker finds out about hidden data in this case a militia.
00:26:49 [W] Yours is going to look for ways to attack your model to understand what it was trained on. You probably are already having this problem around data leakage and the problem here is it just becomes more obvious cated because of mlperf
00:27:18 [W] Or third, you know when you look at overall jogging and and race recommendations and things like that.
00:27:22 [W] They're revealing what you know, the actual layout of potentially confidential buildings and structures.
00:27:29 [W] And again, this wasn't even me or my friends. This was from the community that was able to develop this and roll it out.
00:27:37 [W] Now there's nothing so bad that it can't be made worse, especially with machine learning.
00:27:42 [W] So in this case what you see is where the the model is predicting what I should type next and so if I type a beginning of a sentence, it may move on and reveal other things that come from my Corpus of information.
00:27:57 [W] Basically, my private emails as suggestions for what I should write next.
00:28:00 [W] Now the problem here is of course if it's doing its job, right?
00:28:03 [W] It sounds great.
00:28:04 [W] It sounds exactly like me.
00:28:05 [W] The problem is that this is going to reveal a lot about what I write in other males.
00:28:10 [W] And of course this gets quite bad here.
00:28:14 [W] You have a number of different examples whether or not it's revealing the rest of my address or my phone number maybe it reveals my relationship information or when it starts to get really bad.
00:28:25 [W] You use things like your Visa card where it has a known prefix and suggest the rest or maybe in reveals the social security number where again, it comes from a known prefix and reveals the rest of it.
00:28:37 [W] So in this case the model which is totally obvious gated is revealing all this stuff without ever intending to because it's simply wants to sound like me.
00:28:47 [W] So there's some very cool ways to try and detect these things in this case. There's a way to use a canary
00:28:55 [W] Where you inject that Canary early on in your training data and then later detect for whether or not that Canary leaked through and you know, in fact did reveal information about that private Corpus. The problem here is that even in this case, all you're really doing is
00:29:10 [W] That is already occurring not preventing the leakage in the first place and it's up to you to go back and rerun your data Andre anonymize so that that data isn't being leaked through inappropriately.
00:29:15 [W] Now, there are things like differential privacy which may help over time, but I can't stress enough at the end of the day you're going to reveal things that are private because that's the intent for this thing to work properly.
00:29:28 [W] It should sound like the original user. It should be accurate. The problem is of course, you have to lock it.
00:29:34 [W] Down and you have to be able to detect it very very quickly.
00:29:37 [W] The key here is building a pipeline understanding your exposure quickly and mitigating quickly.
00:29:42 [W] So in summary mlperf you a lot of goodness best practices repeatable workflows and immutable record of what happened and real acceleration around getting to user benefits.
00:29:56 [W] Unfortunately doesn't give you this for free.
00:29:58 [W] There is some work involved but the reality is this is the best we have for making sure your systems are workloads.
00:30:04 [W] ingrate
00:30:07 [W] and the reality is is it's a whole new world data science will touch every individual and it's up to us the people watching this and the data scientist and the mln janeers to put these tools in the hands of people who can really use them to make the world a better place.
00:30:22 [W] Is an immutable record of what happened and real acceleration around getting to user benefits.
00:30:16 [W] Unfortunately, it doesn't give you this for free.
00:30:17 [W] There is some work involved but the reality is this is the best we have for making sure your systems are working great.
00:30:26 [W] And the reality is is it's a whole new world data science will touch every individual and it's up to us the people watching this and the data scientist and the mln janeers to put these tools in the hands of people who can really use them to make the world a better place.
00:31:04 [W] The truth is you can't avoid here. Your models will be attacked. Your pipelines will have issues and the game is all about Mitigation Of Harm's and quick recovery, and you can do that using an ml Ops pipeline.
00:31:16 [W] And with that Here's the final slide with all the papers that you may have any questions and thank you so much.
00:31:49 [W] So thank you all so much for the opportunity to talk.
00:31:53 [W] It's been fantastic.
00:31:55 [W] If you have any other questions, I've answered the ones that came through in the in the chat so far.
00:32:02 [W] Please let us know and feel free to send them on and I'll answer them live.
00:32:09 [W] Otherwise, I'll be in the cube con slack all afternoon and you're more than welcome to join.
00:32:18 [W] I'll drop that in the chat as well.
00:32:41 [W] Okay, there you go.
00:32:45 [W] Doesn't appear we have any other questions go there.
00:33:18 [W] So I have a question there about how do you convince a data scientist of the importance of mlrs, you know, it's actually not as hard as you would think generally speaking they want to move to production.
00:33:33 [W] Quickly and more often than not mlperf.
00:33:26 [W] The only way to do it, you know just like you would never have someone shell into your production environment and drop a you know, go binary or you know, jar file or something like that you would check in to get and then, you know run through some automated
00:33:41 [W] You're gonna have to do the same thing with mlrs, you know that data scientist wants to get their model out to production.
00:33:40 [W] And the way they do that is to you know, use Source control and use automated pipelines.
00:33:47 [W] And so that's really the essence of mlrs is giving you a reproducible way to go about that.
00:33:53 [W] Let's see.
00:33:56 [W] We got a couple more here.
00:33:57 [W] Thank you so much as well though.
00:34:02 [W] Any insights opinions about Alibi defect detect I'm actually not familiar with that specifically.
00:34:10 [W] So unfortunately, I'm going to have to punt on that.
00:34:12 [W] I assume it's some form of, you know, automated system for detecting, you know, errors or violations or things like that and I think a great mlperf Lawton have
00:34:28 [W] Components you saw there in the middle of my talk how to piece together a pipeline whether or not it's inside of kubeflow or externally.
00:34:35 [W] it's up to you. But the goal is to have it be very very flexible. So you can add many of them along the the chain. You never really want to just bet on one, you know, kind of Defense in depth
00:34:50 [W] what else?
00:35:17 [W] Thank you very much. Very kind.
00:35:21 [W] And the presentation I pasted the presentation in the chat.
00:35:23 [W] I also pasted over in the slack where you can jump over there. You just let us know.
00:35:48 [W] Okay, I think I'm out of time.
00:35:49 [W] Just had the 35 minutes.
00:35:47 [W] I'm please let us know if there's anything else you can do.
00:35:49 [W] do. Otherwise, you know, I'm around all afternoon in the slacking and just let me know what questions I can answer for you.
