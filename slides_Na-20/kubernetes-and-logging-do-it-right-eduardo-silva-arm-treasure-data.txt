Kubernetes and Logging: Do It Right: TEEE-4531 - events@cncf.io - Thursday, November 19, 2020 4:52 PM - 30 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello everyone.
00:00:01 [W] My name is a lot of silver and I'm here to talk about kubernative alluring.
00:00:05 [W] How doing it right or how to try to make it right?
00:00:09 [W] And B which is part of the flu in the ecosystem and it worked as a principal engineer at treasured big.
00:00:17 [W] Understanding the workflow in learning is really important if you understand how the data is collected how its processing and been tipping is more easy in general to how to troubleshoot and understand a
00:00:32 [W] Process your data. So let's take a quick overview on how this works in the kubevirt Medias world.
00:00:26 [W] If you think about a cluster is cluster has monster API, then you have all the workers which are called nodes and your applications are deployed deployed in the concept which is called pot and important have one or
00:00:41 [W] Notes and your applications are deployed deployed in the concept which is called pot and important have one or many containers.
00:00:42 [W] So when you deploy you put your also can decide that how many replicas even have for your own application and although sometimes replicas does not fit in just one know they are distributed across different nodes and these kind of a scan
00:00:58 [W] Sometimes replicas does not fit in just one know they are distributed across different nodes and these kind of a scan distributed environment is quite flexible.
00:01:08 [W] But also ETA rates in many challenges when we try to solve logging.
00:01:14 [W] So if you think of the about this diagram that we have how do we collect information from different parts and different notes and correlate with the iterator? It's a complex task.
00:01:26 [W] Imagine that one application just generate a simple message like this opportunity set where we have an IP address.
00:01:34 [W] We have a Time some and then the HTTP message.
00:01:37 [W] I mean the HTTP protocol this simple message generated by a pod needs to be processed and collected as a second application is in Arabia support about has a definition like an idml file right so
00:01:52 [W] In the left side, we have the definition for this spot and this spot is generating a random messages.
00:01:58 [W] That is fine. All these messages by default and go to the file system or in the maybe the kubernative environment its continued to be used with system.
00:02:08 [W] So it's a bit flexible where your data will be. But at the end we used to solve the same problem.
00:02:15 [W] This information when is generated by the pot? Then goes to a string can be the standard all put on some other error and the pain of the configuration if we think about file system all these data will go the file system to apart like
00:02:31 [W] This information when it's generated by the pot then goes to a string. It can be the standard all put on some other error and the pain of the configuration if we think about file system all these data will go the file system to apart like
00:02:54 [W] / Barbie or containers with the right foot named namespace and more information.
00:03:02 [W] Then this message is not just a text message generated by the application because we also had some metadata that comes with it.
00:03:11 [W] For example what this training that is status can be from another what time this data was generated.
00:03:17 [W] So you realize that the simpleness set for any kind of a node can start getting more data and more data, right? But after that we need to also to create other information. Maybe I would like to know why.
00:03:32 [W] Support me more which know this data was generated from a maybe when I deployed my pod.
00:03:39 [W] I said special labels to say.
00:03:42 [W] Hey, maybe this approach in environment label or this is I don't know label color equal to do because at the end when you centralize own your data in the storage, what you want to do is they analysis and to do the analysis
00:03:57 [W] Floyd me for I said special labels to say hey, maybe this approach and environment label or this is have not labeled color equal to do because at the end when you centralize all judaea in the
00:04:28 [W] Stupid was generated from different places.
00:04:31 [W] You need to have some of these special keys or not.
00:04:36 [W] I don't know what to say indexes that's taken place to look for them.
00:04:39 [W] I want to search for all my patterns that the job for example, he goes to linbit or labels equals blue.
00:04:47 [W] So how do we coordinate all this information?
00:04:50 [W] Because all this context of labels annotations in kubernative are not in the same node where this data was generated.
00:04:58 [W] later
00:05:00 [W] Bye-bye spec this day is in a master API server.
00:05:06 [W] And this is where we're going to introduce one big one of the agents for logging that we have in the ecosystem.
00:05:13 [W] They are others but we're going to talk about the fluid ecosystem where flowing bit it's part of it fluentd is a project that was started around two thousand 2015 originally for embedded links, but quickly evolved to the cloud space
00:05:28 [W] One of the agents for logging that we have in the ecosystem they are others but we're going to talk about the fluid ecosystem where flowing bit it's part of it fluentd is a project that was started around two thousand
00:05:56 [W] The Apache License pretty much like fluently.
00:06:00 [W] It's really impure C language in that decision because difficult years ago.
00:06:06 [W] We want his hat and real optimized Logan agent that was able to consume the less you possible and optimize memory usage as best as possible.
00:06:17 [W] also be very unsafe doesn't means that it's limited or restricted by the contrary. We have a plumbing leak detector where we have more than 60.
00:06:26 [W] Lightnings, I would say that maybe seventy as of today and we provide built-in security and different kind of rules. That is very flexible to be configured as a pipeline.
00:06:39 [W] A pipeline is just a concept that we have the from a management perspective right where when collecting data from one side from the Moon and parsing this data this year and then flux through filters buffering and finally is rotated back to the
00:06:54 [W] and this Nations can be many it can be any of the connections that we ship with the fluentd distribution can be for example elastic search Loki in loodse EB Amazon S3, and
00:07:09 [W] Distribution can be for example elastic search Loki influenza B Amazon S3 and their memories. Just somebody to take a look at their implementation.
00:07:14 [W] And it is a correlation is really important.
00:07:17 [W] So if we took this day that like we will have about a lot with a lot of message. We have a key way to stream and the time zone so a condom correlate all this together in kubernative because at the end our
00:07:33 [W] Like we will have a look with a love message.
00:07:35 [W] We have a key way to stream and the time zone so Fordham correlate all this together in kubernative.
00:07:43 [W] Because again, our data will become something like this right where our data will likely be processes.
00:07:49 [W] We're going to get from unstructured format to a structured format.
00:07:53 [W] This is just a Json for example, as an example. We get all the kubernative metadata where we can see that we have a pot name namespace for Ali labels and so on so correlating this information is really important
00:08:08 [W] It weighs you it would be really hard to try to find a the right information that you want at specific moment from a specific place.
00:08:19 [W] and how these can be deployed as an agent to Indian fluentd can be deployed as a site tekton cycle containers or as a team of six and they want safety support but runs on every note of your plaster so a
00:08:34 [W] The basic use case. We just deployed fluid there as a demon said in every note of your cluster, which of course it in the configuration monthly volumes with access to access the container looks that are in the
00:08:43 [W] so as I explained before the portworx writes to the file system through the kubernative city a of course and then we have the demon said consuming all the content information, but after read that information, we need to start creating also the
00:08:54 [W] To Damascus API servers and between all these labels and annotations and for see Sarah is an expensive task.
00:09:01 [W] I would say that it's not that easy.
00:09:03 [W] But at the end you as a user when you want to start shipping the agent deploy a basic configuration and make sure that this is working in a straightforward way and fluent in fluent, baby. That's that
00:09:19 [W] Once you correlate all the information, then you're ready for your next step which is cheap your data to your preferred database or storage engine recently the latest version we have access to apps knative support for possibly SQL.
00:09:34 [W] This is extremely lucky actually bloat elastic cuff Kata.
00:09:38 [W] We have many connectors pretty much everything that is uses in real production environments.
00:09:46 [W] And when you think about to invade you have to think about that flowing dates equals high performance at low cost.
00:09:55 [W] You can deploy any kind of tool to ship your data out of your box, but you always have to consider that as far as your data grows. The amount of data is more data.
00:10:08 [W] you need to process and when you process data that is not cheap, right?
00:10:13 [W] You have CPU Cycles.
00:10:15 [W] And if you run in the cloud environment where you have hundreds of nodes thousands of thoughts is really important that you're looking agent.
00:10:26 [W] be optimized and configured for the best use case possible low still consumption and low memory consumption without loss overall performance or throughput which is really important when
00:10:41 [W] Have in in many areas of engineering but apply to learning is a concept of a back pressure.
00:10:43 [W] I don't know if you remember but we just mention about that.
00:10:46 [W] We have added a pipeline right we have the input. We have the fielders parsec buffers and we sent it out.
00:10:53 [W] That sounds oh this makes sense.
00:10:55 [W] But one thing is theory and the others when you are things are deployed in production and what you have things production sometimes you
00:11:03 [W] You have neighbor Outreach you have a set pieces that doesn't respond to them quickly, but from the inside, but from the input side, you're sequencing data in more and more data.
00:11:17 [W] So, where do you store that data?
00:11:19 [W] What's usually happening with this and this web display the backpressure concepts with a couple of pipes and water.
00:11:26 [W] about that you have a very calming there in the left side and the going data in the right side.
00:11:33 [W] So your eye has a capacity right if you put more water needs.
00:11:39 [W] It won't go through faster.
00:11:41 [W] Sometimes this will start struggling a little bit right will start flowing but then we will get some problems because if we cannot process the data faster enough on the right side, and we're getting more data from the
00:11:57 [W] April we are going to have a real complex scenario. So how do we fix that?
00:11:52 [W] A microphone most of the in agents including filling the influent babe.
00:11:57 [W] We just store the data in memory by default because we operate with data in memory.
00:12:01 [W] But when we are sending this it out if we cannot ship fast enough and we're getting more data this data start accumulating in memory and in the communities or containerized environment you ended up
00:12:16 [W] Is it out if we cannot ship fast enough and we're getting more data this data start accumulating in memory and in the communities or a containerized environment you ended up saying, oh the colonel has killed
00:12:24 [W] The current has killed this container the logging agent and it is it makes sense because you were restoring data as much as possible in memory and that's why we need to offer some mechanisms to deal with this which is
00:12:40 [W] So in order to avoid back pressure is a login or at least in the kubernative serious cases.
00:12:46 [W] It's always two women this mechanism.
00:12:49 [W] For example, can the agent asked about K to ingest more data because if you think about the beginning of this presentation, we talked about that all our data.
00:13:01 [W] It's already in the file system and actually deliver will be there for a couple of minutes before the data gets rotated in most of cases.
00:13:09 [W] So it's ok that sometimes we can pause the vid ingestion and we can flush all the day are at least influence there.
00:13:17 [W] Yeah, that is possible and by the fourth for production environments, we always suggest enable the filesystem buffering mechanism file system means that your needle goes to memory, but it's always making a backup in the file system, but
00:13:32 [W] Put a limit of how much data flowing bit and use in the fluentd configuration whom it will save a you are ingesting so much data, and I can face back pressure.
00:13:42 [W] Okay, all the new data that is coming in will go as a secondary storage with a file system.
00:13:47 [W] So you always keep your main body control and you are going is back pressure is scenarios because just imagine that what would happen if you look your elastic is down on the other end and you're still with you.
00:14:00 [W] In there and you don't want to lose data, right?
00:14:03 [W] You have to enable all these a mechanisms and I will say that through the air flow may not approve of dealing with back pressure and we are pretty happy about that.
00:14:14 [W] We have a game really interesting feedback from different use cases. And I think that most of the default configuration would enable file system buffering is enough for in general use cases.
00:14:29 [W] No, pretty quickly.
00:14:30 [W] we're going to do a demo of how to deploy a fluent in and make a simple integration with the Lucky Strikes made by ravana.
00:14:41 [W] So we're going to switch it right now the camera.
00:14:45 [W] Looks into a low-key instance.
00:14:46 [W] So here my computer and running a minikube communities single cluster.
00:14:52 [W] And as I say, we're going to deploy the first pot which is just a dummy get sad and let me put that generate about your looks messages.
00:15:05 [W] So and we see it's already running so we can take a look at the Docks.
00:15:15 [W] And you will see that we are generating a simple message in her library that with the IP address time Stan and the other components of the HTTP request.
00:15:24 [W] That is fine.
00:15:25 [W] fine. But right now this is all running locally. So the next step in we're going to start up a low-key with Griffin and with whole stack.
00:15:38 [W] So we can access it and trying to connect the dots between the minikube instance and also refined.
00:15:50 [W] Since we are just starting we can start exploring our datum, but it should be almost ending.
00:15:58 [W] So on the mini we are getting back to the minikube going to the toilet fluent bed. The first steps are going to deploy the conflict map.
00:16:08 [W] Which has all the relevant configuration.
00:16:11 [W] We're going to take a look at it pretty quickly.
00:16:12 [W] In the really fun part here is how we are collecting.
00:16:18 [W] The loss. I mean how we are shipping the locks to the lucky in Center is running locally.
00:16:22 [W] So it's pretty simple.
00:16:24 [W] We have the outer kubernative labels on so everything will be fine.
00:16:29 [W] Okay. So now we deploy the demon set flub. It will be run as a team on set.
00:16:36 [W] So it will be able to take all the data from the note and then it just back that into a graph final.
00:16:45 [W] Going to refresh here. We can go to the job we're going to do a simple query is in the job fluent baby. Just show up.
00:16:55 [W] So now we can see in refinement that we have all these locks that are coming up from the main note. So if we click here you will be able to see this job and this is a lot of that is coming from
00:17:10 [W] Minikube the stream the X term and then seconds.
00:16:48 [W] Now we are going to continue with that next part of the presentation and we're going to talk more about a stream processing and other capabilities.
00:16:57 [W] Okay.
00:16:58 [W] So after this demo we're going to continue with the next part which is called string processing. I know you have heard this them a lot recently but not involving actually may be here are familiar with such a flame or Kafka.
00:17:13 [W] Maybe this concept is pretty familiar with you and stream processing as a concept is just ability to process the data and while the data system in motion means that a in general is cases, for example, you store your data and then you do an analysis, right?
00:17:28 [W] That's common pattern.
00:17:29 [W] But what about if when you get your data you create a window in memory and you process the data my chance before to shift the day out.
00:17:38 [W] I feel you can do many good things like distribute the data processing or gets into this new market, which is called H Computing or X processing.
00:17:47 [W] So this concept is really interesting and in general stream processing is paid by events flowmill works with events fluently works with events.
00:17:57 [W] Many things because take a look at this.
00:17:46 [W] This is mostly is a Json map.
00:17:49 [W] I'm not saying that this is just what we Jason this just a human-readable representation.
00:17:54 [W] We have the concept for thanks Stan temperature. Yes Ikea bodies. So what about if we can do analysis already the different keys and values that is flowing through the loading agent.
00:18:08 [W] right
00:18:10 [W] But also a if you think carefully you will say a my dear is not always equal doesn't have the same structure and that is fine because that is where be the first string processing capabilities that do not enforce
00:18:25 [W] Came up from you did right is it has a structure by this is Kimmel is so forget about the notion of like a database table where you have right the red columns and all that stuff here data quite Dynamic, but
00:18:33 [W] Extreme position is that you can process the data.
00:18:29 [W] It doesn't matter.
00:18:30 [W] what the skimmer is. As far as it has some key and value values you can do whatever you want.
00:18:38 [W] I will stream processing you can accomplish many things like first syllable sesang because you don't need tables indexing.
00:18:44 [W] Actually, you can process Data before to send it out to a database or a cloud provider.
00:18:49 [W] I'm not saying that the stream processing is a replacement I'm saying that stripped assessing is like a new way to optimize.
00:18:56 [W] How do we take a look at our data and gather their insights?
00:19:03 [W] So how this works in our case scenario?
00:19:06 [W] Usually this is the common model that we have in the market.
00:19:09 [W] You have the hardware software each one.
00:19:12 [W] They might generating events that I go through the pipeline.
00:19:15 [W] Then you have a central strip assessor the process data and then you send the data to the database or sometimes you process after the stretch in that race.
00:19:25 [W] But when you have this extreme processor, it's you have to think that if you're going to create some processing rules, you need to have a query language to do key selection filtering aggregation functions and so on but what's more important being
00:19:41 [W] In memory because if you are heading to this you are maybe you are not optimizing as much as you want, but every use case is different. I'm talking just about generic use cases.
00:19:52 [W] So if you think about that in the left side, we have the HTML kubernative note right on the right side.
00:20:00 [W] We have the cloud environment what we do usually in the every node model that we have is like explaining before we have the Logan agent on the edge process preprocessor loss send it out the string processor or the database
00:20:15 [W] We can offer all our interesting results there. But the proposal is that we do can do some belonging and extracts.
00:20:13 [W] We're about if we make the stream processing capability as one group more component of the logging item or stream processing on the edge.
00:20:23 [W] I'm taking this model when we are going to propose a as a pro anything.
00:20:29 [W] you're getting really good thing about feedback about this is move all the strength assessing capabilities to the same agent. So be able you can be able to operate all your data on the edge.
00:20:41 [W] You can do it on the edge, but this is quite optional right? So I'm the same processor is quite powerful.
00:20:50 [W] And now we're going to jump into what is new about fluent 81.6.
00:20:54 [W] And sorry before I have about that fourth string processing.
00:20:59 [W] We're going to have more polite socks on the fluentd community.
00:21:02 [W] We're going to share more details about that. And now being back to these fluentd 1.6 was released two weeks ago and have really exciting news about this.
00:21:12 [W] The first one is that what are the new impressed connectors and press connectors are a connectors that we create for Enterprise services.
00:21:19 [W] He's growing a lot.
00:21:16 [W] And I think that if you're looking at this conference the moment at this session is because you are interested more on these two.
00:21:26 [W] As part of filters a spot also of the string processing with Wolves think what how we can innovate on this what we can do differently how we can improve data processing and well-armed just contributed
00:21:41 [W] The to the point ends of flowmill rules on fluentd.
00:21:42 [W] This is quite powerful.
00:21:43 [W] This is not about training machine learning models about how to do all the inference and put a model when it is flowing and Trigger some action if the data is Flowing it's makes a match or not
00:21:58 [W] Voice so there are many use cases. Are we going to explore in the future with Michelle Learning Without give to intensive in like for example trim. This is just about deployment models and the last month.
00:22:08 [W] Without him to intensive in like for example trim. This is just about deploying models.
00:22:13 [W] And the last month we are hearing more than a hundred and seventy million deployments as of this year and this year has not finished yet.
00:22:23 [W] So this is a really important to see how a distraction I'm not saying that in general as a community.
00:22:30 [W] We have a hundred and seventy million users.
00:22:32 [W] But of course, this is a fraction of unique environments that are continuously using fluentd but also growing in the number of notes that discuss
00:22:43 [W] It has so we are keeping that Greg of options since the great Style.
00:22:48 [W] And as an integrated solution also, we are really happy to have all these three the top three providers using and contribute back to fluentd and I'm thinking about Amazon Google cloud and Microsoft also digital ocean.
00:23:03 [W] Sin, and contribute back to fluentd and it's in about Amazon Google cloud and Microsoft also digital ocean is being supported by four made a new one of the new offer for applications.
00:23:17 [W] and every service is using linbit behind the scenes, but also many customers of these companies that you are seeing here are using fluentd. So in general the Synergy between as a community of end users companies and Developers,
00:23:32 [W] Is creating a great value for everybody?
00:23:38 [W] Also firm that is its name in the little by opentelemetry as a pretty maintainers. We're talking with continuous conversation with the opentelemetry to see how fluid can fit on this
00:23:54 [W] Ability new space opentelemetry is being very good right now at Matrix and traces but the missing part is still lost and that's where we are working together to try to bring the a nice Solution.
00:24:04 [W] most of the problem asada for observability agent perspective
00:24:08 [W] Well, the thing presentation has just finished but now we have so many years for question in so we would be pretty happy. If you can mix questions tell us about the elusive fluentd or not. We have the chatter bailable
00:24:23 [W] Make sure to write your question would be really happy to answer old.
00:24:26 [W] All the questions. Well, thanks so much for coming and I hope you enjoy the conference by.
00:24:37 [W] Hello, everyone.
00:24:42 [W] Thanks for your time attending this talk.
00:24:44 [W] I know that we have almost 600 people.
00:24:46 [W] So thanks for joining a if you have any question, please continue asking in the in this chat.
00:24:52 [W] I have been trying to answer most of the questions and we need to talk but also at the same time we have a the meat and the maintainer session for fluid employee.
00:25:01 [W] So just after we finish these quick you a you can jump into the North Station if you want to have a more one-on-one interaction.
00:25:10 [W] Many questioning is a slide deck available for download.
00:25:13 [W] Yeah, it will be available available on scared. I think after this a session finish. Okay.
00:25:21 [W] One question is another one confluent bit take and listen for syslog messages.
00:25:26 [W] Yes, you can listen for the series of messages over TCP UDP Unix socket without any problem.
00:25:32 [W] That's a really common use case. We also support syslog in the output part 2
00:25:39 [W] hopefully use fluent bit as a site curve to a pot. What are the advantage a I think that this is not a matter of Advantage.
00:25:46 [W] Any kind of application that it refers to write the locks to the file system for you is better to have a cycle container with fluent bit processing the looks locally from that specific airport because Normal deployments are kind of
00:25:58 [W] Nation are streaming out the looks through the standard output and standard error interfaces.
00:25:48 [W] So it's a matter for use case. There's no perforated way. It just might have what you want to solve.
00:25:55 [W] What is the main difference between fluent bait and Splunk Splunk is a database and alligator fluent bit is a looks forwarder and process of so, you can use also Splunk for weather, but Splunk forwarded does not
00:26:11 [W] The data out.
00:26:10 [W] So you ended up in just in both data and paying for all that data once with fluent bit and 22. You can some filter some data out.
00:26:18 [W] out. So if you're just going to process 50% of your data, just ingest that 50% and not old next question what happens to the log events when fluent with bras processing get them with stutter or down for a while?
00:26:36 [W] So if the pot get restarted and the locks will be persistent for amount of time so fluently telepresence or will continue processing those look as far as those files are available in the system or through systemd Independence of the interface.
00:26:53 [W] Do you think that writing looks to a database like elastic search and then trying to perform analysis on the database instance of Performing?
00:27:01 [W] Oh and just put many questions in the Stream is an anti-pattern if you think a it depends of II think that there's no right pattern anti-pattern they think is if you have for example you care about real-time responses and your processing,
00:27:16 [W] Information from a thousand of pots arrogate that information index of information will take some time.
00:27:17 [W] But if you want truck for example anomaly detection you want to trap a duplicated credit card transactions.
00:27:24 [W] It's better to do it on the edge, right because you will get a faster response for that.
00:27:29 [W] Right? So it's a matter of a game as a use case not trying to say this is better than the other.
00:27:38 [W] Okay, let's go with a different question any strategy for detecting and assign a lot of levels to instructor locks.
00:27:47 [W] Just you can use a bridge expression with our filters and empowered that with also a Lua script.
00:27:55 [W] Industrial and bit 1.6 supports multi line parsing.
00:27:59 [W] Yes, it doesn't but it doesn't support it. If the line is recognized the Json message like not open container.
00:28:07 [W] We have a solution for fluently, but we are working the solution for fluent bit. So it's not yet available.
00:28:14 [W] Kung Fu linbit be used in another container environment like a BM / mirror.
00:28:19 [W] Yes, we have thousands of deployments every day coming from a Lambda S 4 Google Cloud.
00:28:31 [W] Kubernative he's or the case one.
00:28:33 [W] It depends if you use case for example for a kubernative person th can be a kubernative snowed for am better person could be, you know an embedded device
00:28:39 [W] Embedded devices and better lose devices and Cloud environments.
00:28:33 [W] So time for one more question before we close the QA.
00:28:37 [W] Let's pick one from the illness one. Okay, any good pointers to different architectures?
00:28:46 [W] For example log into files through cost path then just flew in the air very quickly on socket.
00:28:54 [W] See there are in I would say that
00:28:57 [W] It depends. It depends they are many use cases some a things prefers throughput right and others prefer pre-processing. So if you want to have more throughput you try to avoid
00:29:32 [W] So it's a bit generic, but we can follow up that letter if you want on slack or either in the next a session.
00:29:40 [W] That's linbit.
00:29:41 [W] Take care of Love rotation. No fluid does not rule of rotation. It handled of rotation.
00:29:46 [W] There are third-party tool for love rotation kubernative is and all environments.
00:29:50 [W] Later, if you want on slack or either in the next a session dust flowing bit take care of Love rotation.
00:29:54 [W] No fluid does not do love rotation it handle of rotation.
00:29:57 [W] There are three party tool for love rotation kubernative and all environments.
00:30:01 [W] Next question is impossible to feed data into datadog using fluentd follow-up.
00:30:07 [W] Can we use fluentd along with their above Sera agent datadog actually contributes to
00:30:38 [W] Thank you.
