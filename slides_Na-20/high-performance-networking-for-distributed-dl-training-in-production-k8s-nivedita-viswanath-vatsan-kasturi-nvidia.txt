High Performance Networking for Distributed DL Training in Production K8s: YTZH-0034 - events@cncf.io - Wednesday, November 18, 2020 3:02 PM - 40 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello there. My name is Watson custody.
00:00:02 [W] I'm going to be giving this Cube contact talk along with Nevada tarnation up on high-performance networking for distributed Dell training in production k8s.
00:00:13 [W] Hitting 800 GPU cluster within hundred digits over Rocky protocol.
00:00:19 [W] using ethernet Networks
00:00:22 [W] I will cover the first section and the visitor will go into the second section concluding with performance numbers.
00:00:33 [W] So the agenda is tekton is Bradley split into two sections.
00:00:38 [W] The top level is a gray colored boxes which indicate control plane operations and set up which we have to do.
00:00:46 [W] The bottom level side is the green boxes indicate the Network's the speech in the fields.
00:00:53 [W] How do we integrate our dmas rlb? And then the performance numbers coming out of this exercise?
00:01:01 [W] obviously the stock has to be made out first with a computer environment and then we have to launch the DL applications as a container in the k8s environment and collect the metrics coming out of the Telemetry to monitor
00:01:16 [W] Performing along with fairness multi-user multiplexed environment so that the users can get a fair share of the compute.
00:01:11 [W] The multi-node Datacenter consideration has to be involved here for the design of the cluster itself.
00:01:19 [W] And depend depends on the heating and cooling the rock layer along with the finite Works, which are listed here.
00:01:26 [W] What we call as a East-West knocks out ipmi storage Intel net Network.
00:01:32 [W] The EDR hdin grpc IR bandwidth related media is hundred K HDR is 200 Big on ndr phone will be links. And how do they actually communicate GT2 GPU in the East-West Direction
00:01:47 [W] Integrate the nodes RDMA. Sorry Jenna into the community structure.
00:01:42 [W] And then we'll conclude with production here. It has experience and also the performance in the second half.
00:01:51 [W] I'm not going to go into the details of this listing here, but you can see they're fine networks is figure there are speeds and free discussion another design consideration in the data center.
00:02:06 [W] And maybe it'll cover the production kubernative faster multi-node workloads.
00:02:12 [W] And also cni-genie flow flow.
00:02:16 [W] So what is the multi node cluster and meant k8s mnk k8s is a hundred note D GX 1 be 100 cluster connected by the ethernet links for GP GP communication over Rocky V 2 protocol use our dma
00:02:31 [W] with the data
00:02:25 [W] and cluster set up such that we can provide Hands-On and scaling and DJs.
00:02:30 [W] In the data center subject to the condition of Max Pace Power Cooling and Heating.
00:02:37 [W] Garda station is done by the k8s controller while the jobs are actually timeshare. As I said over here control volume across the macstadium layer for fairness of the GPU as well as the time being used on the GPU
00:02:52 [W] Please other users running the amount jobs.
00:02:45 [W] Could be a python object tensor flow inside the container or their eggs getting launched by the k8s in these worker nodes.
00:02:53 [W] So when you set up a cluster like this, there is an expectation of kpi and here is what we have experienced and what you have considered as a good K Pi where I am alone clusters.
00:03:07 [W] We need a design low-latent mnk clusters and it's one of network to keep the cluster.
00:03:15 [W] Zhang simple and what we also observe is instead of being 16 GPU on a one node.
00:03:23 [W] We can see Donna 1400 GQ.
00:03:25 [W] We have a Improvement of for factor of 70 x and this is specifically on mlperf performance.
00:03:33 [W] Pastor model convergence for the data sent in means faster performance numbers coming down into increasing productivity.
00:03:43 [W] and also converge in fasting the accurate States when you have a billion parameters to operate with
00:03:50 [W] The network needs to be Max the multi node cluster GPU targets, so they don't become a bottleneck.
00:03:56 [W] On the floor at the air flow will come out and push it out the right side pictures on the back side.
00:03:57 [W] We call it a power side with the heat will come out in the need the data center.
00:04:04 [W] They're basically six switches here. Each of them are in 28 ports on a gig switches.
00:04:09 [W] And therefore Rings we'll talk about in this in subsequent slides into other more used for leaves to go up the knots are traffic.
00:04:22 [W] The terminology is used in this Tech talk.
00:04:25 [W] So I have listed them here.
00:04:26 [W] not going to go through them, but you can refer them at their terminologies, which you don't understand.
00:04:36 [W] And multi node cluster.
00:04:38 [W] These are the building blocks moving from left to right. There is a data center is power cooling racks. Are you do the placement of the djx in sepia notes?
00:04:47 [W] And this is typically done by the side-ops and the networking team and then there is perimeter Network.
00:04:52 [W] And then the pipe network is right here, which has a k8s bgp layout in the switching along with the villain and the lower and you do have monitoring metrics and logging associated with both the
00:05:08 [W] you jobs
00:05:04 [W] this particular block is where the users actually get to see and interact extensively using CLI gooey and launch the container jobs in this particular infrastructure. The k8s actually controls the worker nodes
00:05:19 [W] The a server obviously associated with this.
00:05:08 [W] This particular picture talks about five different networks, which are the component of the traffic inside the cluster. There is ipmi control The BMC and the host of this k8s control traffic do I'm not South
00:05:22 [W] and the content API servers can be located anywhere this particular green thing GPU GP network is a short Span in plaster band with Harry.
00:05:32 [W] This is where all the ndre Dr. HDR traffic actually flows.
00:05:37 [W] They're very sensitive to our Tiki between the nodes.
00:05:42 [W] You want them to be non-blocking and ensure that there's performance in this particular Network?
00:05:48 [W] And that needs to interact in storage Network.
00:05:50 [W] You can be in the length of the spine to fetch the data and also write the results and the logs.
00:05:58 [W] And obviously the data center needs access to the internet.
00:06:02 [W] For pulling images or data sets for other purposes.
00:06:10 [W] This elliptical orbit has ten blocks, which are listed here each block represent the rack and each rack as like candy gxs.
00:06:19 [W] They all go into the inside ring.
00:06:22 [W] Each of the Ring connects to one of the links in each of the DG axis and they form a ring and so each node is only 1/2 hour away.
00:06:31 [W] There is Uplink associated with this which is about six point four targets the need to go or not South to go to the storage of other needs, but when the gpus are communicating with each other they only one half of it with low-latent
00:06:46 [W] There is a blink associated with this which is about six point four topics. They need to go are not solvable to go to the storage of other needs, but when the gpus are communicating with each other they only one half of it with low-latent
00:07:21 [W] Many of you are aware of this correspondingly model.
00:07:23 [W] This is basically the car switching for knots out on the hundred nodes are stuck here in the racks with six next four of them used for the East-West into them for the knocks out.
00:07:37 [W] And as I indicated earlier gpgpu traffic is significant and you want those to be high performance blue latent.
00:07:48 [W] Network
00:07:52 [W] This is the back side of one of the units, which is called a 100.
00:07:56 [W] I'm just giving you an example of how these Rascals connect therefore here on each side of this box in the DJ x 1.
00:08:06 [W] there's going to be four links and they 100 to be 800 links.
00:08:12 [W] The storage network will move knots out to these particular links in the traditional IP and network is this today?
00:08:26 [W] This is a layered 7 stackrox.
00:08:28 [W] Most of you are familiar with and this talks about the infiniband in the Rocky. And as you can see only the lower layers are a little bit different from the canal for onwards.
00:08:40 [W] It's identical and one can use either Rocky internet and infiniband and you can reconfigure the next.
00:08:49 [W] firm canonical Networks
00:08:53 [W] This is a little more expanded version of the same picture.
00:08:57 [W] But with the with a stack here going through the unity and the version 2 and then infiniband Network layer and the version 1.
00:09:12 [W] I discussed this particular slide earlier about EDR HDR MDR hundred gay 200 and 400 me. And how do they relate to the PCI bus do they saturate the PC across as you can see as we move up
00:09:27 [W] Line rate increases they want to saturate. This PCI bus 3. Oh
00:09:13 [W] and you have to move up to the photo Network.
00:09:17 [W] So this is a good slide actually relate what the nodes can do in terms of PCI standards associated with and make bandwidth and what the expectation the gpus are when you operate the cluster.
00:09:36 [W] This is another view of the network when seeing Inside the Box.
00:09:42 [W] There's a motherboard and lots of memory and the kublr in the containers runs here.
00:09:46 [W] There is a not so traffic going through this particular neck and the kubernative is actually operates like along with the Matrix in collected here.
00:09:58 [W] This particular complex is actually the East-West complex where the switching happens you can use IV or not. We associated with the GPU for each notes.
00:10:08 [W] Nyada, except like this 400 knots
00:10:11 [W] in the plaster
00:10:19 [W] So what is the design considerations for multi node cluster?
00:10:23 [W] and collected weird
00:10:18 [W] This particular complex is actually the East-West complex where the switching happens you can use. I be your rock. We associated with the GPU for each notes.
00:10:28 [W] Neo Dex up like this 400 knots
00:10:32 [W] in the cluster
00:10:39 [W] So whether the design considerations for multi node cluster.
00:10:45 [W] So aligned with ethernet switching model interoperability and ubiquitous ethernet network is the vehicle where you can install this.
00:10:54 [W] All right placement logic fundamentally easy NP and others to layer routing will have some hiccups.
00:11:03 [W] So we prefer to do 1/2 L to network.
00:11:07 [W] And so we don't have complexity while you're operating the cluster.
00:11:13 [W] This particular one is a hundred node cluster, but you can even allow longer cables.
00:11:18 [W] You can actually expand that to more racks and as I've indicated there in one of the pictures it's a 16 rock structure, but we can expand it a 2550 as long as there is switching structure.
00:11:33 [W] And so we don't have complexity while you're operating the cluster.
00:11:39 [W] This particular one is a hundred node cluster, but you can even no longer cables.
00:11:44 [W] You can actually expand that to more racks and as I've indicated there in one of the pictures it's a 16 rock structure, but we can expand it a 2550 as long as there is switching structure.
00:12:05 [W] Align and leverage and CCL which is a nickel a structure the libraries to avoid touching congestion and output ports.
00:12:13 [W] Michael does do tree rings and your garbage rates and ensure that the plastic flow smoothly.
00:12:21 [W] While you're actually bringing up the trusted.
00:12:25 [W] Avoid generic storage sack mixing for the RDMA Nicole going so that we have a clean Network on the east west.
00:12:35 [W] And that actually integrates with Sr. Sr IVC and I and body main differences are actually exposed inside the container.
00:12:46 [W] This is a traditional switching structure, which you most of you are familiar with so it's fine leaffilter. The storage unit decide result very close to the leaf if you want to have a local traffic or
00:13:01 [W] spine
00:13:03 [W] and the IBM at all actually connected to the BMC the switches to managed.
00:13:14 [W] So I'm going to pass here and find this afternoon a visitor you talk about the control plane and mayadata conclude the performance.
00:13:26 [W] Thank you.
00:13:27 [W] Thank you.
00:13:27 [W] But hello everyone, I'm Nevada and I'm glad to be co-presenting this session.
00:13:35 [W] Now what some talked about the best practices and network protocols to be considered while designing a high-performance multi node cluster data center.
00:13:46 [W] I would like to talk about how we leverage those design decisions to enable high speed networking in kubernative.
00:13:55 [W] We have an on-prem production kubernative cluster.
00:13:58 [W] There is mainly used by our internal users. And most of these internal users are research scientists that are running deep learning applications that often require hundreds of gpus.
00:14:11 [W] We have enabled support for running multi-node workloads through a custom job controller the Upstream MPI operator and a custom badge schedule.
00:14:23 [W] A pi server users can submit multi-node jobs through the CID either through the CLI or UI and when a multi node job is submitted the custom job controller watches for these
00:14:26 [W] Creates an MPI job for the multi no job the MPI operator then creates the ports for the MPI job and these pots form a gang which is then scheduled by the custom bad scheduler
00:14:31 [W] container image has the necessary libraries for the framework MPI and nickel and when all the pods are running as a gank, they communicate through nickel the production cluster
00:14:45 [W] Is shared cluster as Watson mentioned it's made up of 100 G GX 1 nodes.
00:14:48 [W] And since this is a shared cluster we have implemented features like gang scheduling starvation handling backfilling as well as support for user Cora's drf and dynamic job priority
00:15:03 [W] Starvation handling backfilling as well as support for user quotas drf and dynamic job priority through our custom schedule.
00:15:11 [W] We have a logging and monitoring pipeline that feeds into dashboards and alerts and helps with day-to-day operations.
00:15:23 [W] Now in order to run distributed deep learning applications high speed networking is extremely crucial.
00:15:30 [W] But before we go into how we enable high speed networking, let's take a high-level. Look at the topology of a GX 1 node. Each djx one has 8 V 100 gpus that
00:15:46 [W] It by Envy Link in a cluster.
00:15:48 [W] It has for mellanox hundred gig Nick's for our DME and dual Copper 10 gig ethernet evenings.
00:15:58 [W] The for RDMA interfaces form a rocky Network or a ring that is used by nickel for gpgpu communication using the one hop VLAN switching fabric nickel can detect the
00:16:13 [W] Between gpus both within and across nodes and the nickel ring formation also helps with output Port congestion and the cluster.
00:16:22 [W] Now this process is described in detail in the link that is shared on the slide the Dual ethernet.
00:16:31 [W] Nick's on the DG x 1 node are primarily used for storage and kubenetes control traffic.
00:16:37 [W] So this gives us two distinct networks one for Rocky and one for storage or control resulting in an isolated environment for performance Centric workloads.
00:16:52 [W] To enable high speed networking for multi-node jobs.
00:16:56 [W] We need to make the multiple interfaces on the host available inside the kubernative spot.
00:17:02 [W] This is achieved through a combination of the Sr.
00:17:06 [W] Iove vice plug-in and multi cni-genie.
00:17:09 [W] The SR ioved device plug-in discovers the Rema interfaces on the host and registers them with Cube. Let the RDMA interfaces can then be managed by kubelet as allocatable resources.
00:17:24 [W] So once a pod that is requesting these idea me interfaces in its resource pack get scheduled on to an or kublr it starts moving the network resources from the host name space to the pot namespace through the registered
00:17:39 [W] Now since the DG x 1 node has multiple types of network interfaces.
00:17:41 [W] We use multis as the cni-genie again, which delegates it's called to the SR iove cni plug-in and the flannel plugin.
00:17:51 [W] As cri-o VCI configures the RDMA interfaces in a poor while flannel configures the default at zero interface all the components in this floor up stream components that have been used with minor customizations.
00:18:09 [W] For each physical idea may interface on the host we create to Sr. Iot interfaces or virtual functions. So a DJ x 1 node that has for RDMA interfaces will result in eight
00:18:24 [W] Each full Note 4 will only use four of these 8 VF Z 1 corresponding to each physical interface and the remaining four vs are used for monitoring and storage Network creation.
00:18:10 [W] Also the Affinity of any VF to a particular GPU is decided at runtime by nickel the first image on the slide highlights the for physical idea be interfaces of the host and the
00:18:25 [W] The 4S are iot interfaces that are created from the physical interfaces and renamed in a pod.
00:18:26 [W] Since this is a production cluster.
00:18:29 [W] We are constantly monitoring the traffic on the idea by interfaces.
00:18:33 [W] This is done by capturing the values of the transmitted and received counters and the image shown here shows the traffic that you typically see while running a machine learning application now we
00:18:48 [W] Most of the transmitted and received counters and the image shown here shows the traffic that you typically see while running a machine learning application.
00:18:53 [W] Now we often see a Sawtooth like pattern and this is a result of the two phases that generally occur in every Epoch of an ml app, the two phases being the compute face and the communicate phase.
00:19:08 [W] In the compute phase the GPU workers are fetching from Storage performing computation creating MPI barriers, and there is no communication with other GPU workers.
00:19:20 [W] Hence the dip or the valley in the traffic in the communicate phase the GPU workers share the result of the previous phase with other workers and this results in the peak in the pattern that you see
00:19:35 [W] Now we ideally want to maximize the compute time and minimize the communicate time in an algorithm to improve Epoch training efficiency.
00:19:48 [W] In addition to monitoring the Rema traffic. We also run nickel or reduce routine and other Collective Primitives as part of a pre-flight check prior to running multi-node workloads
00:20:03 [W] the peak in the pattern that you see
00:20:06 [W] Now we ideally want to maximize the compute time and minimize the communicate time in an algorithm to improve Epoch training efficiency.
00:20:19 [W] In addition to monitoring the Rema traffic. We also run nickel already use routine and other Collective Primitives as part of a pre-flight check prior to running multi-node workloads nickel
00:20:54 [W] Steph captures the average bus bandwidth and it's run with different configurations where we vary the MPI rank per note. The number of threads per rank the number of GPS per thread and so on
00:21:09 [W] Laughs we see that even with increasing number of nodes. We get a pretty consistent average bus bandwidth of about 45 gbps where the theoretical light rate is 50 gbps.
00:21:27 [W] Here, we also have captured the performance of a pie torch virt ja that we have ran into different phases and we see that the throughput scaling which is measured in sequences per second is close to the ideal
00:21:42 [W] Performance of a by torch but job that we have run into different phases and we see that the throughput scaling which is measured in sequences per second is close to the ideal value even when using
00:22:02 [W] Even when using up to 256 concurrent GPS in a shared multi node cluster.
00:22:12 [W] So in conclusion, we wanted to share some of our findings and observations for DC placement and design following. The best practices will result in an environment that gives good
00:22:27 [W] A I am a workloads where bandwidth is concerned comprehensive understanding of the network flows will provide guidance for Designing multi-node clusters with our congestion a TDR and future
00:22:42 [W] bleh
00:22:40 [W] what's to be kept in mind is H trnd arrays are applicable for tours, which is as well and not for dc-to-dc connectivity alone.
00:22:50 [W] Regarding the kubernetes control plane.
00:22:52 [W] We found that the Upstream components and plugins in the community are easy to adopt and customize and by implementing features like gang scheduling starvation handing Cora's in our custom bad scheduler.
00:23:06 [W] We were able to achieve a seamless scheduling of multi-node workloads while maintaining fairness for end users.
00:23:17 [W] I would like to thank the entire team at Nvidia for their help and support and I would also like to thank you God for giving us this opportunity to show our work.
00:23:27 [W] work. Thank you.
00:23:44 [W] Hello, everyone.
00:23:48 [W] Is that a question from Ricardo Roca? Do you have a pointer with more details on your custom batch scheduler?
00:23:56 [W] I can share the slides and the video from that specific session where we go over the implementation details.
00:24:06 [W] Let me pull that up.
00:24:46 [W] One question anytime DC from the leash. Do you see that approach could be placed long approach to Resource scheduling for relatively small Networks.
00:24:59 [W] So I think you know dance is new to the block and Islam has existed for a long time. So
00:25:29 [W] There's no question anytime. We see from the laois DC that approach could be placed long approach to Resource scheduling for literally small Networks.
00:25:42 [W] So I think you know k8s is new to the block and Islam has existed for a long time. So
00:25:56 [W] it's like comparing apples to oranges so I wouldn't compare them together, but we do not have some clusters in time.
00:26:06 [W] There are workloads which are being used for every kind of Labor's long as well as great as our experiences, you know, collecting statistics integrating with young
00:27:45 [W] Workloads which are being used for every kind of me for so long as well as various other experiences, you know collecting statistics integrating with young people k8s is much easier.
00:28:20 [W] It's much easier once you about through these tunnels getting approaches.
00:28:31 [W] What was the argument monitoring to that you displayed?
00:28:34 [W] So we using switching structure which is external to the Clusters.
00:28:42 [W] So this is from stiff stiff arm placement.
00:28:48 [W] So we're using a regular network monitoring and mathematically true.
00:28:55 [W] And actually marking the port.
00:29:02 [W] So see a question from Lauren, were there any applications that give you a harder time than others?
00:29:08 [W] So I would say the beauty with this whole design is that we basically provide a one hop networking irrespective of the number of nodes that are requested.
00:29:21 [W] So what we had to specifically design for is if an application is requesting fewer gpus than complete set that.
00:29:32 [W] Note provides in which case we had to implement features like GPS Affinity to ensure a good performance for those applications.
00:29:40 [W] So I would say the beauty with this whole design is that we basically provide a one hot networking irrespective of the number of nodes that are requested.
00:29:52 [W] So what we had to specifically designed for is if an application is requesting fewer gpus than the complete set that note provides in which case we had to implement features like
00:30:59 [W] Is that the next front under nominal GDP of the background sir?
00:31:03 [W] You had to go through the PCI bus you want to have a good new modifying the other thing which we felt our experiences.
00:31:11 [W] The links are really high.
00:31:13 [W] the number of links in these clusters like former fiber optics names and we do PreFlight checks to ensure links are up and operational prior to launching the job.
00:31:27 [W] Styra qualifying that nothing is going to break if you know if there are links which are coming out if there is redundancy built in side and yet you know, we think that in the long run it's important to
00:31:42 [W] launching the jobs
00:31:50 [W] The David has a question. You really don't last question.
00:31:54 [W] I was unable to see a link to your previous presentation on custom schedule.
00:32:01 [W] Did you post the link to everyone? I don't think I posted everyone at me see if I can do that.
00:32:20 [W] I don't see a way to respond to everyone.
00:32:30 [W] So the name of the session was production multi-node jobs with gang scheduling kubernative.
00:32:46 [W] I can maybe respond in the slack Channel afterwards if it's not visible to everyone.
00:32:56 [W] So there's another question by Yin are gpus in the cluster shared concurrently by scheduler. So right now
00:33:06 [W] Still and while we can share the gpus within an OB don't Advocate a subset of GPU for jobs.
00:33:13 [W] So right now A GP as a whole is allocated to any workloads and the subset of gpus within a node can be allocated to different jobs.
00:34:23 [W] To go to the second page.
00:34:26 [W] There is a power ups sheringham and part supported.
00:34:30 [W] in stage 2
00:34:53 [W] If you go to the second page America GPS sheringham and part supported.
00:34:59 [W] He's great, too.
00:35:06 [W] CPU sharing among pods supported
00:35:13 [W] Like I said, we allocate a single GPU and entire GOP workloads.
00:39:44 [W] There are no more questions you can drop off.
00:39:53 [W] Close it up.
00:40:03 [W] Thank you, everyone.
00:40:07 [W] all right transport
