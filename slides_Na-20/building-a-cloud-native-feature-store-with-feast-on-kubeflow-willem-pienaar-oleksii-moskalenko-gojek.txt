Building a Cloud Native Feature Store with Feast on Kubeflow: JLNC-2044 - events@cncf.io - Friday, November 20, 2020 5:57 PM - 33 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hi, my name is William Adama engineering lead our data science platform team Edgar Jack.
00:00:04 [W] I'm joined today by Alexei moskalenko.
00:00:06 [W] Who's my colleague the senior engineer on the team today.
00:00:10 [W] using machine learning we're going to talk a bit about how Feast can help you with those challenges what Feast is and what Feast is not we're going to have a quick demo and feast then we're going to talk about the project itself where it's been and where it's going and we have three big announcements that we're going to make and
00:00:25 [W] stick around for those
00:00:17 [W] So how does data science work at most teams today?
00:00:20 [W] These projects are typically started because you want to Target some kind of business outcome some kind of metric that you want to push up and data scientists often tasked with building the first model or the first proof of concept for this so they're given data and they're
00:00:36 [W] This model in that see if it's actually viable and so often the basis of the project.
00:00:40 [W] There's the Inception is The Notebook.
00:00:43 [W] This is a linear into inflow that a data scientist comes up with and you give volume that notebook into an end-to-end machine Learning System.
00:00:52 [W] And so you have something that resembles.
00:00:54 [W] What about what we have on the screen now on the left you have your data sources and then you have Transformations.
00:01:01 [W] Let's say batch Transformations you have model training you have
00:01:04 [W] Deployment you have model serving one and two in flow and this often represents and resembles the notebook that's it's being evolved from and the final product.
00:01:15 [W] There is a model that's integrated with the production system and this works and most teams can get to this point by just hacking together open source for proprietary Solutions products at some point.
00:01:25 [W] Somebody will say we need real-time features and real-time data and hook up a stream at some point. Somebody will say we need more contribute to compute so the Hook-Up spot, but ultimately you're left with a knative.
00:01:34 [W] To end system one monolith these models come with some costs.
00:01:39 [W] Sometimes very big costs. One of the first problems is that they're very slow to a trade on. If you have one team that needs to work on feature engineering and other needs to turn modeling another that is for your traits on the online serving of the production requirements.
00:01:52 [W] features often need to be redeveloped when going from training to serving as often because features are written in Python and they you know, they don't performance in half and they need to be written in Java or go and there's all these reasons but inconsistencies arrived when engineers and data scientists need to work together to produce one final output
00:02:05 [W] The third problem is that because of this these rewrites and changes from one environment to the next often you have training in serving inconsistencies in the data and this can be to a performance dropped in your models fourth problem.
00:02:15 [W] Is that data quality problems arise when you don't know that you don't have the proper monitoring in place.
00:02:22 [W] You don't have the property Matrix in place.
00:02:24 [W] You can have validation of your data.
00:02:25 [W] Basically the tools that are available today are not meant to measure data, but at the same time this data is
00:02:32 [W] acting the business outcomes the prediction of the data that's going to the models affect the predictions that are made and those that affect the outcome and the decisions that you make so finally one of the biggest problems that we found at It Go Jack and many of the other teams
00:02:47 [W] Gross lack of reuse of features and yet feature engineering is one of the biggest costs that teams need to incur and so these are kind of the five big problems that we've seen mlperf a are faced with when it comes to operationalizing data.
00:03:00 [W] And exist for a battle tasted open source features Thor to address these problems that we've highlighted and we think these there's that feature storm.
00:03:02 [W] So features A system that aims to solve the key data challenges the production machine learning.
00:03:06 [W] So if we go back to that diagram, I showed you earlier. Typically what you have on the left is, you know, you'd have your data and your data Transformations and then you'd have him all training deployment serving this into inflow would be duplicated for each project that you undertake
00:03:21 [W] Typically what you have on the left is, you know, you have your data and you have your Transformations and then you'd have him all training deployment serving. This end-to-end flow would be duplicated for each project that you undertake. You'd not have reuse or very little reuse across projects.
00:03:35 [W] So how does feast help with this kind of Monolithic into an architecture 50 couples is completely so if we talk about the problems that we were highlighting earlier, the first one we were highlighting was lack of iteration.
00:03:48 [W] So now you've decoupled engineering of features and creation of features, because that process ends with Feast that's one team that can be engineer's data scientists.
00:03:57 [W] Then you have another team that was just purely iterating on model training. They're selecting features from Feast training model and shipping it to a registry.
00:04:05 [W] Then you have a third team.
00:04:07 [W] This can be a production team Engineers. Perhaps they're shipping models into production.
00:04:12 [W] They're hooking those up into communicating them to real time production systems. And you know, they're looking up online features at low latency and they can do this confidently in its scale and with monitoring and instrumentation that it requires and all three teams can
00:04:27 [W] Tree, then you have a third team.
00:04:29 [W] This can be a production team Engineers. Perhaps they're shipping models into production.
00:04:33 [W] They're hooking those up into communicating to real-time production systems. And you know, they're looking up online features at low latency and they can do this confidently in its scale and with monitoring and instrumentation that it requires and all three teams can iterate
00:04:59 [W] And so there's both salt iteration speed and also solves reuse because teams cannot independently use each other's artifacts with the models or features in this loodse architecture.
00:05:09 [W] So what if he's provide these provides essential central registry has reddish is a catalog through which you can Define your features and reuse them and collaborate on those across teams and across projects piece also does ingestion. So it has jobs that
00:05:25 [W] Data in from Upstream sources, whether streaming or batch into the stores in order for the third point, which is serving fees provides a point in time.
00:05:33 [W] Correct a temporarily correct the serving layer that allows you to look up data at scale for training model and it will handle the joints and everything you need and it also allows you to do online look up so low latency look-ups for
00:05:48 [W] Online case it also provides you the monitoring tools to operate your system at scale in production, but in Twin flow is represented on the screen.
00:05:56 [W] So on the left you have your data, so these can just be notebooks dead legs data warehouse and doesn't really matter.
00:06:02 [W] you push your data into feast and this is done through the triggering of ingestion job and that job is a spark job and this job will write your data into the stores. So in this case consistent copy of the data that
00:06:16 [W] you're ingesting is online and offline stores and the definitions of the features in the data that you're ingesting or handle my core Sophie score. Is that central registry?
00:06:25 [W] And then once you've ingested your data into the storage layer is then available for all teams to use so if you know one team wants to try out a feature that has been published and ingested into Feast they can simply query it out of the feast serving layer and then train a model
00:06:40 [W] For training uses SDK and for online serving and uses a low latency API.
00:06:41 [W] So what is Feast not Feast is not a workflow scheduler.
00:06:46 [W] It's not likely region or airflow.
00:06:47 [W] It doesn't do scheduling.
00:06:48 [W] It's not just add a lake or data warehouse like bigquery because it's got the online functionality as well.
00:06:54 [W] well. Although it uses bigquery and some of these tools underneath these doesn't do transformation. So it's unlike spark and pandas. Although it will utilize those tools, but it's made those are Upstream tasks fees does some have some Discovery and catalog
00:07:06 [W] In functionality, but it's not meant to be a discovery were cataloging system Feast does not try and solve lineage of data or data Version Control and feast is not an email or model serving or you know model tracking or metadata tracking solution.
00:07:23 [W] Hi, I'm Lexi from good Rick and I will Shew feast in action. But before that we need to deployed their primary way to deploy theast is to rule out Docker images.
00:07:37 [W] to kubernative using helm
00:07:41 [W] The only thing that needs to be done manually is to creating some secrets.
00:07:48 [W] So I already connected to my kubernative cluster.
00:07:51 [W] So I will create a namespace.
00:07:59 [W] And I would like Secrets first for postgres.
00:08:04 [W] You can see that the specified rest was responsible here.
00:08:08 [W] And also our installation we need Google service account key seems we're using Google storage.
00:08:18 [W] So now we can run Helm install.
00:08:21 [W] So after you clone Beast Repository
00:08:31 [W] You can run Helm install using some overrides over. It's finals that we keep in the Repository.
00:08:43 [W] It takes some time to start it all the containers.
00:08:50 [W] but when
00:08:52 [W] all boys are running we can now connect to the Jupiter no luck.
00:09:10 [W] No, when we finished his deployment, let me briefly talk about these components.
00:09:16 [W] Feature table and entities which are keys in those feature tables the next one. We serving. It provides features in real-time visual latency and mainly used
00:09:08 [W] model serving for running real-time prediction
00:09:08 [W] this serving relies on the online feature storage, which we use red Expo.
00:09:14 [W] Ingestion job here is mainly responsible for populating this online feature storage.
00:09:23 [W] It pulls data from data warehouse or data leak, which we call by child's or from Kafka organizations some kind of streaming.
00:09:39 [W] Source, which we go stream source.
00:09:43 [W] The historical retrieval is another job which mainly the similar job. It pulls data from data warehouse or data Lake to prepare
00:09:59 [W] Kind of streaming source, which we call streams oars.
00:10:05 [W] The historical retrieval is another job which mainly the similar job. It pulls data from data warehouse or data Lake to prepare
00:10:37 [W] For training boost those jobs aimed to provide consistency between data for training and data for surgery.
00:10:52 [W] This is Decay kind of Clues all this together. It used for creating features and feature tables for requesting features from
00:11:07 [W] In to retrieve online features. It also can be used to call historical retrieval or manage your streaming
00:11:22 [W] All historical retrieval War manage your streaming ingestion or by changing job.
00:11:28 [W] In this demo, I want to demonstrate how to use basic functionality of feast.
00:11:33 [W] I will register features in Peace. Corps will show you how to retrieve historical data set for training and of course how to use
00:11:48 [W] Feast serving to retrieve online features
00:11:53 [W] also we will cover how to ingest data from batch answering Source into online feature storage
00:12:04 [W] first we need to instantiate these client
00:12:07 [W] it's essentially an entry point to all functions provided by this SDK
00:12:14 [W] so we will configure it by both parameters to the fiscal Constructor and environment variables
00:12:31 [W] No, we can declare our features for this example.
00:12:38 [W] I will be using driver trips zettaset, which you can see is features like average daily trips for conversion rate and posted ribs today.
00:12:51 [W] The interesting thing about those features that some of them are captivated daily, like average daily trips or conversion rate and some of them like trips today. I'm dating in real time.
00:13:06 [W] And despite the fact that the all of the features are connected to the same key or entity driver ID.
00:13:16 [W] That's because Fisher tables should be aligned to the source.
00:13:21 [W] And now it's the job of the East to join features from different feature tables and provide you all features related to the same entity.
00:13:37 [W] So ingestion job here pulls data from the web source and stores it into the fissure storage another ingestion job consume the data from the stream source and also
00:13:52 [W] to be sure storage and serving the basically who all the features related to one entity and it does it with low latency because we storageos
00:13:57 [W] features related to the same entity together in redis
00:13:57 [W] now important fact here is that populating those data warehouse and stream source is not part of responsibilities of beasts.
00:14:09 [W] So these relies on the fact that our customers already have tools for populating both batch and streams options.
00:14:27 [W] So in addition to features that I already showed the official table has also two properties, but source and stream Source, each feature tables is required
00:14:42 [W] Course because all teacher tables should participate in training data set but not all feature tables not all features are available.
00:14:56 [W] From the stream source.
00:14:52 [W] at this specific example
00:14:56 [W] when you first create feature tables with barge sources only so we will be able to use them in preparing training data set.
00:15:07 [W] So we first
00:15:12 [W] creating the entity
00:15:16 [W] then we declare in feature table referring to this entity and adding all features that are updated daily specifying their types.
00:15:29 [W] And we also specify the source will be directory in Google storage, which will use forget for one also important
00:15:44 [W] creating the entity
00:15:47 [W] then we declare in feature table referring to this entity and adding all features that are updated daily specifying their types.
00:16:00 [W] And we also specify the source will be directory in Google storage which will use for get from what?
00:16:13 [W] Also important part is the time step goals that we use here.
00:16:19 [W] It's being used for point in time correctness.
00:16:26 [W] For example, which I will describe in a moment or just to deduplicate data that can be somehow duplicated in this Source.
00:16:37 [W] There is also the partition column which we use for.
00:16:43 [W] armas pork chops
00:16:46 [W] so
00:16:48 [W] we declared both species tables and now we can easily register them in the quarter.
00:16:56 [W] So I'm calling client don't apply Fisher table and that sense those declaration official tables to the core. Now. We can test that those tissue tables are successfully store.
00:17:15 [W] As I said, these is use that you already have tools that populating your data warehouse or pushing data to Kafka, but for the sake of this demo
00:17:31 [W] so
00:17:33 [W] we declared both species tables and now we can easily register them in the corner.
00:17:42 [W] So I am calling client don't apply Fisher table and that sense those declaration feature tables to the core. Now. We can test that those tissue tables are successfully store.
00:18:01 [W] As I said Feast is use that you already have tools that populating your data warehouse or pushing data to Kafka, but for the sake of this demo
00:18:16 [W] Google storage buckets
00:18:20 [W] So I will just generate several data frames.
00:18:28 [W] And we is this helper function.
00:18:31 [W] I will store them on the goose torch.
00:18:38 [W] As you can see the data has been successfully adjusted individually table which source we can check it by just retrieving words in the Google storage.
00:18:48 [W] You can see there are bunch of pocket files and it used eight call as the partition called.
00:19:00 [W] Now we can talk about actually your machine learning porch the first step in preparing the model is training and for that we need to generate training
00:19:16 [W] Important part about generating data set for training is point in time correctness, as you can see on this graph when we making a prediction.
00:19:30 [W] We use these values with various stamp steps.
00:19:36 [W] So as you can see those features probably from different sources came the different types.
00:19:44 [W] Thus prediction function usually used latest available value.
00:19:52 [W] Since we want our training data set to be as close as possible to what we will have in prediction.
00:20:00 [W] We're making this point in time correctness correction in the for the training data set as well.
00:20:10 [W] so data, scientist must specify the this line here basically this time point
00:20:20 [W] and we will will be doing backward search in finding the most recent tissue value in relation to these time point.
00:20:32 [W] an example
00:20:32 [W] Entities data frame to create a request for training data set.
00:20:37 [W] So we taken a sample of those entities and adding to them random event time step.
00:20:48 [W] Right. This is data frame, which we will provide as entity source for the our get historical features request.
00:21:00 [W] But this function here launch spok job, which can be run on the data Prague emaar or Standalone spark cluster.
00:21:13 [W] The spok job.
00:21:15 [W] We'll pull the features from yugabyte source and combine them with the entity data set which we just provided do this point in time correctness and return you
00:21:30 [W] for training
00:21:36 [W] As you can see features from to feature tables were combined on these resulting data set since I was generating those event aims,
00:21:51 [W] A little that in some point in time values for these features.
00:21:48 [W] Haven't yet been set.
00:21:52 [W] Thus we can see now here.
00:21:57 [W] But in general this data can be used for training your model.
00:22:05 [W] Now, let's move to the online features.
00:22:11 [W] So when you train your model will probably go with it to production and in production you will do real time prediction. So we will be using real-time online features.
00:22:27 [W] You don't have to do that.
00:22:29 [W] You need to have those features in online storage the simplest way to populate the storageos to take everything. You have large storage and put the latest values of each feature for each entity.
00:22:44 [W] To online storage.
00:22:40 [W] So now we will do exactly this. We will run the offline to online ingestion which also starts a spark job which reads from the blue storage and store to rest.
00:22:57 [W] When this job is completed we can again generate some sample of entities.
00:23:06 [W] And make a request to the online serving to retrieve features on those entities as you can see.
00:23:14 [W] This is much faster because it used radius and the Beckett now on those fish values you can run your production prediction.
00:23:31 [W] Is the next step?
00:23:33 [W] We can also add another features to our production prediction namely the real-time features.
00:23:41 [W] So in order to do that, we need to add a streaming source as one of the sources for power to our feature table.
00:23:51 [W] I drove for a month and of course we need our schema. So their ingestion job would know how to decode records coming from Kafka topic.
00:23:57 [W] So this other schema this example consists of feature trips today and indeed the driver 80 and event column called Data time.
00:24:09 [W] Now that we updated feature table in court we can start just job.
00:24:21 [W] As soon as job consuming specified Kafka topic we can start to populate it.
00:24:26 [W] So this function takes the record from our trips data frame.
00:24:36 [W] And encode it with given our schema and put it into caca topic.
00:24:49 [W] Now we should be able to retrieve those features with our get online feature method. So we generate an entity sample again.
00:25:01 [W] doing requests
00:25:11 [W] As soon as job consuming specified Kafka topic we can start to populate it.
00:25:16 [W] So this function takes the record from our trips data frame.
00:25:26 [W] And encode it with given our schema and put it into caca topic.
00:25:39 [W] Now we should be able to retrieve those features with our get online feature method. So we generate in 82 sample again.
00:25:51 [W] And doing the quest.
00:26:00 [W] So you can see here that we have in both average daily trips feature which comes from the barge source and was ingested with vitess ingestion
00:27:26 [W] having Bulls average daily trips feature which comes from the barge source and was ingested with vitess ingestion job and trips today which comes
00:27:41 [W] Source and we just ingested it from Kafka in the same response. Those features are now can be used for your model prediction.
00:27:53 [W] That is everything that I wanted to cover in this demo.
00:27:56 [W] Thank you for attention. You can find this notebook or other examples in our repository on GitHub.
00:28:10 [W] Three years ago Uber introduced the concept of a feature store through their blog post on Michelangelo their machine learning platform since then many companies have built their own features tours like Feast from Go Jack Airbnb is zipline and many others
00:28:26 [W] Large and small are starting to realize how critical a feature stories in the MLS stack of the future.
00:28:32 [W] Where is Feast now what we released 0.1 late 2018.
00:28:36 [W] We're all that out in go-jek, and we've gone from strength to strength from zero point 2 onwards.
00:28:41 [W] We've been working with our community.
00:28:42 [W] We developed decentralized serving. We develop point in time correctness.
00:28:47 [W] We added project isolation isolation and namespacing. We simplified concept and added integration points.
00:28:53 [W] integrated with t of x and t FD V. We allowed for multiple VPC support and request/response logging and monitoring and Matrix in production.
00:29:03 [W] And recently we've added Amazon support and spok support with the help of teams like tekton.
00:29:08 [W] So who are our big adopters and contributors?
00:29:13 [W] Well, we've been working closely with the community of large technology companies like a good.
00:29:18 [W] good. Oh that's delivered Cassandra support Postmates has delivered big table support. We've added some Priests of has added auth and we've been working with Microsoft and far Fetch and they've delivered delivered data bricks and Delta support and
00:29:33 [W] Something we've been working with tekton who've contributed spark and Amazon support.
00:29:38 [W] So there are over forty three contributors and 6 Plus Enterprise customers running feast in production today at scale.
00:29:44 [W] Let's talk about the feast vision for a second.
00:29:47 [W] So there are three aspects that we want to look at.
00:29:49 [W] They're not just running it in production, but they're also contributing code back to the project.
00:29:47 [W] They're collaborating with us these teams need to know that they can rely on the project at scale, but that they can also shape the direction of the project.
00:29:54 [W] So we believe open governance and transparent process is important in driving our project and encouraging further adoption.
00:30:02 [W] Secondly, we want to both Feast into a multi cloud or Cloud agnostic modular and production great feature store meaning if you're a team using feces
00:30:12 [W] Able to cherry-pick the components you need deployed wherever you are running your existing machine learning stackrox.
00:30:42 [W] Sing them so for open governance and standards. We are super excited to announce that Feast is now officially a part of the Linux Foundation meaning that not a single company has any kind of special privilege in the project? The trademark is now part of the
00:30:57 [W] foundation and we will be we are already operating on a governance structure defined by the Linux Foundation, which means it's completely neutral governed and this the process and structure is completely public and transparent
00:31:02 [W] She'll group of maintainers and a democratic process for new companies and teams to become maintainers of the project. Secondly, we believe that we need to work with a greatest Minds in the industry in order to both a best-in-class
00:31:05 [W] We believe that we need to work with a greatest Minds in the industry in order to both a basting class feature store. And we've already worked with many of the greatest Minds over the last couple of months, but one team in particular stands out there and we're super excited to announce
00:31:19 [W] And we're super excited to announce that tekton will be committing a significant amount of resources towards Feast development.
00:31:27 [W] We believe that by joining forces with tekton will be able to build a world-class features Thor that teams large and small can rely on and together.
00:31:36 [W] We hope to set the standard for what a feature store is and so we're super excited for this collaboration opportunity and all of this is happening underneath the Linux Foundation.
00:31:46 [W] umbrella then finally,
00:31:49 [W] We'd like to announce that we are not Optical component of kubeflow.
00:31:52 [W] And this is the start of our integration journey kubeflow is one of the best machine learning platforms out there completely open source, so it's both best of breed and an end-to-end system and we believe that by integrating into kubeflow and improving that integration
00:32:07 [W] Just improve the Integrations of feast with into Mo platforms, but also to Sister projects like model serving or pipelining or other managed services.
00:32:20 [W] So our roadmap going forward for fees 0.9 We Believe which is currently under development.
00:32:26 [W] We believe that we will be able to achieve cloudiness like deployments.
00:32:30 [W] So we've really got as were.
00:32:33 [W] sorry be very good Amazon Google Cloud support and we'll be launching as your support as well as on-prem deployment support will be adding all fine storage support for Delta and we'll be adding feature engineering support which will likely be through spok SQL.
00:32:48 [W] We will also be sending out requests for comments on on-demand feature Transformations and a feature Discovery user interface.
00:32:56 [W] And finally come and say hello or homepage is over at feasts knative. Our source code is all online is completely open and Apache 2 license.
00:33:05 [W] We have a feast channel on the kubeflow slack, and you can find a link to this deck on Tiny URL on the screen right now.
00:33:13 [W] you.
