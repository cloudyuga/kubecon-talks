Sponsored Lightning Talks: WOBA-0297 - events@cncf.io - Tuesday, November 17, 2020 3:47 PM - 15 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello.
00:00:00 [W] thanks for joining and the following I'm going to give you five reasons to rethink your default Ingress controller.
00:00:07 [W] First a little about me.
00:00:09 [W] My name is Daniel Corbett, and I'm the director of product that haproxy Technologies.
00:00:14 [W] Controller fall into the following categories performance reloads health checks observability and overload protection.
00:00:24 [W] Let's start with performance and a cloud native World performance is more important than it probably has ever been as poor performance requires excessive resource usage which results in a higher Cloud spend. It can also translate to a sub optimal user experience the haproxy.
00:00:39 [W] That is English controller was built to supercharge your kubernative environment out-of-the-box benchmarks show that a significantly outperforms the default Anguirus controller which relies heavily on Lua scripts and table to not only serve a little over thirty thousand requests per
00:00:38 [W] But it does so with approximately 20% less CPU and a lower latency ultimately. This means that out of the box, you will instantly be able to handle more requests for second with a smaller instance type and deliver your application faster to your clients.
00:00:50 [W] Here are the graphs from that Benchmark.
00:00:53 [W] You can watch the full video in the demo theater or at the haproxy booth, which is located in the Platinum Hall.
00:00:58 [W] You can also find the code to reproduce the benchmarks yourself in the GitHub link shown here.
00:01:05 [W] Call me of environments are constantly changing new Ingress resources are added routing rules are added or removed and secrets are frequently updated. The default Ingress controller does not have a runtime API.
00:01:18 [W] And so it requires a Reload for each of these changes.
00:01:20 [W] It was found during the previous benchmarks that making changes which forced to reload while running the benchmarks results in server-side errors being sent to the client. The haproxy kubernative Ingress controller does have a runtime API.
00:01:35 [W] Which allows the changes to be made on the fly without reloading for those changes which we which do require reload.
00:01:42 [W] It supports hitless reloads, which means that no traffic will be dropped.
00:01:46 [W] This gives you peace of mind that your end clients are not affected by changes within your kubernative environment.
00:01:52 [W] does not support active health checks and relies on kubernative sliminess and Readiness mechanisms for monitoring pothead with the haproxy kubernative angrist controller does support Active Health checking and it can be configured to use custom methods such as head
00:02:05 [W] And support sending a custom path on a per Ingress basis. It also provides flexibility allowing you to define the interval at which health checks are performed.
00:02:01 [W] So that unhealthy PODS of removed from load balancing sooner than later.
00:02:06 [W] We all know that logs and metrics are a key component for debugging and in spotting anonymously send your application.
00:02:12 [W] Unfortunately, the default Ingress controller provides a minimal amount of observability data went to the sparse amount of information available in the logs limited metrics, and it makes it difficult to even determine what application servers it's currently routing traffic
00:02:27 [W] Observability data went to the sparse amount of information available in the logs limited metrics and it makes it difficult to even determine what application servers it's currently routing traffic to requiring you to install a cube CTL plug-in to get that info.
00:02:42 [W] To install a cube CTL plug-in to get that info the haproxy kubernative Anglers controller provides robust metrics and logs and supports Cloud native logging by allowing you to Output logs to STD out but also supports routing them to
00:02:57 [W] Syslog server the longest contain detailed information such as end-to-end timing data on the request and a session State termination code allowing you to quickly determine whether a connection was completed successfully or not.
00:03:10 [W] Finally. It has a detailed stats page giving you an in-depth view into your application pods that are configured as well as the client sessions that are being rowdy.
00:03:21 [W] The stats page comes enabled out of the box.
00:03:24 [W] Here's a screenshot.
00:03:27 [W] And the last reason that you should rethink your use of the default Ingress controller is overload protection spikes in traffic can cause pain overloads and potentially lead to a snowball effect. The haproxy kubernative angrist controller allows you to specify
00:03:42 [W] Settings on a per pod basis when the maximum connection limit is reached the new connections are placed into a queue and served as soon as there's an available connections lat.
00:03:52 [W] Thanks for joining this talk and I hope I've given you some good reasons to rethink your usage of the default Ingress controller. If you're interested to try out the haproxy kubernative zingers controller, you can quickly get started with its Helm chart if you have any questions
00:04:07 [W] Oxy Booth located in the Platinum Hall and we'll be happy to answer them.
00:04:11 [W] Can I everyone my name is Matthew Erikson.
00:04:13 [W] I'm not going to take a look today at how we can back up our kubernative stata.
00:04:18 [W] So why are we talking about back up when containers are stateless?
00:04:21 [W] Well, there are two main reasons stateless applications eventually need to be migrated for life cycle events or kubernative version upgrades and stateful containers require protection of their persistent data effectively
00:04:36 [W] I mean for the failure of Hardware software or even just plain old human error.
00:04:38 [W] So before we get into the mechanics of how protection actually works, it's important to consider all of the sources of data outside of your kubernative cluster as well whether that be the sandbox development environments for your
00:04:53 [W] Your container image Registries and Helm charts to the eventual writing of your persistent data down to your storage subsystem, which I've shown here is rook and CSI that could be any one of the software-defined or
00:05:05 [W] two vendors available today
00:05:04 [W] So what is an application?
00:05:06 [W] Well, that's a little bit of a work in progress.
00:05:09 [W] The application CID program is working on recording the dependencies between API resources or objects.
00:05:18 [W] But today you can safely assume that an application is a collection of API resources and optionally some persistent data.
00:05:27 [W] So let's take a look now at the to use cases or the two methods by which you can perform protection today.
00:05:35 [W] Looking at data Centric back up.
00:05:37 [W] There is a custom operator.
00:05:39 [W] Typically that coordinates the scheduling of volume snapshot and then replication to an alternate or secondary storage array, which you can see at the bottom there and that custom operator
00:05:54 [W] That coordinates the scheduling of volume snapshot and then replication to an alternate or secondary storage array, which you can see at the bottom there and that custom operator provides
00:06:11 [W] Is that allowed both scheduling and retention of those snapshots?
00:06:17 [W] The recovery in that use case is taking those snapshots or snapshot creating a volume out of that and then rescheduling the application on the remote cluster using developer supplied
00:06:32 [W] application manifests
00:06:34 [W] the application Centric version of protection has one important distinction which we can see in step one here in Step 1. We are collecting the application manifests or config so that both
00:06:50 [W] Who application metadata is recorded along with the persistent data in Step 2?
00:06:56 [W] This method allows us to then go and recreate the volume and then reschedule the application and step to using all those the recorded application metadata, and this allows us to recover that.
00:07:11 [W] Action to any API compatible cluster.
00:07:13 [W] So which method is best which one should you use as always it depends if you are orchestrating your infrastructure from kubernative and you're using Hume CTL or gitops to manage your infrastructure.
00:07:29 [W] Is it depends if you are orchestrating your infrastructure from kubernative and you're using Cube CTL or it gitops to manage your infrastructure then the the data driven?
00:08:03 [W] In the the data driven approach offers a vastly enhanced protection capability because you can leverage things like change block tracking and replication that are not available via the CSI today
00:08:18 [W] Realized operations teams that are performing live cycle or centralized backup and recovery for traditional and containerized apps. Then the application Centric model may be more appropriate for you.
00:08:34 [W] Hang on if we look at our Harbor data. Now we can see that the the screenshot in the bottom right here indicates that even Harbor the harbor project recommend performing backup of your Harbor instance before you do upgrades
00:08:46 [W] Location is not a backup here. You should absolutely have a method for taking a copy of your Harbor application config or metadata and the underlying data as well.
00:08:56 [W] So what's happening now in the industry, you know, you've got you've got your existing applications completely under control and protected what else is happening?
00:09:06 [W] Well, there's lots of enhancement going on with the CSI so that CSI will eventually be able to support things like change.
00:09:14 [W] Block tracking or consistency group snapshots. So lots going on definitely get involved in the community stay on top of what the data protection working group are doing there and and bring your knowledge and experience in
00:09:29 [W] Experience in providing data protection to that group.
00:09:31 [W] Enjoy the rest of the show.
00:09:32 [W] Thank you.
00:09:32 [W] Today.
00:09:33 [W] We're going to be talking about how to deploy and monitor a scalable services in kubernative.
00:09:37 [W] He's using academics. We're going to be showcasing some forward-looking material and road map information today and we appreciate that you read the Safe Harbor slide here prior to us moving forward with the presentation. Let's dive in now.
00:09:52 [W] We know that there are several challenges of deploying workloads using kubernative use and
00:09:57 [W] our orchestration today the four key challenges that we've highlighted here based on a 450 one research study conducted in 2020 on kubernative use and containerd deployments include number one kubernative vendor choice now, there
00:10:12 [W] Is available in the market today to enable you to deploy or workloads on the cloud and using container services.
00:10:18 [W] Number two is app migration.
00:10:20 [W] Do you need to refactor Legacy apps into microservices or move them as is and which apps should you move number three end-to-end visibility is their key concern followed by monitoring vendor Choice including commercial and open source
00:10:35 [W] Number two is app migration.
00:10:29 [W] Do you need to refactor Legacy apps into microservices or move them as is and which apps should you move number three end-to-end visibility is their key concern followed by monitoring vendor Choice including commercial and open source
00:11:32 [W] orange is how to measure success and failures before and after from a technology perspective and before and after from business perspective now, when we look at the epidemics cluster region, the cluster agent was introduced about a year ago, and it works
00:11:47 [W] Kubernative distributions some of the key use cases of the cluster agent include application migration capacity planning and mapping app performance on kubermatic has to business outcomes, which is a key differentiator that happen Amex and Cisco author key product features.
00:12:02 [W] Features include real-time visibility into kubernative clusters correlating cluster metrics with application performance and the including all Container runtime supported so you can see a graphic on your screen now of the cluster agent and what it looks like
00:12:15 [W] issues utilization cetera, it's a really robust product that's available to you right now now drilling further into the cluster aging we then get to event and podría downs for deeper visibility, you know, this capability really ensures that your able to troubleshoot
00:12:30 [W] sure is that you're able to troubleshoot your pot of Vents and filter events and poddisruptionbudgets Pacific clusters and see the list of errors in order to find the root cause of the problem associated with a specific set of containers
00:12:43 [W] It allows you to filter your kubernative events on status type and name to focus on diagnosing specific classes of issues. Now moving forward. We've also introduced the auto instrumentation of a p.m.
00:12:56 [W] The kubernative cluster region is now driving a p.m.
00:12:59 [W] Instrumentation across your agent types, whether that's java.net know Jazz Etc the container namespacing labels are now automatically instrumented with a p.m.
00:13:09 [W] So this is a very powerful capability that we now offer all of our courses.
00:13:13 [W] Customers and really you're able to use your kubernative resource management for APM instrumentation from the one you use for monitoring itself.
00:13:22 [W] So really powerful next I want to talk about what we're introducing in are full of observability platform going forward. Number one.
00:13:29 [W] We're going to be introducing kubernative scalability and performance improvement with the additional scale and performance that our cluster agent is now able to partake we able to really handle increased scalability providing the health of all elements of your cluster.
00:13:44 [W] And were able to drill down into kubernative data and metadata to provide multi dimensional representations of your kubernative posture. Next. We're really excited to provide additional visibility into your container Name ID and stateful sets in events and inventory.
00:13:59 [W] That now we're able to provide complete visibility into your containers. And cause number one you're able to visualize what container and event applies that you can triage location of the problem in the cluster and number two, you can observe stateful sets on your cluster.
00:14:11 [W] So, you know, which are persistent items that are unavailable in a standard to learn these are state.
00:14:16 [W] So now that we've gone through what the kubernative cluster Asian offers you today and what's coming out next.
00:14:21 [W] I want to give you a broader. Look at our roadmap over the next 12 months. We're going to be working on
00:14:26 [W] log Snippets, which is going to enable you to now plug in your log monitoring option into our cluster aging and then we're able to then continue to expand our support of large number of pots upwards of thousands of pause that you're deploying
00:14:41 [W] Out knative visualization ux in which your and now able to correlate 8 p.m. And kubernative your name spaces and tags and we're working on opentelemetry exists alongside kubenetes including ci/cd releasing
00:14:53 [W] and servicemeshcon
