Scalable and Multitenant Networking in XDP and Kubernetes Operators: VOAQ-9029 - events@cncf.io - Friday, November 20, 2020 5:07 PM - 46 minutes

Participant: wordly [W] English (US)

Transcription for wordly [W]

00:00:00 [W] Hello everyone.
00:00:01 [W] Welcome to cook count 2020 and welcome to the session of more tender networking for kubermatic.
00:00:46 [W] Hello everyone.
00:00:48 [W] Welcome to cook count 2020 and welcome to the session of more tender networking for kubermatic.
00:00:54 [W] My name is in Shawn. I'm with futurewei Technologies in this talk today Sheriff who is now with Microsoft and I will present a multi-tenant scalable Network solution for kubernative.
00:01:10 [W] We hope you will enjoy the talk and and the end of the session. Hopefully you learned something and interested in contributing to this project or building your own Network Solutions with that.
00:01:24 [W] Let's get started.
00:01:25 [W] This is the agenda for today's talk.
00:01:28 [W] I will give an introduction and some background on the project why we're doing this and what's the context?
00:01:35 [W] I will also talking about current Network model in kubernative.
00:01:39 [W] At ease and the introduce you to have label design of our new multi-tenant model for kubernative.
00:01:46 [W] We have a forked version of kubernative score arctos, you can consider articles as a multi-tenant kubernative our introduced characters more in next slides after that
00:02:19 [W] And I will present a motor tendon scalable Network a solution for kubernative.
00:02:24 [W] We hope you will enjoy the talk and and the end of the session. Hopefully you'll learn something and interested in contributing to this project or building your own Network Solutions.
00:02:39 [W] With that let's get started.
00:02:41 [W] This is the agenda for today's talk.
00:02:44 [W] I will give an introduction and some background on the project why we're doing this and what's the context?
00:02:51 [W] I will also talking about current Network model in kubernative and introduce you to have label design of our new multi-tenant model for kubernative.
00:03:01 [W] We have a forked version of kubernative score Arcturus. You can consider arctos as a multi 10
00:03:09 [W] And kubernative our introduced actors more in next slides.
00:03:15 [W] After that, sheriff will give you hallway implement the more tender networking model in kubernative and introduce your demesia project Leda is a virtual Network solution that has both control plane and the data plan
00:03:46 [W] Hallway implement the more tender now working model in kubernative and introduce your demesia project meter is a virtual Network solution that has both control plane and the data plan based on the XDP technology.
00:04:03 [W] So these are project is part of umbrella project called Center has sinned Tara's project includes two independent projects one is called octopus when it's called Visa meter again is what we discussed
00:04:18 [W] talk hi, but I like to briefly introduce octopus project first so that you understand some of the context as I mentioned earlier actors project is a folk version of kubernative with a major
00:04:33 [W] They all three goals for actors a project first. We want to unify way more campaign orchestration and around time for that would explain product part that Venetian and the make the wrong time agent, which is kubernative
00:04:42 [W] 18 that support both barium and containers
00:04:45 [W] Second we want kubernative to support from 50k 200 K nodes in a cluster for that.
00:04:53 [W] We have to partition kubernative components such as a pi server schedulers controllers and etcd.
00:05:00 [W] Third.
00:05:02 [W] We want to build a true multi-tenant platform.
00:05:06 [W] So we design a multi-tenant solution for kubernative including a new tendon objects and a new network model.
00:05:14 [W] So we believe based those three goals Arcturus takes kubenetes to the next label make a true Cloud infrastructure platforms.
00:05:26 [W] So back to you.
00:05:29 [W] The main topic we have today, which is a major major Network solution is trying to address the following problems.
00:05:36 [W] And first we want to provide a virtual Network solution for kubenetes. So that kubernative apart the from different tendencies that will reside in the different virtual Network.
00:05:49 [W] so you can want to address the problem of fast provisioning of network resources for parts basically how to get a possibility as quick as possible from Network perspective so
00:06:05 [W] First we want to provide a virtual Network solution for kubenetes. So that kubernative apart the from different tendencies that will reside in the different virtual Network.
00:06:14 [W] So you can want to address the problem of fast provisioning of network resources for parts basically how to get a possibility as quick as possible from Network perspective.
00:06:28 [W] so that we want to have a scalable virtual Network solution to support the networking within a cluster of more than a hundred K host. And that's the problem to which other side from mr. Perspective.
00:06:45 [W] That's the most of most of you already know that current Network model for in kubernative is a flat Mount is a single address space and they shared a single DNA is Anna by default every part or containers can
00:07:00 [W] Perspective as the most of most of you already know that current Network model for in kubernative is a flat Mount is a single address space and they shared a single
00:07:28 [W] Ways every other part or containers in the Clusters, so by default there's an obit attendant for Network perspective.
00:07:38 [W] Kubenetes, introduced Network policy to isolate containerd or part each other's however Network policy is not secure or as not strong as isolated as virtual networking can do for example,
00:07:53 [W] Not prevent packet asleep in put in somewhere where the traffic is passing by and extract the information density of information or data of out of a packet additional needs. Some of now policies are implemented based on
00:08:08 [W] Out of the packet additional needs some of network policies are implemented based on Unix feature called net filter which user petabyte-scale with.
00:08:15 [W] In reality. Epi table rules could get huge and then cause overhead and increase never departed Network latency, not a security issue, but is a not desirable as far as Solutions we want.
00:08:34 [W] So in actors, we're introduced never gotten a new say RT object that represents a V PC or subnet each part to be created has to be associated with economic object and each never
00:08:49 [W] has his own IP address space so pause or containers created in the different network are naturally isolated with the network boundary as you can see from this diagram under
00:09:04 [W] Old campaigners created in the different network are naturally isolated virtual Network boundary as you can see from this diagram under slice now with the in each Network.
00:09:09 [W] You can still use Network policies to manage now because security within a 10 Single tended
00:09:17 [W] the new now got objects will introduce is an abstraction of an algorithm uses but not actual implementation. Someone still needs to actually create a VP series Creator subnet and a manage the IP address
00:09:46 [W] each Network you can still use now policies to manage now because security within a 10 Single tended
00:09:56 [W] the new now got objects will introduce is an abstraction of an algorithm uses but not actual implementation. Someone still needs to actually create a VP series Creator subnet and a manage the IP address
00:10:11 [W] This is where meets our comes into play meter is one of the network implementation for more tender Network model in kubernative.
00:10:20 [W] So now I will pass on to share Rev.
00:10:25 [W] He will present the detail of Misa design and implementation. Thank you.
00:10:33 [W] Hi everyone.
00:10:34 [W] This is Sharif.
00:10:35 [W] I live the development of the measured project and I am now a software engineer with Microsoft.
00:10:41 [W] We built these are from the ground up to accelerate pulse Network provisioning at scale actually to rethink Cloud networking imager altogether.
00:10:53 [W] The busiest in the exact same way as we build distributed systems in the cloud to make Cloud Network simple to understand and simpler to operate.
00:11:03 [W] In the rest of this talk I will walk you through our thought process and how miserable works from high level.
00:11:10 [W] These are consists of a CRT operators a demon and cni-genie.
00:11:21 [W] The demon exposes a grpc interface or the operator and the scene.
00:11:27 [W] I we eliminate any API from the worker nodes to the API server.
00:11:31 [W] This prevents operators failures to amplify as we add more workers to the cluster.
00:11:37 [W] Walk you through our thought process and how miserable works from high level. These are consists of a CRT operators a demon and cni-genie.
00:11:52 [W] The demon exposes a grpc interface or the operator and the scene.
00:11:58 [W] I we eliminate any API from the worker nodes to the API server.
00:12:02 [W] This prevents operators failures to amplify as we add more workers to the cluster.
00:12:11 [W] On the met on the data plan side measured consists of a set of multiple XDP programs. The process nodes packets.
00:12:18 [W] I will detail exactly how the X DB program processes the packets later in this architecture.
00:12:25 [W] We rethink the data plan programming model to scale the management plan accelerate Pope provisioning and develop customized Logic for Network Services as a result in misery nipples scalable and multi-tenant
00:12:40 [W] I will detail exactly how the xdb program processes the packets later.
00:12:45 [W] In this architecture, we rethink the data plan programming model scale the management plan accelerate Pope provisioning and develop customized Logic for Network Services as a result in misery nipples scalable and multi-tenant.
00:13:03 [W] Before I detail how miserable works I would like to discuss the limitations of Club is Network programming.
00:13:10 [W] What based Network programming is the defective programming model in Virtual switches including open V switch? I will take all the n and obvious as an example.
00:13:21 [W] obviously uses the concept of logical port to create a large logical switch that spans multiple hosts with this model creating ten thousands logical ports generates more than 40,000 sport bindings
00:13:36 [W] Which approach does not scale as we increase the number of worker nodes of a cluster?
00:13:42 [W] Moreover during flood programming it's not uncommon to observe an increase in the CPU utilization during flow passing.
00:13:51 [W] With the logical switch architecture that time to provision network resources for each new container depends on the number of containers that already exist in the system and the number of worker nodes of the cluster so clearly The Logical switch approach restricts skill
00:14:06 [W] Suitable for dynamic Cloud applications that have a short life time span like serverless.
00:14:09 [W] With the limitations of flow programming model in mind. We redesigned the hosts networking in Mazar to interconnect containers only with XDP programs.
00:14:20 [W] And to do this we attach an X DP program on each physical interface of a worker node.
00:14:25 [W] We name this program Transit XDP, this program processes all increase packets to the worker nodes.
00:14:35 [W] We also attach another XDP program on the vfp are connecting a container in the route name space.
00:14:43 [W] We call this program the transit agent.
00:14:46 [W] The transit agent attached to the VF peer to process all the egress traffic from each container.
00:14:54 [W] From a management plan perspective all what we needed to do.
00:14:58 [W] Next is to expose the EPF.
00:15:00 [W] userspace API as grpc RPC interface has the operator programs logical functions of the xdb program through these are PC interfaces.
00:15:12 [W] Understand the rule of virtual function.
00:15:16 [W] We need to look into the new network organisation of kubernative that measure enables.
00:15:20 [W] From a management plan perspective all what we needed to do.
00:15:19 [W] Next is to expose the EPF.
00:15:21 [W] userspace API as grpc RPC interface has the operator programs logical functions of the X DB program through these are PC interfaces.
00:15:33 [W] Understand the rule of virtual function.
00:15:37 [W] We need to look into the new network organisation of kubernative that measure enables.
00:15:42 [W] We extended kubernative with two resources that we typically find in any multi-tenant cloud system virtual private clouds dpc's and subnets within the ppc's.
00:15:54 [W] Greeting BP season subnets is straightforward and kubernative with see are these and operators on the data plan.
00:16:01 [W] We introduce new logical functions within the XDP programs.
00:16:05 [W] first logical function is bouncers within the network scope.
00:16:11 [W] And the second are dividers within the VPC scope.
00:16:15 [W] Unlike logical routers or switches the bouncers and dividers are in network distributed hash tables and I will detail exactly how they work in the next few slides.
00:16:28 [W] Bouncers and dividers are The Logical functions that make up the V PCS to isolate pods traffic for multi-tenant and allow tenants to reuse the same network address space.
00:16:44 [W] The user creates a VPC like any objecting kubernative as a yellow file, the user specifies designer range of the VPC and the number of VPC dividers.
00:16:55 [W] Because the divider is a distributed hash table. We can provide any number of dividers in the object definition with one being the default.
00:17:06 [W] When the VPC operator receives the VPC object, it schedules the VPC divider on one of the worker nodes of the cluster.
00:17:13 [W] And that Meek of the V PCS to isolate pods traffic for multi-tenant and allow tenants to reuse the same network address space.
00:17:26 [W] The user creates a VPC like any object in kubernative as a yellow file, the user specifies the side arranged of the VPC and the number of VPC dividers.
00:17:38 [W] Because the divider is a distributed hash table. We can provide any number of dividers in the object definition with one being the default.
00:17:48 [W] When the VPC operator receives the VPC object its schedules the VPC divider on one of the worker nodes of the cluster.
00:17:56 [W] Scheduling the divider does not mean that the operator runs any new code on the Node all what the operator does is labeling the selected host as a divider for the VPC.
00:18:09 [W] The kubernative operator also assigns a unique identifier for the VP see that the data plan uses two separate traffic, which is known as the virtual Network identifier.
00:18:24 [W] After creating the bpc the user now creates a subnet without within the VPC.
00:18:31 [W] We also provide any number of pounces to create for each network with one being the default.
00:18:37 [W] When the network operator receives the subnet object it schedules the bouncers on some of the worker nodes of the cluster.
00:18:45 [W] That evening the bouncer involves two actions first the operator labels the host as bouncers on the management plan second a programs. The dividers worker node through RPC calls.
00:18:58 [W] The RPC code simply populate Sandy bof map in the transit X DB program within the network within the VPC and IP addresses of the hosts that are bouncers to these networks in this example net one has
00:19:14 [W] The bouncer involves two actions first the operator labels the host as bouncers on the management plan second a programs. The dividers worker node through RPC calls the RPC calls simply
00:19:37 [W] And it too has a bouncer to and both of them are populated in the EPF map of the divider host.
00:19:46 [W] Now comes the interesting part where the user creates a pot within a multi Network a multi-tenant network.
00:19:54 [W] Knative Lincoln it is and similar to what you typically find in any cloud system.
00:19:59 [W] We do that we use annotations of the Pod object to specify the VPC and the subnet of the Pod articles controller adds the Network and Nick Nick annotations that I'm showing in these slides.
00:20:14 [W] Let me zoom out operator uses these annotations provision the Pod within the requested VPC and the third boundary.
00:20:22 [W] the Missouri operator provision the network resources with a constant number of RPC calls typically to
00:20:29 [W] the number of RPC calls does not depend on the number of worker nodes in the cluster all the pods already provisioned this what allows the network provisioning to scale.
00:20:41 [W] The operator makes one call to the bouncer host of the subnet and this cold effectively adds an entry in one EPF map with the IP address of the node hosting the pod.
00:20:53 [W] The other code Provisions the vef peer interface for the Pod and make it ready for the scene.
00:21:01 [W] The number of RPC calls does not depend on the number of worker nodes in the cluster all the pods already provisioned this what allows the network provisioning to scale.
00:21:13 [W] The operator makes one call to the bouncer host of the subnet and this call effectively adds an entry in one EPF map with the IP address of the head node hosting the pod.
00:21:25 [W] The other code Provisions the vef peer interface for the Pod and make it ready for the scene.
00:21:31 [W] I too consumed internally when the scene is adds the network interface.
00:21:36 [W] It makes a local call to the measure demon to consume that interface this design significantly simplifies the scene I
00:21:45 [W] When the scene I adds an interface, it only consumes the interface that is already created by the majority me.
00:21:55 [W] The effect of this provisioning workflow is significantly better scale and significantly better time to provision networking for the pod.
00:22:04 [W] The time to provision network resources for the Pod is now constant and independent on the number of worker nodes of the cluster or even the number of pots already provisioned in the cluster compare this govt n which does not scale
00:22:19 [W] It only consumes the interface that is already created by the majority.
00:22:26 [W] The effect of this provisioning workflow is significantly better scale and significantly better time to provision networking for the pod.
00:22:36 [W] The time to provision network resources for the pot is now constant and independent on the number four color nodes of the cluster or even the number of pots already provisioned in the cluster compare this to the oven which does not scale
00:23:15 [W] more nodes in the cluster or if the number of poles existing already increases
00:23:20 [W] Up until this point I described the management plan operations and in the next few slides, I will describe in detail how the X DB program on hose process packets.
00:23:33 [W] Consider the case in which a pot with IP address 10001 on host a sends a packet to appalled with an IP address 10002 on hasbi.
00:23:44 [W] And XDP program intercepts the outgoing packet from the Pod when the V is here receives it in the root name space.
00:23:53 [W] The XDP program simply looks up a static configuration in an EP F map and encapsulated the packet into a genève bucket.
00:24:01 [W] It also assigns the virtual Network identifier of the VPC in the Geneva header.
00:24:08 [W] Several tenants still use the same address space of the VPC where the network distinguishes traffic within a VPC by the vinai field.
00:24:18 [W] The only information available to the transit agent at this stage is the IP address of the bouncer.
00:24:24 [W] So it seems the packets to the bouncer by redirecting it for transmission on the physical interface.
00:24:32 [W] When the bouncer receives the packet that and XDP program is first to process it on the bouncer host.
00:24:39 [W] in the root namespace
00:24:41 [W] the XDP program simply looks up a static configuration in an EP F map and encapsulated the packet into a genève packet.
00:24:50 [W] It also assigns the virtual Network identifier of the VPC in the Geneva header.
00:24:56 [W] Several tenants still use the same address space of the VPC where the network distinguishes traffic within a VPC by the vinai field.
00:25:06 [W] The only information available to the transit agent at this stage is the IP address of the bouncer.
00:25:13 [W] So it seems the packets to the bouncer by redirecting it for transmission on the physical interface.
00:25:20 [W] When the bouncer receives the packet that and XDP program is first to process it on the bouncer host.
00:25:28 [W] The X DB program looks up the inner destination address of an EPS map.
00:25:32 [W] Then it rewrite the outer destination IP address to host C, which is the worker node running the destination Port 10002.
00:25:43 [W] When the packet arrives at whole see the XDP program d-cups elates the packet and redirected to the ve Spear of the Pod to receive it.
00:25:52 [W] This approach greatly simplifies the provisioning but it has a serious drawback all the packets now reverse one extra. Hope to reach their destination.
00:26:04 [W] I will now describe how we solve this entirely in XDP.
00:26:09 [W] Overcome the extra hop problem.
00:26:12 [W] We modify the X DB program running on the bouncer host to respond to our queries since we already have the pods IP and Mac address configure Pi the misery operator when you provision the pulse Network, so it makes sense to respond to our queries
00:27:01 [W] Address to host C which is the worker node running the destination Port 10002.
00:27:08 [W] When the packet arrives at Host see the XDP program d-cup solids the packet and redirected to the ve Spear of the Pod and receive it.
00:27:17 [W] This approach greatly simplifies the provisioning but it has a serious drawback all the packets now reverse one extra hop to reach their destination. I will now describe how we solve this entirely in XDP.
00:27:34 [W] Overcome the extra Hub problem we modify the X DB program running on the bouncer host to respond to our queries since we already have the pods IP and Mac address configured by the Missouri operator when you provision the pulse
00:27:49 [W] Really the bouncer response with the MAC address of 10002 but it does not only respond to our queries the bouncer also a de genève option in the outer package.
00:28:00 [W] We tell the transit agent. The 10002 is hosted at Host see when hosts are receives the ARP reply it extracts Degen adoption and adds the host mapping information in it. Cpf map.
00:28:15 [W] Now the transit agent of 10001 sends packets directly to the destination port and this direct communication happens from the very first packet of the flow and it remains throughout the pods lifetime.
00:28:30 [W] There's no more. There's one more detail here.
00:28:34 [W] When the Transit Agency sends a packet directly to the destination prod it will gets one pit in a Geneva option to tell the destination host that the packet is sent directly from the source portworx are nodes and not from the bouncer this
00:28:49 [W] Transit XDP at Host see to also return packet directly to The Source Point
00:28:55 [W] and this simple mechanism allows all flows in the cluster to be direct without traversing the bouncer.
00:29:01 [W] And at the same time allows the management plan to provision the network by only making few RPC calls to a couple of hosts in the cluster not all the hosts in the cluster.
00:29:13 [W] And if you think about the role of the bouncer now comparing T to a logical switch or The Logical router in all the end. It is an in-network as the in controller rather than a virtual switch.
00:29:25 [W] It's like a microservices in the network the prophet the provides distributed functions to the end points.
00:29:33 [W] We take this observation to extend Mazar functionality Beyond providing simple connectivity between the balls essentially we extended the bouncer functionality to implement kubernative Services as well.
00:29:47 [W] This is best to be explained by an example consider the 10001 poddisruptionbudgets lending packets to 1/2 the 192.
00:29:56 [W] 0 service the transit agent XDP program first process the packet which knows nothing about the network except sending the packet to the bouncer at hose be
00:30:08 [W] When the bouncer receives the packet it look up the destination IP address of the inner packet and it reminds it is for a service IP.
00:30:16 [W] Cancer can make right now including rewriting the inner destination IP address and sending the packet to a back in pot like any conventional net or lower balancer device.
00:30:19 [W] But I will describe a different type of the bouncer instead a de genève option of its decision.
00:30:27 [W] It instructs the pulse Transit agent have the service should modify the inner packet in the crew needs ample. The modification options says to rewrite the service IP address to ten zero zero four and Port
00:30:42 [W] Returns the packet to the center hosts, which is host a
00:30:35 [W] the transit XDP program on the clients host add this information operating in any in and EPF map entry.
00:30:42 [W] Then it rewrites the packet according to the accordingly and reasons the packet again, but this time not with the service IP but to the backend Hog Pen 004
00:30:55 [W] from this point forward the Transit Agency sends all the packets for this flow to the backend pot with going through the bouncers iptables or any other intermediate step.
00:31:10 [W] This powerful concept enable by X DB and kubernative operators allow us to scale the services and replace Q proxy without compromising the advantage of direct communication between poles over service IPS.
00:31:24 [W] These are skills out the number of panzers and dividers in the network to become a distributed in network controller that serves any traffic.
00:31:34 [W] The transit agent also implements allowed balancing function on the outer IP header to rebalance the traffic at put the bouncers what we typically find that a single bouncer is enough in most cases as it only
00:31:49 [W] compromising the advantage of direct communication between pods / service IPS
00:31:46 [W] These are skills out the number of cancers and dividers in the network to become a distributed in network controller that serves any traffic.
00:31:55 [W] The transit agent also implements allowed balancing function on the outer IP header to rebalance the traffic at to the bouncers what we typically find that a single bouncer is enough in most cases as it only
00:32:32 [W] lot of queries and the first packets of flows
00:32:35 [W] with this. I have provided an overview of Mazar and there's a lot of a lot more to it in the probe map and I conclude this talk and we will now play a recorded them before moving to the QA. Thank you very much.
00:32:56 [W] Hi for this demo today.
00:32:58 [W] We have a three node companies cluster using kind with knees are installed these Arts installed on this cluster via a demon set and an operator appointment.
00:33:07 [W] We bootstrapped the cluster with a default VPC and network each with their own divider and bouncer here. We use me as are simple and points to deploy pods.
00:33:18 [W] Now in each node, we see that an exit.
00:33:21 [W] program is load on the main interface and on the V device in the route name space. We load the agent x v program.
00:33:28 [W] Next we demonstrate Ping On the two recently created pods.
00:33:37 [W] Next we will create another VPC with two subnets.
00:33:41 [W] The VPC has two dividers.
00:33:44 [W] And here each of its subnets will have a single bouncer.
00:33:55 [W] The VPC has two dividers.
00:33:58 [W] And here each of its subnets will have a single bouncer.
00:34:14 [W] Now for each of these subnets, we will create an endpoint or a pod.
00:34:38 [W] Now with these two recently created pods, we will demonstrate cross net ping and here to demonstrate in our isolation. We will try to Ping across the PCS.
00:34:55 [W] Now we will demonstrate our operator provisioning 40 and points.
00:35:06 [W] Regardless of the existing number of endpoints are pause on the system all subsequent endpoints are provisioned at a constant time of about 0.35 seconds.
00:35:26 [W] Now in the next section we demonstrate intra and Inter Network direct path.
00:35:31 [W] For internetwork direct path only the first art packet goes through the bouncer.
00:35:37 [W] Once both sides have cashed the endpoint host information any traffic thereafter will only flow between the two endpoints hosts.
00:35:44 [W] 0.35 seconds
00:35:56 [W] now in the next section we demonstrate intra and Inter Network direct path.
00:36:01 [W] For internetwork direct path only the first art packet goes through the bouncer.
00:36:07 [W] Once both sides have cached and point host information any traffic thereafter will only flow between the two endpoint hosts.
00:36:20 [W] For internetwork direct path. The first art packet goes through the Vidor and both bouncers.
00:36:27 [W] Here the divider and the two endpoint hosts must cash the host information.
00:36:53 [W] Once all three have cash standpoint host information any traffic thereafter will flow between the two endpoint hose and the divider as an intermediary.
00:37:09 [W] Finally in this part of the demo we demonstrate using these are scaled endpoint as a replacement for the Carini's cluster IP service.
00:37:31 [W] When a service is created me as art creates a corresponding scaled endpoint.
00:37:43 [W] Once all three have cash standpoint host information any traffic thereafter will flow between the two endpoint host and the divider as an intermediary.
00:37:59 [W] Finally in this part of the demo we demonstrate using these are scale endpoint as a replacement for the communities cluster IP service.
00:38:21 [W] When a service is created me start create the corresponding scaled endpoint.
00:38:36 [W] Here we label the two recently created odds.
00:38:40 [W] To add them as back ends for the scaled endpoint.
00:38:57 [W] Now for this demonstration, we will curl the surface my third pod and in the reply we will see that the service replies with a pod name of one of the back ends.
00:39:07 [W] Here both pot 1 and pot to reply to curl.
00:39:16 [W] We can also ping the service this is possible because of the current scaled endpoint implementation.
00:40:06 [W] We can also ping the service this is possible because of the current scaled endpoint implementation.
00:40:56 [W] Hello, we have a couple questions from decision and let me also one of the first one of them from Adam a simple cases divider is roughly a
00:41:11 [W] Is and the bouncer is a symphony a arbitrary of traffic.
00:41:15 [W] You're right the router the weather is a router between with PCS and the bouncer route the traffic within a subnet and bottle itself has a simple lb load balancing
00:41:30 [W] Can low balance different and point or different a destination?
00:41:41 [W] Another question we have here is the ccnp functions for direct pass determination.
00:41:48 [W] If the station was to serve a web-based the multiple possible destination Hall and openebs down for the traffic with what EPF Sheriff. Do you want answer the question?
00:42:06 [W] Yes, either question again.
00:42:21 [W] you guys give me
00:42:25 [W] Yes, we can now.
00:42:30 [W] So the service is actually implemented in XDP as a presentation and what it does is that it receives the traffic flow and make the decision and relied back to the host and then the host is
00:42:45 [W] the right traffic directly to the backend so
00:42:47 [W] the regional conflict actually study bathroom
00:43:04 [W] There is another question that they can this be run on any public Cloud providers k3s or either place and the answer is yes, as long as the support HDPE. So as far as I'm aware that some of them support HTTP in
00:43:19 [W] Smooth so you can run it this way when they have it in driver mode or support offloading you will gain the most benefit of the acceleration.
00:43:25 [W] And that's another question is is a is a missile ready for production system.
00:43:30 [W] We have open doors open source, the major project a year ago we have now using in production yet with the numb we are not and it's not way around but welcome to the two doctors
00:43:45 [W] contribution of putting the production
00:43:52 [W] one more question here is like using this we need to add two Network latency and the answer is no because we are avoiding the extra help because of this bouncer using the data pack. So the idea is that the first packet which is the
00:44:07 [W] Because the bouncer and after that all different all the package from the same flow is directly between the parties.
00:44:17 [W] And there's a simple question say is a demo available by VT.
00:44:21 [W] Yes.
00:44:21 [W] yes, we're going to put in the video and they will be available for the demo.
00:44:26 [W] In fact, I think this them are already available in the open source web site is course entire arts instructor iot Center ovhcloud IO from there. We can find out some of the
00:44:41 [W] It's available there you can watch.
00:44:45 [W] I have a couple of questions from them as well. So we can assimilate or trace the path of a packet similar to of pro two traits and not rendered fully understand the question. So if you cancel out of line
00:45:00 [W] And I was Abed and the second one is about the NDP.
00:45:01 [W] So basically actually the all the traffic that is coming out of the code is oblivious to the to the to the bouncers and the device as long as there are IP version 4 traffic.
00:45:16 [W] For IP version 6 is not supported yet. Not sure about the roadmap that this is something that you can commit.
00:45:34 [W] And there is a questions about what is the mechanism to you separate attendance?
00:45:40 [W] Oh, sorry.
00:45:44 [W] Sheriff you want to answer that?
00:45:45 [W] Oh, sorry.
00:45:41 [W] So so the the mechanism to support two separate tenants on the data plan is actually every single of has its own from the 6 DB program that
00:45:56 [W] Disagree, so and then on the data planetscale from the wires, so it's a gene to Geneva protocol that secret of the Gentiles.
00:46:25 [W] How to tell time I see the sort of question we're going to answer your offline and soak you very much for attending physicians and thank you again.
00:46:41 [W] And thank you everybody if I will keep on pushing on these like these sync up with your fine.
